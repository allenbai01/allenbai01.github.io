# jemdoc: menu{MENU}{publications.html}
= Publications

* Preprints *

- [https://arxiv.org/abs/2410.13835 Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs.] \n
  Tianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael I. Jordan, Song Mei.

- [https://arxiv.org/abs/2209.11745 Unified Algorithms for RL with Decision-Estimation Coefficients: No-Regret, PAC, and Reward-Free Learning. ] \n
  Fan Chen, Song Mei, Yu Bai.

* Publications *

- [https://arxiv.org/abs/2404.05868 Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning.] \n
  Ruiqi Zhang, Licong Lin, Yu Bai, Song Mei. \n
  Conference on Language Modeling (COLM) 2024.
 
- [https://arxiv.org/abs/2312.00054 Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning? A Theoretical Perspective.] \n
  Lei Zhao, Mengdi Wang, Yu Bai. \n
  ICML 2024.

- [https://arxiv.org/abs/2310.10616 How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations.] \n
  Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, Yu Bai. \n
  ICLR 2024.

- [https://arxiv.org/abs/2310.08566 Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining.] \n
  Licong Lin, Yu Bai, Song Mei. \n
  ICLR 2024.
  
- [https://arxiv.org/abs/2307.02884 Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight.] \n
  Jiacheng Guo, Minshuo Chen, Huan Wang, Caiming Xiong, Mengdi Wang, Yu Bai. \n
  ICLR 2024.
  # Preliminary version appearing at ICML 2023 Workshop on The Many Facets of Preference-based Learning (MFPL).

- [https://arxiv.org/abs/2306.04637 Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection.] \n
  Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, Song Mei. \n
  NeurIPS 2023 (*Oral*).
  # Preliminary version appearing at ICML 2023 Workshop on Efficient Systems for Foundation Models (ES-FOMO).
  [https://github.com/allenbai01/transformers-as-statisticians \[Code\]]

- [https://arxiv.org/abs/2307.11353 What can a Single Attention Layer Learn? A Study Through the Random Features Lens.] \n
  Hengyu Fu, Tianyu Guo, Yu Bai, Song Mei. \n
  NeurIPS 2023.
  
- [https://arxiv.org/abs/2306.01243 Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations.] \n
  Minshuo Chen, Yu Bai, H. Vincent Poor, Mengdi Wang. \n
  NeurIPS 2023.
  # Preliminary version appearing at ICML 2023 Workshop on New Frontiers in Learning, Control, and Dynamical Systems (4LDC).

- [https://arxiv.org/abs/2302.06606 Breaking the Curse of Multiagency: Provably Efficient Decentralized Multi-Agent RL with Function Approximation.] \n
  Yuanhao Wang, Qinghua Liu, Yu Bai, Chi Jin. \n
  COLT 2023.
  
- [http://arxiv.org/abs/2302.01333 Lower Bounds for Learning in Revealing POMDPs.] \n
  Fan Chen, Huan Wang, Caiming Xiong, Song Mei, Yu Bai. \n
  ICML 2023.

- [https://arxiv.org/abs/2302.07869 Improved Online Conformal Prediction via Strongly Adaptive Online Learning.] \n
  Aadyot Bhatnagar, Huan Wang, Caiming Xiong, Yu Bai. \n
  ICML 2023.
  
- [https://arxiv.org/abs/2302.02571 Offline Learning in Markov Games with General Function Approximation.] \n
  Yuheng Zhang, Yu Bai, Nan Jiang. \n
  ICML 2023.

- [https://arxiv.org/abs/2209.14990 Partially Observable RL with B-Stability: Unified Structural Condition and Sharp Sample-Efficient Algorithms.] \n
  Fan Chen, Yu Bai, Song Mei. \n
  ICLR 2023 (*Notable-top-25% / "Spotlight"*).

- [https://arxiv.org/abs/2210.04157 The Role of Coverage in Online Reinforcement Learning.] \n
  Tengyang Xie, Dylan J. Foster, Yu Bai, Nan Jiang, Sham M. Kakade. \n
  ICLR 2023 (*Notable-top-5% / "Oral"*).

- [https://arxiv.org/abs/2210.11402 Learning Rationalizable Equilibria in Multiplayer Games.] \n
  Yuanhao Wang, Dingwen Kong, Yu Bai, Chi Jin. \n
  ICLR 2023.

- [http://arxiv.org/abs/2205.15294 Efficient Phi-Regret Minimization in Extensive-Form Games via Online Mirror Descent.] \n
  Yu Bai, Chi Jin, Song Mei, Ziang Song, Tiancheng Yu. \n
  NeurIPS 2022 (*Oral*).

- [https://arxiv.org/abs/2206.02640 Policy Optimization for Markov Games: Unified Framework and Faster Convergence.] \n
  Runyu Zhang, Qinghua Liu, Huan Wang, Caiming Xiong, Na Li, Yu Bai. \n
  NeurIPS 2022.

- [https://arxiv.org/abs/2206.03688 Identifying Good Directions to Escape the NTK Regime and Efficiently Learn Low-Degree Plus Sparse Polynomials.] \n
  Eshaan Nichani, Yu Bai, Jason D. Lee. \n
  NeurIPS 2022.

- [https://arxiv.org/abs/2205.07223 Sample-Efficient Learning of Correlated Equilibria in Extensive-Form Games.] \n
  Ziang Song, Song Mei, Yu Bai. \n
  NeurIPS 2022.

- [https://arxiv.org/abs/2210.12619 Conformal Predictor for Improving Zero-Shot Text Classification Efficiency.] \n
  Prafulla Kumar Choubey, Yu Bai, Chien-Sheng Wu, Wenhao Liu, Nazneen Rajani. \n
  EMNLP 2022.
  
- [https://arxiv.org/abs/2102.10809 Local Calibration: Metrics and Recalibration.] \n
  Rachel Luo, Aadyot Bhatnagar, Yu Bai, Shengjia Zhao, Huan Wang, Caiming Xiong, Silvio Savarese, Edward Schmerling, Marco Pavone. \n
  UAI 2022.
  
- [http://arxiv.org/abs/2202.01752 Near-Optimal Learning of Extensive-Form Games with Imperfect Information.] \n
  Yu Bai, Chi Jin, Song Mei, Tiancheng Yu. \n
  ICML 2022.
  # \n Preliminary version selected as /contributed talk/ at ICLR 2022 Workshop on Gamification and Multiagent Solutions.

- [https://arxiv.org/abs/2110.04184 When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?] \n
  Ziang Song, Song Mei, Yu Bai. \n
  ICLR 2022.

- [https://arxiv.org/abs/2202.11091 Efficient and Differentiable Conformal Prediction with General Function Classes.] \n
  Yu Bai, Song Mei, Huan Wang, Yingbo Zhou, Caiming Xiong. \n
  ICLR 2022. [https://github.com/allenbai01/cp-gen \[Code\]]
  
- [https://arxiv.org/abs/2106.05515 Understanding the Under-Coverage Bias in Uncertainty Estimation.] \n
  Yu Bai, Song Mei, Huan Wang, Caiming Xiong. \n
  NeurIPS 2021 (*Spotlight*). \n
  # Appearing as /spotlight presentation/ at ICML 2021 Workshop on Distribution-Free Uncertainty Quantification. \n
  # Also appearing at ICML 2021 Workshop on Uncertainty \& Robustness in Deep Learning.
  
- [https://arxiv.org/abs/2106.04895 Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning.] \n
  Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, Yu Bai. \n
  NeurIPS 2021. \n
  # Appearing at ICML 2021 Workshop on Reinforcement Learning Theory.
  
- [https://arxiv.org/abs/2102.11494 Sample-Efficient Learning of Stackelberg Equilibria in General-Sum Games.] \n
  Yu Bai, Chi Jin, Huan Wang, Caiming Xiong. \n
  NeurIPS 2021. \n
  # Appearing as /spotlight presentation/ at ICML 2021 Workshop on Reinforcement Learning Theory.
  
- [https://arxiv.org/abs/2102.01748 Near-Optimal Offline Reinforcement Learning via Double Variance Reduction.] \n
  Ming Yin, Yu Bai, Yu-Xiang Wang. \n
  NeurIPS 2021. \n
  # Appearing at ICML 2021 Workshop on Reinforcement Learning Theory.
  
- [https://arxiv.org/abs/2102.07856 Don't Just Blame Over-parametrization for Over-confidence: Theoretical Analysis of Calibration in Binary Classification.] \n
  Yu Bai, Song Mei, Huan Wang, Caiming Xiong. \n
  ICML 2021.

- [https://arxiv.org/abs/2103.04554 Exact Gap between Generalization Error and Uniform Convergence in Random Feature Models.] \n
  Zitong Yang, Yu Bai, Song Mei. \n
  ICML 2021.

- [http://arxiv.org/abs/2010.05843 How Important is the Train-Validation Split in Meta-Learning?] \n
  Yu Bai, Minshuo Chen, Pan Zhou, Tuo Zhao, Jason D. Lee, Sham Kakade, Huan Wang, Caiming Xiong. \n
  ICML 2021.
  # Preliminary version appearing at NeurIPS 2020 workshop on Meta-Learning.

- [http://arxiv.org/abs/2010.01604 A Sharp Analysis of Model-based Reinforcement Learning with Self-Play.] \n
  Qinghua Liu, Tiancheng Yu, Yu Bai, Chi Jin. \n
  ICML 2021.

- [https://arxiv.org/abs/2007.03760 Near Optimal Provable Uniform
  Convergence in Off-Policy Evaluation for Reinforcement Learning.] \n
  Ming Yin, Yu Bai, Yu-Xiang Wang. \n
  AISTATS 2021 (*Oral*).  \n
  # Preliminary version appearing at NeurIPS 2020 workshop on Offline Reinforcement Learning.
  
- [https://arxiv.org/abs/2006.13436 Towards Understanding Hierarchical
  Learning: Benefits of Neural Representations.] \n
  Minshuo Chen, Yu Bai, Jason D. Lee, Tuo Zhao, Huan Wang, Caiming
  Xiong, Richard Socher. \n
  NeurIPS 2020. \n

- [https://arxiv.org/abs/2006.12007 Near-Optimal Reinforcement
  Learning with Self-Play.] \n
  Yu Bai, Chi Jin, Tiancheng Yu. \n
  NeurIPS 2020. \n
  # Preliminary version appeared as /oral presentation/ at ICML 2020 Workshop on the Theoretical Foundations of Reinforcemnet Learning.

- [https://arxiv.org/abs/2002.04017 Provable Self-Play Algorithms for Competitive Reinforcement Learning.] \n
  Yu Bai, Chi Jin. \n
  ICML 2020.
  
- [https://arxiv.org/abs/1910.01619 Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks.] \n
  Yu Bai, Jason D. Lee. \n
  ICLR 2020.
  
- [https://arxiv.org/abs/1905.12849 Provably Efficient Q-Learning with Low Switching Cost.] \n
  Yu Bai, Tengyang Xie, Nan Jiang, Yu-Xiang Wang. \n
  NeurIPS 2019.
  
- [https://arxiv.org/abs/1810.10702
  Subgradient Descent Learns Orthogonal Dictionaries.] \n
  Yu Bai, Qijia Jiang, Ju Sun. \n
  ICLR 2019.

- [https://arxiv.org/abs/1810.00861 
  ProxQuant: Quantized Neural Networks via Proximal Operators.] \n
  Yu Bai, Yu-Xiang Wang, Edo Liberty. \n
  ICLR 2019. [https://github.com/allenbai01/ProxQuant \[Code\]]

- [https://arxiv.org/abs/1806.10586 Approximability of Discriminators
  Implies Diversity in GANs.] \n
  Yu Bai, Tengyu Ma, Andrej Risteski. \n
  ICLR 2019.

- [https://arxiv.org/abs/1607.06534 The Landscape of Empirical Risk
  for Nonconvex Losses.] \n
  Song Mei, Yu Bai, Andrea Montanari, 2016.
  \n /The Annals of Statistics,/ Volume 46, Number 6A (2018), 2747-2774.

* Other technical reports *


- [https://arxiv.org/abs/2201.01163 Finding General Equilibria in Many-Agent Economic Simulations Using Deep Reinforcement Learning. ] \n
  Michael Curry, Alexander Trott, Soham Phade, Yu Bai, Stephan Zheng. \n

- [https://arxiv.org/abs/2002.04010 Taylorized Training: Towards Better Approximation of Neural Network Training at Finite Width.] \n
  Yu Bai, Ben Krause, Huan Wang, Caiming Xiong, Richard Socher. \n
  [https://github.com/allenbai01/taylorized-training \[Code\]]

- [https://arxiv.org/abs/1903.00184 Proximal algorithms for constrained composite optimization, with applications to solving low-rank SDPs.] \n
  Yu Bai, John C. Duchi, Song Mei. \n
  # Preprint, 2019.

- [https://arxiv.org/abs/1805.08756 Analysis of Sequantial Quadratic
  Programming through the Lens of Riemannian Optimization.] \n
  Yu Bai, Song Mei. \n
  # 2018.
  
- [https://arxiv.org/abs/1707.03073 TAPAS: Two-pass Approximate
  Adaptive Sampling for Softmax.] \n
  Yu Bai, Sally Goldman, Li Zhang. \n
  # 2017. 
