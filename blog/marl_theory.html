<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Recent Progresses in Multi-Agent RL Theory | MARL Theory</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Recent Progresses in Multi-Agent RL Theory" />
<meta name="author" content="Yu Bai, Chi Jin" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Reinforcement learning (RL) has made substantial empirical progresses in solving hard AI challenges in the past few years. A big portion of these progresses—Go, Dota 2, Starcraft, economic simulation, social behavior learning, and so on—come from multi-agent RL, that is, sequential decision making involving more than one agents. While the theoretical study of (single-agent) RL has a long history and a vastly growing recent interest, multi-agent RL theory is arguably a newer and less developed field, with its own unique challenges and opportunities that we feel very excited about. In this extended blog post, we present a brief overview of the basics of multi-agent RL theory, along with some recent theoretical developments in the past few years. We will focus on learning Markov games, and cover the basic formulations, learning goals, planning algorithms, as well as recent advances in sample-efficient learning algorithms under the interactive (exploration) setting. The majority of this blog post will assume familiarity with the basics of RL theory in the single-agent setting (for example, materials in this fantastic book). We will focus on “what is different” when it comes to multi-agent, and discuss the various recent developments and opportunities therein. \(\def\cS{\mathcal S}\) \(\def\cA{\mathcal A}\) \(\def\cF{\mathcal F}\) \(\def\mc{\mathcal}\) \(\def\P{\mathbb P}\) \(\def\E{\mathbb E}\) \(\def\V{\mathbb V}\) \(\def\R{\mathbb R}\) \(\def\tO{\widetilde{\mathcal{O}}}\) \(\def\eps{\varepsilon}\) \(\def\epsilon{\varepsilon}\) \(\def\setto{\leftarrow}\) \(\def\up{\overline}\) \(\def\low{\underline}\) \(\def\what{\widehat}\) \(\def\ba{\boldsymbol{a}}\) 1. Formulation: Markov Games There are various formulations for multi-agent RL. In this blog post, we will focus on Markov Games (MG; Shapley 1953, Littman 1994), a generalization of the widely-used Markov Decision Process (MDP) framework into the case of multiple agents. We remark that there exist various other frameworks for modeling multi-agent sequential decision making, such as extensive-form games, which can be considered as Markov games with special (tree-like) structures in transition dynamics; when combined with imperfect information, this formulation is more suitable for modeling games such as Poker. Roughly speaking, a Markov Game has the same (state, action) -&gt; (reward, next state) structure as in an MDP, except that now “action” is replaced by the joint actions of all players, and “reward” is replaced by a collection of reward functions so that each player has her own reward. In words: Markov Games with m players: (state, m actions) -&gt; (m rewards, next state). Formally, we consider a multi-player general-sum Markov game with $m$ players, which can be described as a tuple \(\text{MG}(H, \mathcal{S}, \{\mathcal{A}_{i}\}_{i=1}^{m}, \mathbb{P},\{r_{i}\}_{i=1}^{m})\), where $H$ is the horizon length, $\mathcal{S}$ is the state space with $\vert \mathcal{S}\vert=S$. \(\mathcal{A}_i\) is the action space for the $i$-th player with \(\vert\mathcal{A}_i\vert=A_i\). All players act simultaneously; we use \(\ba=(a_1,\dots,a_m)\in \prod_{i=1}^m \mathcal{A}_i\) to denote a joint action, \(\mathbb{P}=\{\mathbb{P}_h\}\) is the collection of transition probabilities, where \(\mathbb{P}_h(s_{h+1} \vert s_h, \ba_h)\) is the probability of transiting to the next state \(s_{h+1}\), given state-action pair $(s_h, \ba_h)$ at step $h$, \(r_i=\{r_{i,h}\}\) is the reward function for the \(i\)-th player. At step $h$, the $i$-th player will experience reward $r_{i,h}(s_h, \ba_h)\in[0, 1]$. For simplicity of presentation, we assume the reward is deterministic. A trajectory of an MG looks like this: [ s_1, (a_{1, 1}, \dots, a_{m, 1}), (r_{1,1},\dots, r_{m, 1}), s_2, \dots, s_H, (a_{1, H}, \dots, a_{m, H}), (r_{1,H}, \dots, r_{m,H}). ] Finally, a (Markov) policy for the $i$-th player is denoted by \(\pi_i=\{\pi_{i,h}\}_{h=1}^H\), where \(\pi_{i,h}(a_{i,h}\vert s_h)\) is the probability of taking action $a_{i,h}$ at step $h$ and state $s_h$. We use \(V_{i,h}^{\pi}(s_h)\) and $Q_{i,h}^{\pi}(s_h, \ba_h)$ to denote the value and Q-value functions for player $i$ at step $h$, when joint policy $\pi$ is used. (This can either be a product policy $\pi=(\pi_1,\dots,\pi_m)$ which can be executed independently, or a correlated policy which samples the joint action $\ba_h$ for all the players together in a potentially correlated fashion.) We assume the initial state $s_1$ is deterministic, so that \(V_{i,1}^{\pi} := V_{i,1}^\pi(s_1)\) is the overall return for the $i$-th player. 1.1 Special case: Zero-sum games An important special case of Markov games is zero-sum (competitive) MGs: A zero-sum Markov Game is a Markov Game with $m=2$, and $r_1\equiv -r_2$. As each player wishes to maximize her own reward, in a zero-sum MG, the two players are in a purely competitive relationship. We thus define the overall value in a zero-sum MG to be player 1’s value function: $V_h:=V_{1,h}$. We call player 1 the max player and player 2 the min player. Zero-sum MGs model a number of real-world two-player strategic games, such as Go, Chess, Shogi, etc. From a mathematical perspective, zero-sum MGs is perhaps the most immediate extension of a single-agent MDP—solving it can be thought of as a certain min-max optimization, akin to how solving a single-agent MDP is a certain maximization problem. 2. Nash equilibrium In an MDP, the learning goal for an agent is to maximize her own cumulative reward. In an MG, this is still the learning goal for each individual agent, but we have to additionally decide on some notion of equilibrium to combine these individual optimality conditions. Throughout the majority of this blog post, we consider the celebrated notion of Nash equilibrium (Nash 1951), which states that for each player $i$, fixing all other player’s policies, her own policy is a best response (i.e. maximizes her own cumulative reward): Nash equilibrium: Each player plays the best response to all other player’s policies. It is clear that the Nash equilibrium strictly generalizes the notion of an optimal policy for MDPs—An MG reduces to an MDP if player 1 acts and all other players are dummy (in the sense their actions do not affect transition and rewards). In this case, the best response of player 1 reduces to maximizing her own reward with respect to the environment (MDP) only. Formally, we define an $\eps$-approximate Nash equilibrium (henceforth also $\eps$-Nash) as follows. Below, $\pi_{-i}$ denotes the collection of all but the $i$-th player’s policy within $\pi$. Definition: A product policy $\pi=(\pi_1,\dots, \pi_m)$ is an $\eps$-approximate Nash equilibrium, if we have [ \max_{i\in[m]} \big( \max_{\pi_i’} V_{i,1}^ {\pi_i’, \pi_{-i}}- V_{i,1}^{\pi_i, \pi_{-i}} \big) \le \eps. ] We say $\pi$ is an (exact) Nash equilibrium if the above holds with $\eps=0$. The Nash equilibrium need not be unique for (general-sum) MGs. Also, note that by definition any Nash policy must be a product policy (i.e. all the players execute their own policy independently). There exist other proper notions of equilibria when players do play in a correlated fashion; we will dive deeper into this in Section 5. 2.1 Nash equilibrium in zero-sum games The Nash equilibrium enjoys additional nice properties in zero-sum MGs. Recall that $V_1^{\pi_1, \pi_2}$ denotes player 1’s value for which $\pi_1$ seeks to maximize and $\pi_2$ seeks to minimize. Proposition: For any zero-sum MG, we have (Unique Nash value) There exists a $V_1^{\star}\in[0,H]$ such that all Nash equilibrium $(\pi_1^\star, \pi_2^\star)$ achieves this same value: \(V_1^{\pi_1^\star, \pi_2^\star} = V_1^{\star},\) even though the Nash policy $(\pi_1^\star, \pi_2^\star)$ may not be unique. (Strong duality) Any Nash equilibrium is also a min-max solution and a max-min solution. In other words, the following two inequalities (which holds for any pair of $(\pi_1, \pi_2)$) \[ \left\{ \begin{aligned} &amp; \max_{\pi_1&#39;} V_1^{\pi_1&#39;, \pi_2} \ge \min_{\pi_2&#39;} \max_{\pi_1&#39;} V_1^{\pi_1&#39;, \pi_2&#39;} \ge \max_{\pi_1&#39;} \min_{\pi_2&#39;} V_1^{\pi_1&#39;, \pi_2&#39;} \ge \min_{\pi_2&#39;} V_1^{\pi_1, \pi_2&#39;} \\ &amp; \max_{\pi_1&#39;} V_1^{\pi_1&#39;, \pi_2} \ge V_1^{\pi_1, \pi_2} \ge \min_{\pi_2&#39;} V_1^{\pi_1, \pi_2&#39;} \end{aligned} \right. \] become equalities ($=V_1^{\star}$) if $(\pi_1, \pi_2)$ is a Nash equilibrium. These nice properties make zero-sum MGs an appealing setting for learning Nash equilibria, which will be the focus of our next two sections. 3. Planning algorithms in zero-sum MGs Given the above setup, a natural first question is: How do we compute the Nash equilibrium in zero-sum MGs assuming known transitions and rewards? In other words, what is a good planning algorithm for learning Nash equilibria? It turns out that a direct multi-agent adaptation of Value Iteration, dating back to (Shapley 1953), gives an algorithm for computing the Nash equilibrium for a known game. We term this algorithm as Nash-VI (Nash Value Iteration). Similar to how Value Iteration (VI) computes the optimal values and policies for an MDP using backward dynamical programming, Nash-VI performs similar backward updates on an MG except for the policy selection mechanism: the maximization over the vector $Q^\star(s, \cdot)$ in VI is now replaced by a \(\text{MatrixNash}\) subroutine over the matrix $Q^{\star}(s, \cdot, \cdot)$. This makes sense given that the learning goal is now learning a Nash equilibrium policy over the two players jointly, instead of the reward maximizing policy for a single player. In words, Nash-VI = Value Iteration with \(\text{Max}(\cdot)\) replaced by \(\text{MatrixNash}(\cdot)\). Algorithm (Nash-VI): Initialize \(V_{H+1}(s)\equiv 0\) for all \(s\in\cS\). For $h=H,\dots,1$, compute the following over $s\in\cS$ and all $(a_1, a_2)\in\cA_1\times \cA_2$: \[ \begin{aligned} &amp; Q_h^{\star}(s, a_1, a_2) = r_h(s, a_1, a_2) + (\P_h V_{h+1}^\star)(s, a_1, a_2), \\\ &amp; (\pi_{1,h}^\star(\cdot\vert s), \pi_{2,h}^\star(\cdot\vert s)) = \text{MatrixNash}(Q_h^{\star}(s,\cdot,\cdot)), \\\ &amp; V^{\star}_h(s) = \pi_{1,h}^\star(\cdot \vert s)^\top Q_h(s,\cdot,\cdot) \pi_{2,h}^\star(\cdot \vert s). \end{aligned} \] Return policies $(\pi_1^\star, \pi_2^\star)$ where \(\pi_i^\star=\{\pi_{i,h}^\star(\cdot\vert s)\}_{(h,s)\in[H]\times \cS}\). Above, the \(\text{MatrixNash}\) subroutine for any matrix \(M\in \R^{A_1\times A_2}\) is defined as \[ \begin{align}\label{equation:matrixnash} \text{MatrixNash}(M) := \arg \big( \max_{\pi_1 \in \Delta_{\cA_1}} \min_{\pi_2 \in \Delta_{\cA_2}} \pi_1^\top M \pi_2 \big), \end{align} \] which can be implemented efficiently using linear programming. Here $\Delta_{\cA_1}$ and $\Delta_{\cA_2}$ are the probability simplex over action sets $\cA_1$ and $\cA_2$ respectively. Using backward induction over $h$, it is straightforward to show that the policies $(\pi_1^\star, \pi_2^\star)$ returned by Nash-VI is indeed a Nash equilibrium of the desired MG. The $V_h^{\star}(s_h)$ and $Q_h^{\star}(s,a_1,a_2)$ computed above are uniquely defined; we call them Nash values of the game at state $s$ and state-action tuple $(s,a_1,a_2)$ respectively. Value Iteration is not the only algorithm for computing Nash. As an alternative, the Nash Q-Learning algorithm (Hu &amp; Wellman 2003) performs incremental updates on the Q values, and uses the same matrix Nash subroutine to compute (Nash) V values from the Q values. Further, both Nash-VI and Nash Q-learning can be adapted when the game transitions and rewards are not known and have to be estimated from samples. For example, using random samples from a simulator (can query any $(s_h, a_{h,1}, a_{h,2})$ and obtain a sample of $(r_h, s_{h+1})$), an $\eps$-Nash can be learned with $\tO(H^3SA_1A_2/\eps^2)$ samples with high probability, using variants of either Nash-VI (Zhang et al. 2020) or Nash Q-learning (Sidford et al. 2019). Centralization. Finally, we remark that both Nash-VI and Nash Q-learning are centralized training algorithms—that is, the computation for the two players are coupled (due to the joint computation in the matrix Nash subroutine). In order to execute these algorithms, the two players could not act in isolation, and have to coordinate or even live in a same centralized machine (self-play). In Section 4.2, we will present another decentralized algorithm, which is further capable of improving the dependency of sample complexity on the number of actions from \(\tO(A_1A_2)\) to \(\tO(\max\{A_1, A_2\})\). 4. Sample-efficient learning of Nash equilibria in zero-sum MGs We now move on to the more challenging setting of learning Nash equilibria from interactive learning environments. Rather than assuming full knowledge or simulators of the game, we now assume that the algorithm can only learn about the game by playing repeated episodes of the game in a trial-and-error fashion (also known as the interactive/exploration setting). This requires the algorithms to further address the challenge of balancing exploration and exploitation. Our goal is to design algorithms that are sample-efficient, i.e. can learn an $\eps$-Nash with as few episodes of play as possible. 4.1 Optimistic Nash-VI Our first algorithm is an optimistic variant of the Nash-VI algorithm we have seen in Section 3. This algorithm first appeared in (Liu et al. 2020), and builds on earlier sample-efficient Nash-VI type algorithms within (Bai &amp; Jin 2020, Xie et al. 2020). Algorithm (Optimistic Nash-VI, sketch): Initialize \(\up{Q}_h(s, a_1, a_2)\setto H\) and \(\low{Q}_h(s, a_1, a_2)\setto 0\) for all \((s, a_1, a_2, h)\). For episode $k=1,\dots,K$: For step $h=H,\dots,1$: For all $(s, a_1, a_2)\in\cS\times \cA_1\times \cA_2$: Set $t\setto N_h(s, a_1, a_2)$ (visitation count). If $t&gt;0$ then Compute bonus \(\beta\setto \text{BernsteinBonus}(t, \what{\V}_h[(\up{V}_{h+1} + \low{V}_{h+1})/2] (s, a_1, a_2)\). Compute additional bonus \(\gamma\setto (c/H) \what{\P}_h(\up{V}_{h+1} - \low{V}_{h+1})(s, a_1, a_2)\). \(\up{Q}_h(s, a_1, a_2)\setto \min\{ (r_h + \what{P}_h\up{V}_{h+1})(s, a_1, a_2) + \beta + \gamma, H \}\). \(\low{Q}_h(s, a_1, a_2)\setto \max\{ (r_h + \what{P}_h\low{V}_{h+1})(s, a_1, a_2) - \beta - \gamma, 0 \}\). For all $s\in \cS$: \(\pi_h(\cdot, \cdot \vert s) \setto \text{MatrixCCE}(\up{Q}_h(s, \cdot, \cdot), \low{Q}_h(s, \cdot, \cdot))\). \(\up{V}_h(s) \setto \langle \pi_h(\cdot, \cdot \vert s), \up{Q}_h(s, \cdot, \cdot) \rangle\). \(\low{V}_h(s) \setto \langle \pi_h(\cdot, \cdot \vert s), \low{Q}_h(s, \cdot, \cdot) \rangle\). Play an episode using (correlated) policy $\pi$, and update the empirical models $\what{\P}_h$, $N_h$. Set $(\pi_1^k, \pi_2^k)$ to be the marginal policy of the current policy $\pi$. Main techniques. Optimistic Nash-VI enhances the original Nash-VI in two main aspects in order to perform sample-efficient exploration in an interactive learning setting: The algorithm maintains two optimistic Q estimates: An upper estimate $\up{Q}_h$, and a lower estimate $\low{Q}_h$. The amount of optimisticity is governed by the two bonus functions $\beta$ and $\gamma$, where $\beta$ is similar to the usual Bernstein bonus for exploration in single-agent settings (Azar et al. 2017, Jin et al. 2018), and $\gamma$ is a specifically designed model-based bonus that allows a tighter analysis, akin to the one used in (Dann et al. 2018). We remark that different from the single-agent setting, the introduction of $\gamma$ is key in achieving $\mathcal{O}(S)$ sample complexity instead of $\mathcal{O}(S^2)$. The policy is now computed using a matrix CCE subroutine. The use of matrix CCE in the context of learning Markov game is first introduced by (Xie et al. 2020). For any pair of matrices $\up{M}, \low{M}\in\R^{A_1\times A_2}$, $ \text{MatrixCCE}(\up{M}, \low{M})$ is defined as any correlated policy $\pi \in \Delta_{\cA_1\times \cA_2}$ such that the following holds: \[ \begin{aligned} &amp; \E_{(a_1, a_2)\sim \pi} [\up{M}(a_1, a_2)] \ge \max_{a_1&#39;\in \cA_1} \E_{(a_1, a_2)\sim \pi}[\up{M}(a_1&#39;, a_2)], \\ &amp; \E_{(a_1, a_2)\sim \pi} [\low{M}(a_1, a_2)] \le \min_{a_2&#39;\in \cA_2} \E_{(a_1, a_2)\sim \pi}[\low{M}(a_1, a_2&#39;)]. \end{aligned} \] In other words, if the two players jointly play $\pi$, the max player has no gain in deviating for the payoff matrix $\up{M}$, and the min player has no gain in deviating for the payoff matrix $\low{M}$. The purpose of this matrix CCE subroutine is to find a good policy with respect to the upper / lower estimates in a computationally efficient fashion (this subroutine can be implemented using linear programming). Indeed, here a general-sum matrix Nash subroutine with respect to $(\up{Q}_h(s,\cdot,\cdot), \low{Q}_h(s,\cdot,\cdot))$ may as well be used (and leads to the same sample complexity results), yet would suffer from computational difficulties for large $A_1,A_2$, as finding Nash in a two-player general-sum matrix game is PPAD hard (Daskalakis et al. 2013). These techniques enable the optimistic Nash-VI algorithm to achieve the following sample complexity guarantee for learning Nash equilibria in zero-sum MGs. Theorem (Liu et al. 2020): With high probability, optimistic Nash-VI outputs an $\eps$-Nash within $K=\tO(H^3SA_1A_2/\eps^2)$ episodes of play. Compared with the minimax sample complexity $H^3SA/\eps^2$ for single-agent MDPs achieved by the UCBVI algorithm (Azar et al. 2017, with matching lower bound in (Jin et al. 2018, Domingues et al. 2021)), the sample complexity achieved by Nash-VI for zero-sum MGs only pays an additional $A_1A_2$, i.e. product of the action space. This may seem natural at first sight as the joint action space in a zero-sum MG does have size $A_1A_2$. However, as we will see in the next subsection, it is possible to design a decentralized learning algorithm that achieves a \(\max\{A_1,A_2\}\) dependence on the action spaces. Centralized training, decentralized execution. Note that the $\pi$ returned by the matrix CCE subroutines and used in the exploration step is a correlated policy. Therefore, the entire algorithm has to be executed in a centralized fashion. However, the final output is the marginals of this correlated policy and thus is an (approximately Nash) product policy that can be deployed in a decentralized way. This is reminiscent of the “centralized training, decentralized execution” paradigm which is also commonly used in empirical multi-agent RL. 4.2 V-Learning Our second algorithm is a recently proposed algorithm, V-Learning, which first appeared in (Bai et al. 2020) in the name of Nash V-Learning. Here, we present the simplified version of the algorithm in its journal version (Jin et al. 2021c). V-Learning itself can be viewed as a single-agent RL algorithm. To run V-learning in the multiagent setting, we simply let each player run the single-agent V-learning entirely on her own as long as she observes the states, rewards, and next states (as if it were a single-agent MDP). The opponent’s action does not need to be observed. Therefore, V-learning is naturally a decentralized algorithm. We present the algorithm as follows. Algorithm (V-Learning): For all $(h, s, a)\in [H]\times \cS\times \cA$, initialize \(V_h(s)\setto H-h+1\), \(\pi_{h}(a \vert s)\setto 1/A\), and $N_h(s)\setto 0$. For episode $k=1,\dots,K$: Receive $s_1$. For $h=1,\dots,H$: Take action $a_{h}\sim \pi_{h}(\cdot \vert s_h)$, observe reward $r_h$ and next state $s_{h+1}$. (No need to observe the opponent’s action.) $t=N_h(s_h)\setto N_h(s_h)+1$. \(V_{h}(s_h)\setto (1-\alpha_t) V_h(s_h) + \alpha_t (r_h + \min\{V_{h+1}(s_{h+1}), H-h\} + \beta_t)\). \(\pi_{h}(\cdot \vert s_h) \setto \text{Adv_Bandit_Update}(a_{h}, \frac{H-r_h-\min\{V_{h+1}(s_{h+1}), H-h\} }{H})\) on the $(s_h, h)$-th adversarial bandit. Algorithm (Executing output policy of V-Learning): Sample $k\setto\text{Unif}([K])$. For step $h=1,\dots,H$: Observe $s_h$, and set $t\setto N_h^k(s_h)$. Set $k\setto k_h^i(s_h)$, where $k_h^1(s_h)&lt;\dots&lt;k_h^t(s_h)$ are the indices of all past visitations to $(h, s_h)$ prior to episode $k$, and $i\in [t]$ is sampled randomly with probability $\alpha_t^i$. Take action $a_{h}\sim \pi_{h}^k(\cdot\vert s_h)$. The structure of the V-Learning algorithm is rather distinct from either Nash-VI or Nash Q-learning (perhaps its closest relative among classical RL algorithms). We briefly explain its three main components: Incremental update of $V$: This part is similar to Q-learning, except that we do not model the Q functions, but rather directly model the V functions (hence the name V-learning). Consequently, this update avoids constructing Q functions in the multiagent setting (which is of the size $\cA_1 \cA_2$). It also “ignores” the actions taken by the opponent. The incremental update uses a step-size sequence \(\{\alpha_t\}_{t\ge 0}\). We choose \(\alpha_t=(H+1)/(H+t)\) following the analysis of Q-learning in (Jin et al. 2018). Adversarial bandit subroutine: Recall that in Q-learning (or its optimistic version), the greedy (argmax) rule over the estimated Q function is used at each $(s,h)$ to determine the policy. In V-learning, the policy at each $(s_h, h)$ is rather maintained by an adversarial bandit subroutine (there are $SH$ such adversarial bandits in total). Concretely, after $(s_h, h)$ is encountered and $(r_h, s_{h+1})$ are received, the corresponding adversarial bandit performs an update step of the form “Action $a_{h}\in\cA$ suffered loss \(\frac{H-r_h-\min\{V_{h+1}(s_{h+1}), H-h\} }{H}\in [0,1]\).” The adversarial bandit subroutine is then responsible for determining how to actually compute the new policy given this update. In V-Learning, we instantiate the adversarial bandit as a weighted follow-the-regularized-leader algorithm, in order to achieve a per-state weighted regret guarantee of a certain form, at each $(s, h)$. Output policy: The output policy of V-learning is a nested mixture policy: A random integer $k$ is updated by random sampling throughout the execution of this policy, and at each $h$ this $k$ determines a particular policy for drawing the action $a_{h}$. This makes the policy non-Markov in general, which is drastically different from Nash-VI as well as most classical algorithms in single-agent RL which outputs Markov policies. Technically, such an output policy is critically required in order to extract from the per-state “regret” guarantees at each $(s,h)$ a policy that enjoys an overall near-Nash guarantee on the overall game value. (Hence its original name “certified policy” in (Bai et al. 2020) as this policy “certifies” the value functions appearing in the per-state regret bounds.) Here, the sampling probabilities $\alpha_t^i$ are defined as [ \alpha_t^0 = \prod_{j=1}^t (1-\alpha_j),~~~\alpha_t^i = \alpha_i \prod_{j=i+1}^t (1-\alpha_j). ] The above techniques lead to the following guarantee on V-Learning for zero-sum MGs. Theorem (Bai et al. 2020, Jin et al. 2021c): Suppose both players run the V-Learning algorithm with the \(\text{Adv_Bandit_Update}\) instantiated as a suitable weighted Follow-The-Regularized-Leader algorithm for each $(s,h)$. Then, running this for $K$ episodes, (the product of) their certified output policies $(\what{\pi}_1, \what{\pi}_2)$ is an $\epsilon$-Nash as long as \(K\ge \tO(H^5S\max\{A_1, A_2\}/\eps^2)\). Compared with optimistic Nash-VI, Nash V-Learning achieves milder dependence on the action space (\(\max\{A_1,A_2\}\) vs. $A_1A_2$) thanks to its decentralized nature, at the cost of worse $H$ factors. We remark that the original sample complexity presented in (Bai et al. 2020) is \(\tO(H^6S\max\{A_1,A_2\}/\eps^2)\), and one $H$ factor can be improved by adopting the sharper analysis in (Tian et al. 2020). 4.3 Summary of algorithms The table below summarizes the algorithms we have introduced so far for learning $\eps$-Nash in zero-sum MGs. Note that here Nash Q-Learning (and its optimistic version) was not presented in this blog post; the actual algorithm and theoretical guarantee can be found in Section 3 of (Bai et al. 2020). Algorithm Training Update Main estimand Sample complexity Nash-VI centralized model-based \(\P_h(s&#39;\vert s,a_1,a_2)\) \(\tO(H^3SA_1A_2/\eps^2)\) Nash Q-Learning centralized model-free \(Q_h^{\star}(s,a_1,a_2)\) \(\tO(H^5SA_1A_2/\eps^2)\) V-Learning decentralized model-free \(V_h^{\star}(s)\) \(\tO(H^5S\max\{A_1, A_2\}/\eps^2)\) 5. General-sum MGs with many players We now shift attention to the more general case of general-sum MGs with $m$ players for any $m\ge 2$. For general-sum MGs, we are not only interested in learning near-Nash product policies, but also the broader class of correlated equilibria for correlated policies, which we define as follows. 5.1 Correlated Equilibria (CE) and Coarse Correlated Equilibria (CCE) Definition (Coarse Correlated Equilibrium): A correlated policy $\pi$ is an $\eps$-Coarse Correlated Equilibrium (CCE), if any player could not improve her own reward by more than $\eps$ by deviating from $\pi$ and playing some other policy on her own: [ \max_{i\in[m]} \max_{\pi_i’} \Big( V_{i,1}^{\pi_i’, \pi_{-i}} - V_{i,1}^{\pi} \Big) \le \eps. ] Definition (Correlated Equilibrium; Informal): A correlated policy $\pi$ is an $\eps$-Correlated Equilibrium (CE), if any player could not improve her own reward by more than $\eps$ by first observing her own action sampled from the correlated policy at each state, then deviating to some other action: [ \max_{i\in[m]} \max_{\phi\in \Phi_i} \Big( V_{i,1}^{\phi\diamond \pi} - V_{i,1}^{\pi} \Big) \le \eps. ] Above, $\Phi_i$ is the set of all possible strategy modification functions for player $i$ (for a formal definition see (Liu et al. 2020)). In general, we have {Nash}$\subset${CE}$\subset${CCE}. Therefore, CE, CCE can be thought of as relaxations of Nash and make sense as learning goals on their own right as general-sum Nash suffers from PPAD hardness in the computational efficiency anyway (Daskalakis et al. 2013). We remark that similar as Nash, the notions of CE and CCE are standard and widely studied in the game theory literature, with many real-world implications (see e.g. this book &amp; Wikipedia). The above definitions are direct extensions of the original definitions (for strategic games) into Markov games. 5.2 Value Iteration algorithms for Nash, CE, and CCE The model-based optimistic Nash-VI algorithm can be adapted straightforwardly to the case of multiple players, by simply replacing the MatrixCCE subroutine in Optimistic Nash-VI algorithm with (multi-dimensional) matrix {Nash,CE,CCE} subroutines. This gives the following result for learning equilibria in general-sum MGs. Theorem (Liu et al. 2020): A multi-player adaptation of the optimistic Nash-VI algorithm coupled with (multi-dimensional) matrix {Nash,CE,CCE} subroutines can learn an $\eps$-{Nash, CE, CCE} of a general-sum MG within \(\tO(H^4S^2\prod_{i\le m} A_i/\eps^2)\) episodes of play. The conceptual advantage of this algorithm is that it estimates the full transition model $\P_h(s_{h+1} \vert s_h, \ba_h)$ directly; it can then learn all three kinds of equilibria by plugging in the corresponding (multi-dimensional) matrix subroutines. However, this algorithm suffers from the curse of multiagents: the sample complexity scales as $\prod_{i\le m} A_i$, which is not surprising given that it estimates the full transition model. Also note that, despite the sample efficiency, the multi-player matrix Nash subroutine involved in the above algorithm does not likely admit a poly time implementation due to the PPAD hardness in computing general-sum Nash. 5.3 V-Learning for CE and CCE Recall that V-Learning is a decentralized algorithm that can be deployed by each player independently, and for zero-sum MGs one could extract from its execution history a suitable output policy that is near Nash. It turns out that this paradigm generalizes nicely to the problems of learning CE and CCE in general-sum MGs. Specifically, recent concurrent works show that V-Learning algorithms instantiated with suitable adversarial bandit subroutines could learn CE/CCE sample-efficiently, without suffering from the curse of multiagents: Theorem (Song et al. 2021 &amp; Jin et al. 2021c) For $m$-player general-sum MGs, suppose each player run the V-Learning algorithm independently with suitably chosen adversarial bandit subroutines, and we extract a correlated output policy in a suitable fashion. Then: Instantiating the adversarial bandit subroutine as a regret minimizer (CCE-V-Learning), the correlated output policy is an $\eps$-CCE within \(\tO(H^5S\max_{i\le m} A_i/\eps^2)\) episodes of play; Instantiating the adversarial bandit subroutine as a swap regret minimizer (CE-V-Learning), the correlated output policy is an $\eps$-CE within \(\tO(H^5S(\max_{i\le m} A_i)^2/\eps^2)\) episodes of play. Compared with the VI algorithm from the last section, V-Learning breaks the curse of multiagents as the sample complexity scales polynomially in $\max_{i\le m}A_i$, as opposed to $\prod_{i\le m} A_i$ which is exponential in $m$. This is thanks to the decentralized learning structure of the V-Learning algorithm. (Bib note: We remark that Song et al. 2021 actually achieves $\tO(H^6S(\max_{i\le m}A_i)^2/\eps^2)$ for learning CE, which is one $H$ factor worse than Jin et al. 2021c due to using a slightly different swap regret minimizer. The concurrent work of Mao &amp; Basar 2021 also achieves a $\tO(H^6S\max_{i\le m}A_i/\eps^2)$ result for learning $\eps$-CCE.) 6. Learning MGs with function approximation In single-agent MDPs, there is a rich body of work on the theory of function approximation—that is, sample-efficient learning in large state/action spaces equipped with a restricted function class. Here we briefly review some recent advances on function approximation in Markov Games. Throughout this section, we shift back to considering two-player zero-sum MGs. 6.1 Linear function approximation Similar as a linear MDP, a (zero-sum) linear MG is a Markov Game whose transitions and rewards satisfy the following linear structure: \[ \begin{aligned} &amp; \P_h(s&#39; | s, a_1, a_2) = \langle \phi(s, a_1, a_2), \mu_h(s&#39;) \rangle, \\ &amp; r_h(s, a_1, a_2) = \langle \phi(s, a_1, a_2), \theta_h \rangle, \end{aligned} \] where $\phi:\cS\times \cA_1\times \cA_2\to \R^d$ is a known feature map and $\mu_h$, $\theta_h$ are (unknown) parameters for the transitions and rewards. The following result shows that linear MGs can be learned sample-efficiently with mild polynomial dependencies on $d,H$, similar as in linear MDPs. Their algorithm is similar to the optimistic Nash-VI algorithm, and further builds upon the single-agent optimistic least-squares value iteration algorithm (Jin et al. 2020) to utilize the linear structure in the transitions and rewards, by using a bonus function similar to linear bandits. To adapt this algorithm into the multi-agent setting, their algorithm again replaces the maximization over actions by matrix CCE subroutine, and uses both the upper bounds and lower bounds. Theorem (Xie et al. 2020): For linear zero-sum MG with horizon length $H$ and feature dimension $d$, there exists an algorithm that learns an \(\eps\)-Nash within \(\tO(d^3H^4/\eps^2)\) episodes of play. We remark that Chen et al. 2021 consider the related model of linear mixture MGs and design an algorithm with episode complexity $\tO(d^2H^3/\eps^2)$. 6.2 General function approximation; The power of exploiter The above sample complexity results are based on a direct adaptation of Nash VI into the linear setting. Such an approach critically depends on a restrictive assumption called optimistic closure, which is true for tabular MGs and linear MGs, but rarely holds in general function approximation. To remove such an assumption, in the single-agent setting, we can modify the standard optimistic VI algorithm which computes an upper bound of value functions at each step, to a globally optimistic version which only computes the upper bound of value functions at the first step. The latter approach considers confidence sets at all steps simultaneously, thus providing a much sharper optimistic value, and allowing the algorithm to learn a near-optimal policy sample-efficiently (Zanette et al. 2020, Jin et al. 2021a). To adapt such an idea to the multi-agent setting, it turns out that we need one more technique—the use of an exploiter. In contrast to the Nash-VI algorithm where both agents compute Nash policies simultaneously, the new type of algorithm treats two agents asymmetrically. It picks one agent as the main (learning) agent, while letting the other agent be the exploiter. During training, the main agent tries to learn the Nash policy, while the exploiter keeps playing the best response to the current policies of the main agents. That is, the exploiter facilitates the learning of the main player by deliberately exploiting her weakness. The resulting algorithm after combining VI with global optimism and exploiter is the algorithm Golf_with_Exploiter (Jin et al. 2021b). The following result shows that this algorithm is capable of sample-efficient learning if the function class has a low multiagent Bellman Eluder (BE) dimension—a complexity measure adapted from its single-agent version (Jin et al. 2021a). Theorem (Jin et al. 2021b): For any zero-sum MG with horizon length $H$ equipped with a Q-function class $\cF$ whose multi-agent Bellman Eluder dimension is \(\tO(d)\), Golf_with_Exploiter algorithm learns an $\eps$-Nash within \(\tO(H^2d\log(\vert \cF\vert)/\eps^2)\) episodes of play. The concurrent work of Huang et al. 2021 also establish a \(\tO(1/\eps^2)\) sample complexity guarantee for zero-sum MG with general function approximation, under the slightly different complexity measure of minimax Eluder dimension. 7. Other research frontiers; End note In this blog post, we presented a brief overview of recent advances in multi-agent RL theory. We presented several learning goals (Nash, CE, CCE), planning and sample-efficient learning algorithms for these learning goals in two tabular settings (two-player zero-sum MGs, multi-player general-sum MGs), and touched on the related topic of function approximation. Apart from the aforementioned questions, there are many other active research topics in MARL theory we have not covered in this blog post: Further design and analysis of decentralized algorithms. Policy optimization algorithms for Markov Games. Other notions of equilibria (e.g. Stackelberg equilibria). Markov potential games. Imperfect-information games. Similar to the topics covered in our earlier sections, the above topics also admit unique theoretical challenges brought by multiple agents that are not present in single-agent settings. We believe that tackling these challenges will provide many exciting opportunities for theory research down the road. (Last edited: May 20, 2022)" />
<meta property="og:description" content="Reinforcement learning (RL) has made substantial empirical progresses in solving hard AI challenges in the past few years. A big portion of these progresses—Go, Dota 2, Starcraft, economic simulation, social behavior learning, and so on—come from multi-agent RL, that is, sequential decision making involving more than one agents. While the theoretical study of (single-agent) RL has a long history and a vastly growing recent interest, multi-agent RL theory is arguably a newer and less developed field, with its own unique challenges and opportunities that we feel very excited about. In this extended blog post, we present a brief overview of the basics of multi-agent RL theory, along with some recent theoretical developments in the past few years. We will focus on learning Markov games, and cover the basic formulations, learning goals, planning algorithms, as well as recent advances in sample-efficient learning algorithms under the interactive (exploration) setting. The majority of this blog post will assume familiarity with the basics of RL theory in the single-agent setting (for example, materials in this fantastic book). We will focus on “what is different” when it comes to multi-agent, and discuss the various recent developments and opportunities therein. \(\def\cS{\mathcal S}\) \(\def\cA{\mathcal A}\) \(\def\cF{\mathcal F}\) \(\def\mc{\mathcal}\) \(\def\P{\mathbb P}\) \(\def\E{\mathbb E}\) \(\def\V{\mathbb V}\) \(\def\R{\mathbb R}\) \(\def\tO{\widetilde{\mathcal{O}}}\) \(\def\eps{\varepsilon}\) \(\def\epsilon{\varepsilon}\) \(\def\setto{\leftarrow}\) \(\def\up{\overline}\) \(\def\low{\underline}\) \(\def\what{\widehat}\) \(\def\ba{\boldsymbol{a}}\) 1. Formulation: Markov Games There are various formulations for multi-agent RL. In this blog post, we will focus on Markov Games (MG; Shapley 1953, Littman 1994), a generalization of the widely-used Markov Decision Process (MDP) framework into the case of multiple agents. We remark that there exist various other frameworks for modeling multi-agent sequential decision making, such as extensive-form games, which can be considered as Markov games with special (tree-like) structures in transition dynamics; when combined with imperfect information, this formulation is more suitable for modeling games such as Poker. Roughly speaking, a Markov Game has the same (state, action) -&gt; (reward, next state) structure as in an MDP, except that now “action” is replaced by the joint actions of all players, and “reward” is replaced by a collection of reward functions so that each player has her own reward. In words: Markov Games with m players: (state, m actions) -&gt; (m rewards, next state). Formally, we consider a multi-player general-sum Markov game with $m$ players, which can be described as a tuple \(\text{MG}(H, \mathcal{S}, \{\mathcal{A}_{i}\}_{i=1}^{m}, \mathbb{P},\{r_{i}\}_{i=1}^{m})\), where $H$ is the horizon length, $\mathcal{S}$ is the state space with $\vert \mathcal{S}\vert=S$. \(\mathcal{A}_i\) is the action space for the $i$-th player with \(\vert\mathcal{A}_i\vert=A_i\). All players act simultaneously; we use \(\ba=(a_1,\dots,a_m)\in \prod_{i=1}^m \mathcal{A}_i\) to denote a joint action, \(\mathbb{P}=\{\mathbb{P}_h\}\) is the collection of transition probabilities, where \(\mathbb{P}_h(s_{h+1} \vert s_h, \ba_h)\) is the probability of transiting to the next state \(s_{h+1}\), given state-action pair $(s_h, \ba_h)$ at step $h$, \(r_i=\{r_{i,h}\}\) is the reward function for the \(i\)-th player. At step $h$, the $i$-th player will experience reward $r_{i,h}(s_h, \ba_h)\in[0, 1]$. For simplicity of presentation, we assume the reward is deterministic. A trajectory of an MG looks like this: [ s_1, (a_{1, 1}, \dots, a_{m, 1}), (r_{1,1},\dots, r_{m, 1}), s_2, \dots, s_H, (a_{1, H}, \dots, a_{m, H}), (r_{1,H}, \dots, r_{m,H}). ] Finally, a (Markov) policy for the $i$-th player is denoted by \(\pi_i=\{\pi_{i,h}\}_{h=1}^H\), where \(\pi_{i,h}(a_{i,h}\vert s_h)\) is the probability of taking action $a_{i,h}$ at step $h$ and state $s_h$. We use \(V_{i,h}^{\pi}(s_h)\) and $Q_{i,h}^{\pi}(s_h, \ba_h)$ to denote the value and Q-value functions for player $i$ at step $h$, when joint policy $\pi$ is used. (This can either be a product policy $\pi=(\pi_1,\dots,\pi_m)$ which can be executed independently, or a correlated policy which samples the joint action $\ba_h$ for all the players together in a potentially correlated fashion.) We assume the initial state $s_1$ is deterministic, so that \(V_{i,1}^{\pi} := V_{i,1}^\pi(s_1)\) is the overall return for the $i$-th player. 1.1 Special case: Zero-sum games An important special case of Markov games is zero-sum (competitive) MGs: A zero-sum Markov Game is a Markov Game with $m=2$, and $r_1\equiv -r_2$. As each player wishes to maximize her own reward, in a zero-sum MG, the two players are in a purely competitive relationship. We thus define the overall value in a zero-sum MG to be player 1’s value function: $V_h:=V_{1,h}$. We call player 1 the max player and player 2 the min player. Zero-sum MGs model a number of real-world two-player strategic games, such as Go, Chess, Shogi, etc. From a mathematical perspective, zero-sum MGs is perhaps the most immediate extension of a single-agent MDP—solving it can be thought of as a certain min-max optimization, akin to how solving a single-agent MDP is a certain maximization problem. 2. Nash equilibrium In an MDP, the learning goal for an agent is to maximize her own cumulative reward. In an MG, this is still the learning goal for each individual agent, but we have to additionally decide on some notion of equilibrium to combine these individual optimality conditions. Throughout the majority of this blog post, we consider the celebrated notion of Nash equilibrium (Nash 1951), which states that for each player $i$, fixing all other player’s policies, her own policy is a best response (i.e. maximizes her own cumulative reward): Nash equilibrium: Each player plays the best response to all other player’s policies. It is clear that the Nash equilibrium strictly generalizes the notion of an optimal policy for MDPs—An MG reduces to an MDP if player 1 acts and all other players are dummy (in the sense their actions do not affect transition and rewards). In this case, the best response of player 1 reduces to maximizing her own reward with respect to the environment (MDP) only. Formally, we define an $\eps$-approximate Nash equilibrium (henceforth also $\eps$-Nash) as follows. Below, $\pi_{-i}$ denotes the collection of all but the $i$-th player’s policy within $\pi$. Definition: A product policy $\pi=(\pi_1,\dots, \pi_m)$ is an $\eps$-approximate Nash equilibrium, if we have [ \max_{i\in[m]} \big( \max_{\pi_i’} V_{i,1}^ {\pi_i’, \pi_{-i}}- V_{i,1}^{\pi_i, \pi_{-i}} \big) \le \eps. ] We say $\pi$ is an (exact) Nash equilibrium if the above holds with $\eps=0$. The Nash equilibrium need not be unique for (general-sum) MGs. Also, note that by definition any Nash policy must be a product policy (i.e. all the players execute their own policy independently). There exist other proper notions of equilibria when players do play in a correlated fashion; we will dive deeper into this in Section 5. 2.1 Nash equilibrium in zero-sum games The Nash equilibrium enjoys additional nice properties in zero-sum MGs. Recall that $V_1^{\pi_1, \pi_2}$ denotes player 1’s value for which $\pi_1$ seeks to maximize and $\pi_2$ seeks to minimize. Proposition: For any zero-sum MG, we have (Unique Nash value) There exists a $V_1^{\star}\in[0,H]$ such that all Nash equilibrium $(\pi_1^\star, \pi_2^\star)$ achieves this same value: \(V_1^{\pi_1^\star, \pi_2^\star} = V_1^{\star},\) even though the Nash policy $(\pi_1^\star, \pi_2^\star)$ may not be unique. (Strong duality) Any Nash equilibrium is also a min-max solution and a max-min solution. In other words, the following two inequalities (which holds for any pair of $(\pi_1, \pi_2)$) \[ \left\{ \begin{aligned} &amp; \max_{\pi_1&#39;} V_1^{\pi_1&#39;, \pi_2} \ge \min_{\pi_2&#39;} \max_{\pi_1&#39;} V_1^{\pi_1&#39;, \pi_2&#39;} \ge \max_{\pi_1&#39;} \min_{\pi_2&#39;} V_1^{\pi_1&#39;, \pi_2&#39;} \ge \min_{\pi_2&#39;} V_1^{\pi_1, \pi_2&#39;} \\ &amp; \max_{\pi_1&#39;} V_1^{\pi_1&#39;, \pi_2} \ge V_1^{\pi_1, \pi_2} \ge \min_{\pi_2&#39;} V_1^{\pi_1, \pi_2&#39;} \end{aligned} \right. \] become equalities ($=V_1^{\star}$) if $(\pi_1, \pi_2)$ is a Nash equilibrium. These nice properties make zero-sum MGs an appealing setting for learning Nash equilibria, which will be the focus of our next two sections. 3. Planning algorithms in zero-sum MGs Given the above setup, a natural first question is: How do we compute the Nash equilibrium in zero-sum MGs assuming known transitions and rewards? In other words, what is a good planning algorithm for learning Nash equilibria? It turns out that a direct multi-agent adaptation of Value Iteration, dating back to (Shapley 1953), gives an algorithm for computing the Nash equilibrium for a known game. We term this algorithm as Nash-VI (Nash Value Iteration). Similar to how Value Iteration (VI) computes the optimal values and policies for an MDP using backward dynamical programming, Nash-VI performs similar backward updates on an MG except for the policy selection mechanism: the maximization over the vector $Q^\star(s, \cdot)$ in VI is now replaced by a \(\text{MatrixNash}\) subroutine over the matrix $Q^{\star}(s, \cdot, \cdot)$. This makes sense given that the learning goal is now learning a Nash equilibrium policy over the two players jointly, instead of the reward maximizing policy for a single player. In words, Nash-VI = Value Iteration with \(\text{Max}(\cdot)\) replaced by \(\text{MatrixNash}(\cdot)\). Algorithm (Nash-VI): Initialize \(V_{H+1}(s)\equiv 0\) for all \(s\in\cS\). For $h=H,\dots,1$, compute the following over $s\in\cS$ and all $(a_1, a_2)\in\cA_1\times \cA_2$: \[ \begin{aligned} &amp; Q_h^{\star}(s, a_1, a_2) = r_h(s, a_1, a_2) + (\P_h V_{h+1}^\star)(s, a_1, a_2), \\\ &amp; (\pi_{1,h}^\star(\cdot\vert s), \pi_{2,h}^\star(\cdot\vert s)) = \text{MatrixNash}(Q_h^{\star}(s,\cdot,\cdot)), \\\ &amp; V^{\star}_h(s) = \pi_{1,h}^\star(\cdot \vert s)^\top Q_h(s,\cdot,\cdot) \pi_{2,h}^\star(\cdot \vert s). \end{aligned} \] Return policies $(\pi_1^\star, \pi_2^\star)$ where \(\pi_i^\star=\{\pi_{i,h}^\star(\cdot\vert s)\}_{(h,s)\in[H]\times \cS}\). Above, the \(\text{MatrixNash}\) subroutine for any matrix \(M\in \R^{A_1\times A_2}\) is defined as \[ \begin{align}\label{equation:matrixnash} \text{MatrixNash}(M) := \arg \big( \max_{\pi_1 \in \Delta_{\cA_1}} \min_{\pi_2 \in \Delta_{\cA_2}} \pi_1^\top M \pi_2 \big), \end{align} \] which can be implemented efficiently using linear programming. Here $\Delta_{\cA_1}$ and $\Delta_{\cA_2}$ are the probability simplex over action sets $\cA_1$ and $\cA_2$ respectively. Using backward induction over $h$, it is straightforward to show that the policies $(\pi_1^\star, \pi_2^\star)$ returned by Nash-VI is indeed a Nash equilibrium of the desired MG. The $V_h^{\star}(s_h)$ and $Q_h^{\star}(s,a_1,a_2)$ computed above are uniquely defined; we call them Nash values of the game at state $s$ and state-action tuple $(s,a_1,a_2)$ respectively. Value Iteration is not the only algorithm for computing Nash. As an alternative, the Nash Q-Learning algorithm (Hu &amp; Wellman 2003) performs incremental updates on the Q values, and uses the same matrix Nash subroutine to compute (Nash) V values from the Q values. Further, both Nash-VI and Nash Q-learning can be adapted when the game transitions and rewards are not known and have to be estimated from samples. For example, using random samples from a simulator (can query any $(s_h, a_{h,1}, a_{h,2})$ and obtain a sample of $(r_h, s_{h+1})$), an $\eps$-Nash can be learned with $\tO(H^3SA_1A_2/\eps^2)$ samples with high probability, using variants of either Nash-VI (Zhang et al. 2020) or Nash Q-learning (Sidford et al. 2019). Centralization. Finally, we remark that both Nash-VI and Nash Q-learning are centralized training algorithms—that is, the computation for the two players are coupled (due to the joint computation in the matrix Nash subroutine). In order to execute these algorithms, the two players could not act in isolation, and have to coordinate or even live in a same centralized machine (self-play). In Section 4.2, we will present another decentralized algorithm, which is further capable of improving the dependency of sample complexity on the number of actions from \(\tO(A_1A_2)\) to \(\tO(\max\{A_1, A_2\})\). 4. Sample-efficient learning of Nash equilibria in zero-sum MGs We now move on to the more challenging setting of learning Nash equilibria from interactive learning environments. Rather than assuming full knowledge or simulators of the game, we now assume that the algorithm can only learn about the game by playing repeated episodes of the game in a trial-and-error fashion (also known as the interactive/exploration setting). This requires the algorithms to further address the challenge of balancing exploration and exploitation. Our goal is to design algorithms that are sample-efficient, i.e. can learn an $\eps$-Nash with as few episodes of play as possible. 4.1 Optimistic Nash-VI Our first algorithm is an optimistic variant of the Nash-VI algorithm we have seen in Section 3. This algorithm first appeared in (Liu et al. 2020), and builds on earlier sample-efficient Nash-VI type algorithms within (Bai &amp; Jin 2020, Xie et al. 2020). Algorithm (Optimistic Nash-VI, sketch): Initialize \(\up{Q}_h(s, a_1, a_2)\setto H\) and \(\low{Q}_h(s, a_1, a_2)\setto 0\) for all \((s, a_1, a_2, h)\). For episode $k=1,\dots,K$: For step $h=H,\dots,1$: For all $(s, a_1, a_2)\in\cS\times \cA_1\times \cA_2$: Set $t\setto N_h(s, a_1, a_2)$ (visitation count). If $t&gt;0$ then Compute bonus \(\beta\setto \text{BernsteinBonus}(t, \what{\V}_h[(\up{V}_{h+1} + \low{V}_{h+1})/2] (s, a_1, a_2)\). Compute additional bonus \(\gamma\setto (c/H) \what{\P}_h(\up{V}_{h+1} - \low{V}_{h+1})(s, a_1, a_2)\). \(\up{Q}_h(s, a_1, a_2)\setto \min\{ (r_h + \what{P}_h\up{V}_{h+1})(s, a_1, a_2) + \beta + \gamma, H \}\). \(\low{Q}_h(s, a_1, a_2)\setto \max\{ (r_h + \what{P}_h\low{V}_{h+1})(s, a_1, a_2) - \beta - \gamma, 0 \}\). For all $s\in \cS$: \(\pi_h(\cdot, \cdot \vert s) \setto \text{MatrixCCE}(\up{Q}_h(s, \cdot, \cdot), \low{Q}_h(s, \cdot, \cdot))\). \(\up{V}_h(s) \setto \langle \pi_h(\cdot, \cdot \vert s), \up{Q}_h(s, \cdot, \cdot) \rangle\). \(\low{V}_h(s) \setto \langle \pi_h(\cdot, \cdot \vert s), \low{Q}_h(s, \cdot, \cdot) \rangle\). Play an episode using (correlated) policy $\pi$, and update the empirical models $\what{\P}_h$, $N_h$. Set $(\pi_1^k, \pi_2^k)$ to be the marginal policy of the current policy $\pi$. Main techniques. Optimistic Nash-VI enhances the original Nash-VI in two main aspects in order to perform sample-efficient exploration in an interactive learning setting: The algorithm maintains two optimistic Q estimates: An upper estimate $\up{Q}_h$, and a lower estimate $\low{Q}_h$. The amount of optimisticity is governed by the two bonus functions $\beta$ and $\gamma$, where $\beta$ is similar to the usual Bernstein bonus for exploration in single-agent settings (Azar et al. 2017, Jin et al. 2018), and $\gamma$ is a specifically designed model-based bonus that allows a tighter analysis, akin to the one used in (Dann et al. 2018). We remark that different from the single-agent setting, the introduction of $\gamma$ is key in achieving $\mathcal{O}(S)$ sample complexity instead of $\mathcal{O}(S^2)$. The policy is now computed using a matrix CCE subroutine. The use of matrix CCE in the context of learning Markov game is first introduced by (Xie et al. 2020). For any pair of matrices $\up{M}, \low{M}\in\R^{A_1\times A_2}$, $ \text{MatrixCCE}(\up{M}, \low{M})$ is defined as any correlated policy $\pi \in \Delta_{\cA_1\times \cA_2}$ such that the following holds: \[ \begin{aligned} &amp; \E_{(a_1, a_2)\sim \pi} [\up{M}(a_1, a_2)] \ge \max_{a_1&#39;\in \cA_1} \E_{(a_1, a_2)\sim \pi}[\up{M}(a_1&#39;, a_2)], \\ &amp; \E_{(a_1, a_2)\sim \pi} [\low{M}(a_1, a_2)] \le \min_{a_2&#39;\in \cA_2} \E_{(a_1, a_2)\sim \pi}[\low{M}(a_1, a_2&#39;)]. \end{aligned} \] In other words, if the two players jointly play $\pi$, the max player has no gain in deviating for the payoff matrix $\up{M}$, and the min player has no gain in deviating for the payoff matrix $\low{M}$. The purpose of this matrix CCE subroutine is to find a good policy with respect to the upper / lower estimates in a computationally efficient fashion (this subroutine can be implemented using linear programming). Indeed, here a general-sum matrix Nash subroutine with respect to $(\up{Q}_h(s,\cdot,\cdot), \low{Q}_h(s,\cdot,\cdot))$ may as well be used (and leads to the same sample complexity results), yet would suffer from computational difficulties for large $A_1,A_2$, as finding Nash in a two-player general-sum matrix game is PPAD hard (Daskalakis et al. 2013). These techniques enable the optimistic Nash-VI algorithm to achieve the following sample complexity guarantee for learning Nash equilibria in zero-sum MGs. Theorem (Liu et al. 2020): With high probability, optimistic Nash-VI outputs an $\eps$-Nash within $K=\tO(H^3SA_1A_2/\eps^2)$ episodes of play. Compared with the minimax sample complexity $H^3SA/\eps^2$ for single-agent MDPs achieved by the UCBVI algorithm (Azar et al. 2017, with matching lower bound in (Jin et al. 2018, Domingues et al. 2021)), the sample complexity achieved by Nash-VI for zero-sum MGs only pays an additional $A_1A_2$, i.e. product of the action space. This may seem natural at first sight as the joint action space in a zero-sum MG does have size $A_1A_2$. However, as we will see in the next subsection, it is possible to design a decentralized learning algorithm that achieves a \(\max\{A_1,A_2\}\) dependence on the action spaces. Centralized training, decentralized execution. Note that the $\pi$ returned by the matrix CCE subroutines and used in the exploration step is a correlated policy. Therefore, the entire algorithm has to be executed in a centralized fashion. However, the final output is the marginals of this correlated policy and thus is an (approximately Nash) product policy that can be deployed in a decentralized way. This is reminiscent of the “centralized training, decentralized execution” paradigm which is also commonly used in empirical multi-agent RL. 4.2 V-Learning Our second algorithm is a recently proposed algorithm, V-Learning, which first appeared in (Bai et al. 2020) in the name of Nash V-Learning. Here, we present the simplified version of the algorithm in its journal version (Jin et al. 2021c). V-Learning itself can be viewed as a single-agent RL algorithm. To run V-learning in the multiagent setting, we simply let each player run the single-agent V-learning entirely on her own as long as she observes the states, rewards, and next states (as if it were a single-agent MDP). The opponent’s action does not need to be observed. Therefore, V-learning is naturally a decentralized algorithm. We present the algorithm as follows. Algorithm (V-Learning): For all $(h, s, a)\in [H]\times \cS\times \cA$, initialize \(V_h(s)\setto H-h+1\), \(\pi_{h}(a \vert s)\setto 1/A\), and $N_h(s)\setto 0$. For episode $k=1,\dots,K$: Receive $s_1$. For $h=1,\dots,H$: Take action $a_{h}\sim \pi_{h}(\cdot \vert s_h)$, observe reward $r_h$ and next state $s_{h+1}$. (No need to observe the opponent’s action.) $t=N_h(s_h)\setto N_h(s_h)+1$. \(V_{h}(s_h)\setto (1-\alpha_t) V_h(s_h) + \alpha_t (r_h + \min\{V_{h+1}(s_{h+1}), H-h\} + \beta_t)\). \(\pi_{h}(\cdot \vert s_h) \setto \text{Adv_Bandit_Update}(a_{h}, \frac{H-r_h-\min\{V_{h+1}(s_{h+1}), H-h\} }{H})\) on the $(s_h, h)$-th adversarial bandit. Algorithm (Executing output policy of V-Learning): Sample $k\setto\text{Unif}([K])$. For step $h=1,\dots,H$: Observe $s_h$, and set $t\setto N_h^k(s_h)$. Set $k\setto k_h^i(s_h)$, where $k_h^1(s_h)&lt;\dots&lt;k_h^t(s_h)$ are the indices of all past visitations to $(h, s_h)$ prior to episode $k$, and $i\in [t]$ is sampled randomly with probability $\alpha_t^i$. Take action $a_{h}\sim \pi_{h}^k(\cdot\vert s_h)$. The structure of the V-Learning algorithm is rather distinct from either Nash-VI or Nash Q-learning (perhaps its closest relative among classical RL algorithms). We briefly explain its three main components: Incremental update of $V$: This part is similar to Q-learning, except that we do not model the Q functions, but rather directly model the V functions (hence the name V-learning). Consequently, this update avoids constructing Q functions in the multiagent setting (which is of the size $\cA_1 \cA_2$). It also “ignores” the actions taken by the opponent. The incremental update uses a step-size sequence \(\{\alpha_t\}_{t\ge 0}\). We choose \(\alpha_t=(H+1)/(H+t)\) following the analysis of Q-learning in (Jin et al. 2018). Adversarial bandit subroutine: Recall that in Q-learning (or its optimistic version), the greedy (argmax) rule over the estimated Q function is used at each $(s,h)$ to determine the policy. In V-learning, the policy at each $(s_h, h)$ is rather maintained by an adversarial bandit subroutine (there are $SH$ such adversarial bandits in total). Concretely, after $(s_h, h)$ is encountered and $(r_h, s_{h+1})$ are received, the corresponding adversarial bandit performs an update step of the form “Action $a_{h}\in\cA$ suffered loss \(\frac{H-r_h-\min\{V_{h+1}(s_{h+1}), H-h\} }{H}\in [0,1]\).” The adversarial bandit subroutine is then responsible for determining how to actually compute the new policy given this update. In V-Learning, we instantiate the adversarial bandit as a weighted follow-the-regularized-leader algorithm, in order to achieve a per-state weighted regret guarantee of a certain form, at each $(s, h)$. Output policy: The output policy of V-learning is a nested mixture policy: A random integer $k$ is updated by random sampling throughout the execution of this policy, and at each $h$ this $k$ determines a particular policy for drawing the action $a_{h}$. This makes the policy non-Markov in general, which is drastically different from Nash-VI as well as most classical algorithms in single-agent RL which outputs Markov policies. Technically, such an output policy is critically required in order to extract from the per-state “regret” guarantees at each $(s,h)$ a policy that enjoys an overall near-Nash guarantee on the overall game value. (Hence its original name “certified policy” in (Bai et al. 2020) as this policy “certifies” the value functions appearing in the per-state regret bounds.) Here, the sampling probabilities $\alpha_t^i$ are defined as [ \alpha_t^0 = \prod_{j=1}^t (1-\alpha_j),~~~\alpha_t^i = \alpha_i \prod_{j=i+1}^t (1-\alpha_j). ] The above techniques lead to the following guarantee on V-Learning for zero-sum MGs. Theorem (Bai et al. 2020, Jin et al. 2021c): Suppose both players run the V-Learning algorithm with the \(\text{Adv_Bandit_Update}\) instantiated as a suitable weighted Follow-The-Regularized-Leader algorithm for each $(s,h)$. Then, running this for $K$ episodes, (the product of) their certified output policies $(\what{\pi}_1, \what{\pi}_2)$ is an $\epsilon$-Nash as long as \(K\ge \tO(H^5S\max\{A_1, A_2\}/\eps^2)\). Compared with optimistic Nash-VI, Nash V-Learning achieves milder dependence on the action space (\(\max\{A_1,A_2\}\) vs. $A_1A_2$) thanks to its decentralized nature, at the cost of worse $H$ factors. We remark that the original sample complexity presented in (Bai et al. 2020) is \(\tO(H^6S\max\{A_1,A_2\}/\eps^2)\), and one $H$ factor can be improved by adopting the sharper analysis in (Tian et al. 2020). 4.3 Summary of algorithms The table below summarizes the algorithms we have introduced so far for learning $\eps$-Nash in zero-sum MGs. Note that here Nash Q-Learning (and its optimistic version) was not presented in this blog post; the actual algorithm and theoretical guarantee can be found in Section 3 of (Bai et al. 2020). Algorithm Training Update Main estimand Sample complexity Nash-VI centralized model-based \(\P_h(s&#39;\vert s,a_1,a_2)\) \(\tO(H^3SA_1A_2/\eps^2)\) Nash Q-Learning centralized model-free \(Q_h^{\star}(s,a_1,a_2)\) \(\tO(H^5SA_1A_2/\eps^2)\) V-Learning decentralized model-free \(V_h^{\star}(s)\) \(\tO(H^5S\max\{A_1, A_2\}/\eps^2)\) 5. General-sum MGs with many players We now shift attention to the more general case of general-sum MGs with $m$ players for any $m\ge 2$. For general-sum MGs, we are not only interested in learning near-Nash product policies, but also the broader class of correlated equilibria for correlated policies, which we define as follows. 5.1 Correlated Equilibria (CE) and Coarse Correlated Equilibria (CCE) Definition (Coarse Correlated Equilibrium): A correlated policy $\pi$ is an $\eps$-Coarse Correlated Equilibrium (CCE), if any player could not improve her own reward by more than $\eps$ by deviating from $\pi$ and playing some other policy on her own: [ \max_{i\in[m]} \max_{\pi_i’} \Big( V_{i,1}^{\pi_i’, \pi_{-i}} - V_{i,1}^{\pi} \Big) \le \eps. ] Definition (Correlated Equilibrium; Informal): A correlated policy $\pi$ is an $\eps$-Correlated Equilibrium (CE), if any player could not improve her own reward by more than $\eps$ by first observing her own action sampled from the correlated policy at each state, then deviating to some other action: [ \max_{i\in[m]} \max_{\phi\in \Phi_i} \Big( V_{i,1}^{\phi\diamond \pi} - V_{i,1}^{\pi} \Big) \le \eps. ] Above, $\Phi_i$ is the set of all possible strategy modification functions for player $i$ (for a formal definition see (Liu et al. 2020)). In general, we have {Nash}$\subset${CE}$\subset${CCE}. Therefore, CE, CCE can be thought of as relaxations of Nash and make sense as learning goals on their own right as general-sum Nash suffers from PPAD hardness in the computational efficiency anyway (Daskalakis et al. 2013). We remark that similar as Nash, the notions of CE and CCE are standard and widely studied in the game theory literature, with many real-world implications (see e.g. this book &amp; Wikipedia). The above definitions are direct extensions of the original definitions (for strategic games) into Markov games. 5.2 Value Iteration algorithms for Nash, CE, and CCE The model-based optimistic Nash-VI algorithm can be adapted straightforwardly to the case of multiple players, by simply replacing the MatrixCCE subroutine in Optimistic Nash-VI algorithm with (multi-dimensional) matrix {Nash,CE,CCE} subroutines. This gives the following result for learning equilibria in general-sum MGs. Theorem (Liu et al. 2020): A multi-player adaptation of the optimistic Nash-VI algorithm coupled with (multi-dimensional) matrix {Nash,CE,CCE} subroutines can learn an $\eps$-{Nash, CE, CCE} of a general-sum MG within \(\tO(H^4S^2\prod_{i\le m} A_i/\eps^2)\) episodes of play. The conceptual advantage of this algorithm is that it estimates the full transition model $\P_h(s_{h+1} \vert s_h, \ba_h)$ directly; it can then learn all three kinds of equilibria by plugging in the corresponding (multi-dimensional) matrix subroutines. However, this algorithm suffers from the curse of multiagents: the sample complexity scales as $\prod_{i\le m} A_i$, which is not surprising given that it estimates the full transition model. Also note that, despite the sample efficiency, the multi-player matrix Nash subroutine involved in the above algorithm does not likely admit a poly time implementation due to the PPAD hardness in computing general-sum Nash. 5.3 V-Learning for CE and CCE Recall that V-Learning is a decentralized algorithm that can be deployed by each player independently, and for zero-sum MGs one could extract from its execution history a suitable output policy that is near Nash. It turns out that this paradigm generalizes nicely to the problems of learning CE and CCE in general-sum MGs. Specifically, recent concurrent works show that V-Learning algorithms instantiated with suitable adversarial bandit subroutines could learn CE/CCE sample-efficiently, without suffering from the curse of multiagents: Theorem (Song et al. 2021 &amp; Jin et al. 2021c) For $m$-player general-sum MGs, suppose each player run the V-Learning algorithm independently with suitably chosen adversarial bandit subroutines, and we extract a correlated output policy in a suitable fashion. Then: Instantiating the adversarial bandit subroutine as a regret minimizer (CCE-V-Learning), the correlated output policy is an $\eps$-CCE within \(\tO(H^5S\max_{i\le m} A_i/\eps^2)\) episodes of play; Instantiating the adversarial bandit subroutine as a swap regret minimizer (CE-V-Learning), the correlated output policy is an $\eps$-CE within \(\tO(H^5S(\max_{i\le m} A_i)^2/\eps^2)\) episodes of play. Compared with the VI algorithm from the last section, V-Learning breaks the curse of multiagents as the sample complexity scales polynomially in $\max_{i\le m}A_i$, as opposed to $\prod_{i\le m} A_i$ which is exponential in $m$. This is thanks to the decentralized learning structure of the V-Learning algorithm. (Bib note: We remark that Song et al. 2021 actually achieves $\tO(H^6S(\max_{i\le m}A_i)^2/\eps^2)$ for learning CE, which is one $H$ factor worse than Jin et al. 2021c due to using a slightly different swap regret minimizer. The concurrent work of Mao &amp; Basar 2021 also achieves a $\tO(H^6S\max_{i\le m}A_i/\eps^2)$ result for learning $\eps$-CCE.) 6. Learning MGs with function approximation In single-agent MDPs, there is a rich body of work on the theory of function approximation—that is, sample-efficient learning in large state/action spaces equipped with a restricted function class. Here we briefly review some recent advances on function approximation in Markov Games. Throughout this section, we shift back to considering two-player zero-sum MGs. 6.1 Linear function approximation Similar as a linear MDP, a (zero-sum) linear MG is a Markov Game whose transitions and rewards satisfy the following linear structure: \[ \begin{aligned} &amp; \P_h(s&#39; | s, a_1, a_2) = \langle \phi(s, a_1, a_2), \mu_h(s&#39;) \rangle, \\ &amp; r_h(s, a_1, a_2) = \langle \phi(s, a_1, a_2), \theta_h \rangle, \end{aligned} \] where $\phi:\cS\times \cA_1\times \cA_2\to \R^d$ is a known feature map and $\mu_h$, $\theta_h$ are (unknown) parameters for the transitions and rewards. The following result shows that linear MGs can be learned sample-efficiently with mild polynomial dependencies on $d,H$, similar as in linear MDPs. Their algorithm is similar to the optimistic Nash-VI algorithm, and further builds upon the single-agent optimistic least-squares value iteration algorithm (Jin et al. 2020) to utilize the linear structure in the transitions and rewards, by using a bonus function similar to linear bandits. To adapt this algorithm into the multi-agent setting, their algorithm again replaces the maximization over actions by matrix CCE subroutine, and uses both the upper bounds and lower bounds. Theorem (Xie et al. 2020): For linear zero-sum MG with horizon length $H$ and feature dimension $d$, there exists an algorithm that learns an \(\eps\)-Nash within \(\tO(d^3H^4/\eps^2)\) episodes of play. We remark that Chen et al. 2021 consider the related model of linear mixture MGs and design an algorithm with episode complexity $\tO(d^2H^3/\eps^2)$. 6.2 General function approximation; The power of exploiter The above sample complexity results are based on a direct adaptation of Nash VI into the linear setting. Such an approach critically depends on a restrictive assumption called optimistic closure, which is true for tabular MGs and linear MGs, but rarely holds in general function approximation. To remove such an assumption, in the single-agent setting, we can modify the standard optimistic VI algorithm which computes an upper bound of value functions at each step, to a globally optimistic version which only computes the upper bound of value functions at the first step. The latter approach considers confidence sets at all steps simultaneously, thus providing a much sharper optimistic value, and allowing the algorithm to learn a near-optimal policy sample-efficiently (Zanette et al. 2020, Jin et al. 2021a). To adapt such an idea to the multi-agent setting, it turns out that we need one more technique—the use of an exploiter. In contrast to the Nash-VI algorithm where both agents compute Nash policies simultaneously, the new type of algorithm treats two agents asymmetrically. It picks one agent as the main (learning) agent, while letting the other agent be the exploiter. During training, the main agent tries to learn the Nash policy, while the exploiter keeps playing the best response to the current policies of the main agents. That is, the exploiter facilitates the learning of the main player by deliberately exploiting her weakness. The resulting algorithm after combining VI with global optimism and exploiter is the algorithm Golf_with_Exploiter (Jin et al. 2021b). The following result shows that this algorithm is capable of sample-efficient learning if the function class has a low multiagent Bellman Eluder (BE) dimension—a complexity measure adapted from its single-agent version (Jin et al. 2021a). Theorem (Jin et al. 2021b): For any zero-sum MG with horizon length $H$ equipped with a Q-function class $\cF$ whose multi-agent Bellman Eluder dimension is \(\tO(d)\), Golf_with_Exploiter algorithm learns an $\eps$-Nash within \(\tO(H^2d\log(\vert \cF\vert)/\eps^2)\) episodes of play. The concurrent work of Huang et al. 2021 also establish a \(\tO(1/\eps^2)\) sample complexity guarantee for zero-sum MG with general function approximation, under the slightly different complexity measure of minimax Eluder dimension. 7. Other research frontiers; End note In this blog post, we presented a brief overview of recent advances in multi-agent RL theory. We presented several learning goals (Nash, CE, CCE), planning and sample-efficient learning algorithms for these learning goals in two tabular settings (two-player zero-sum MGs, multi-player general-sum MGs), and touched on the related topic of function approximation. Apart from the aforementioned questions, there are many other active research topics in MARL theory we have not covered in this blog post: Further design and analysis of decentralized algorithms. Policy optimization algorithms for Markov Games. Other notions of equilibria (e.g. Stackelberg equilibria). Markov potential games. Imperfect-information games. Similar to the topics covered in our earlier sections, the above topics also admit unique theoretical challenges brought by multiple agents that are not present in single-agent settings. We believe that tackling these challenges will provide many exciting opportunities for theory research down the road. (Last edited: May 20, 2022)" />
<link rel="canonical" href="/blog/marl_theory.html" />
<meta property="og:url" content="/blog/marl_theory.html" />
<meta property="og:site_name" content="MARL Theory" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-11-14T09:00:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Recent Progresses in Multi-Agent RL Theory" />
<script type="application/ld+json">
{"description":"Reinforcement learning (RL) has made substantial empirical progresses in solving hard AI challenges in the past few years. A big portion of these progresses—Go, Dota 2, Starcraft, economic simulation, social behavior learning, and so on—come from multi-agent RL, that is, sequential decision making involving more than one agents. While the theoretical study of (single-agent) RL has a long history and a vastly growing recent interest, multi-agent RL theory is arguably a newer and less developed field, with its own unique challenges and opportunities that we feel very excited about. In this extended blog post, we present a brief overview of the basics of multi-agent RL theory, along with some recent theoretical developments in the past few years. We will focus on learning Markov games, and cover the basic formulations, learning goals, planning algorithms, as well as recent advances in sample-efficient learning algorithms under the interactive (exploration) setting. The majority of this blog post will assume familiarity with the basics of RL theory in the single-agent setting (for example, materials in this fantastic book). We will focus on “what is different” when it comes to multi-agent, and discuss the various recent developments and opportunities therein. \\(\\def\\cS{\\mathcal S}\\) \\(\\def\\cA{\\mathcal A}\\) \\(\\def\\cF{\\mathcal F}\\) \\(\\def\\mc{\\mathcal}\\) \\(\\def\\P{\\mathbb P}\\) \\(\\def\\E{\\mathbb E}\\) \\(\\def\\V{\\mathbb V}\\) \\(\\def\\R{\\mathbb R}\\) \\(\\def\\tO{\\widetilde{\\mathcal{O}}}\\) \\(\\def\\eps{\\varepsilon}\\) \\(\\def\\epsilon{\\varepsilon}\\) \\(\\def\\setto{\\leftarrow}\\) \\(\\def\\up{\\overline}\\) \\(\\def\\low{\\underline}\\) \\(\\def\\what{\\widehat}\\) \\(\\def\\ba{\\boldsymbol{a}}\\) 1. Formulation: Markov Games There are various formulations for multi-agent RL. In this blog post, we will focus on Markov Games (MG; Shapley 1953, Littman 1994), a generalization of the widely-used Markov Decision Process (MDP) framework into the case of multiple agents. We remark that there exist various other frameworks for modeling multi-agent sequential decision making, such as extensive-form games, which can be considered as Markov games with special (tree-like) structures in transition dynamics; when combined with imperfect information, this formulation is more suitable for modeling games such as Poker. Roughly speaking, a Markov Game has the same (state, action) -&gt; (reward, next state) structure as in an MDP, except that now “action” is replaced by the joint actions of all players, and “reward” is replaced by a collection of reward functions so that each player has her own reward. In words: Markov Games with m players: (state, m actions) -&gt; (m rewards, next state). Formally, we consider a multi-player general-sum Markov game with $m$ players, which can be described as a tuple \\(\\text{MG}(H, \\mathcal{S}, \\{\\mathcal{A}_{i}\\}_{i=1}^{m}, \\mathbb{P},\\{r_{i}\\}_{i=1}^{m})\\), where $H$ is the horizon length, $\\mathcal{S}$ is the state space with $\\vert \\mathcal{S}\\vert=S$. \\(\\mathcal{A}_i\\) is the action space for the $i$-th player with \\(\\vert\\mathcal{A}_i\\vert=A_i\\). All players act simultaneously; we use \\(\\ba=(a_1,\\dots,a_m)\\in \\prod_{i=1}^m \\mathcal{A}_i\\) to denote a joint action, \\(\\mathbb{P}=\\{\\mathbb{P}_h\\}\\) is the collection of transition probabilities, where \\(\\mathbb{P}_h(s_{h+1} \\vert s_h, \\ba_h)\\) is the probability of transiting to the next state \\(s_{h+1}\\), given state-action pair $(s_h, \\ba_h)$ at step $h$, \\(r_i=\\{r_{i,h}\\}\\) is the reward function for the \\(i\\)-th player. At step $h$, the $i$-th player will experience reward $r_{i,h}(s_h, \\ba_h)\\in[0, 1]$. For simplicity of presentation, we assume the reward is deterministic. A trajectory of an MG looks like this: [ s_1, (a_{1, 1}, \\dots, a_{m, 1}), (r_{1,1},\\dots, r_{m, 1}), s_2, \\dots, s_H, (a_{1, H}, \\dots, a_{m, H}), (r_{1,H}, \\dots, r_{m,H}). ] Finally, a (Markov) policy for the $i$-th player is denoted by \\(\\pi_i=\\{\\pi_{i,h}\\}_{h=1}^H\\), where \\(\\pi_{i,h}(a_{i,h}\\vert s_h)\\) is the probability of taking action $a_{i,h}$ at step $h$ and state $s_h$. We use \\(V_{i,h}^{\\pi}(s_h)\\) and $Q_{i,h}^{\\pi}(s_h, \\ba_h)$ to denote the value and Q-value functions for player $i$ at step $h$, when joint policy $\\pi$ is used. (This can either be a product policy $\\pi=(\\pi_1,\\dots,\\pi_m)$ which can be executed independently, or a correlated policy which samples the joint action $\\ba_h$ for all the players together in a potentially correlated fashion.) We assume the initial state $s_1$ is deterministic, so that \\(V_{i,1}^{\\pi} := V_{i,1}^\\pi(s_1)\\) is the overall return for the $i$-th player. 1.1 Special case: Zero-sum games An important special case of Markov games is zero-sum (competitive) MGs: A zero-sum Markov Game is a Markov Game with $m=2$, and $r_1\\equiv -r_2$. As each player wishes to maximize her own reward, in a zero-sum MG, the two players are in a purely competitive relationship. We thus define the overall value in a zero-sum MG to be player 1’s value function: $V_h:=V_{1,h}$. We call player 1 the max player and player 2 the min player. Zero-sum MGs model a number of real-world two-player strategic games, such as Go, Chess, Shogi, etc. From a mathematical perspective, zero-sum MGs is perhaps the most immediate extension of a single-agent MDP—solving it can be thought of as a certain min-max optimization, akin to how solving a single-agent MDP is a certain maximization problem. 2. Nash equilibrium In an MDP, the learning goal for an agent is to maximize her own cumulative reward. In an MG, this is still the learning goal for each individual agent, but we have to additionally decide on some notion of equilibrium to combine these individual optimality conditions. Throughout the majority of this blog post, we consider the celebrated notion of Nash equilibrium (Nash 1951), which states that for each player $i$, fixing all other player’s policies, her own policy is a best response (i.e. maximizes her own cumulative reward): Nash equilibrium: Each player plays the best response to all other player’s policies. It is clear that the Nash equilibrium strictly generalizes the notion of an optimal policy for MDPs—An MG reduces to an MDP if player 1 acts and all other players are dummy (in the sense their actions do not affect transition and rewards). In this case, the best response of player 1 reduces to maximizing her own reward with respect to the environment (MDP) only. Formally, we define an $\\eps$-approximate Nash equilibrium (henceforth also $\\eps$-Nash) as follows. Below, $\\pi_{-i}$ denotes the collection of all but the $i$-th player’s policy within $\\pi$. Definition: A product policy $\\pi=(\\pi_1,\\dots, \\pi_m)$ is an $\\eps$-approximate Nash equilibrium, if we have [ \\max_{i\\in[m]} \\big( \\max_{\\pi_i’} V_{i,1}^ {\\pi_i’, \\pi_{-i}}- V_{i,1}^{\\pi_i, \\pi_{-i}} \\big) \\le \\eps. ] We say $\\pi$ is an (exact) Nash equilibrium if the above holds with $\\eps=0$. The Nash equilibrium need not be unique for (general-sum) MGs. Also, note that by definition any Nash policy must be a product policy (i.e. all the players execute their own policy independently). There exist other proper notions of equilibria when players do play in a correlated fashion; we will dive deeper into this in Section 5. 2.1 Nash equilibrium in zero-sum games The Nash equilibrium enjoys additional nice properties in zero-sum MGs. Recall that $V_1^{\\pi_1, \\pi_2}$ denotes player 1’s value for which $\\pi_1$ seeks to maximize and $\\pi_2$ seeks to minimize. Proposition: For any zero-sum MG, we have (Unique Nash value) There exists a $V_1^{\\star}\\in[0,H]$ such that all Nash equilibrium $(\\pi_1^\\star, \\pi_2^\\star)$ achieves this same value: \\(V_1^{\\pi_1^\\star, \\pi_2^\\star} = V_1^{\\star},\\) even though the Nash policy $(\\pi_1^\\star, \\pi_2^\\star)$ may not be unique. (Strong duality) Any Nash equilibrium is also a min-max solution and a max-min solution. In other words, the following two inequalities (which holds for any pair of $(\\pi_1, \\pi_2)$) \\[ \\left\\{ \\begin{aligned} &amp; \\max_{\\pi_1&#39;} V_1^{\\pi_1&#39;, \\pi_2} \\ge \\min_{\\pi_2&#39;} \\max_{\\pi_1&#39;} V_1^{\\pi_1&#39;, \\pi_2&#39;} \\ge \\max_{\\pi_1&#39;} \\min_{\\pi_2&#39;} V_1^{\\pi_1&#39;, \\pi_2&#39;} \\ge \\min_{\\pi_2&#39;} V_1^{\\pi_1, \\pi_2&#39;} \\\\ &amp; \\max_{\\pi_1&#39;} V_1^{\\pi_1&#39;, \\pi_2} \\ge V_1^{\\pi_1, \\pi_2} \\ge \\min_{\\pi_2&#39;} V_1^{\\pi_1, \\pi_2&#39;} \\end{aligned} \\right. \\] become equalities ($=V_1^{\\star}$) if $(\\pi_1, \\pi_2)$ is a Nash equilibrium. These nice properties make zero-sum MGs an appealing setting for learning Nash equilibria, which will be the focus of our next two sections. 3. Planning algorithms in zero-sum MGs Given the above setup, a natural first question is: How do we compute the Nash equilibrium in zero-sum MGs assuming known transitions and rewards? In other words, what is a good planning algorithm for learning Nash equilibria? It turns out that a direct multi-agent adaptation of Value Iteration, dating back to (Shapley 1953), gives an algorithm for computing the Nash equilibrium for a known game. We term this algorithm as Nash-VI (Nash Value Iteration). Similar to how Value Iteration (VI) computes the optimal values and policies for an MDP using backward dynamical programming, Nash-VI performs similar backward updates on an MG except for the policy selection mechanism: the maximization over the vector $Q^\\star(s, \\cdot)$ in VI is now replaced by a \\(\\text{MatrixNash}\\) subroutine over the matrix $Q^{\\star}(s, \\cdot, \\cdot)$. This makes sense given that the learning goal is now learning a Nash equilibrium policy over the two players jointly, instead of the reward maximizing policy for a single player. In words, Nash-VI = Value Iteration with \\(\\text{Max}(\\cdot)\\) replaced by \\(\\text{MatrixNash}(\\cdot)\\). Algorithm (Nash-VI): Initialize \\(V_{H+1}(s)\\equiv 0\\) for all \\(s\\in\\cS\\). For $h=H,\\dots,1$, compute the following over $s\\in\\cS$ and all $(a_1, a_2)\\in\\cA_1\\times \\cA_2$: \\[ \\begin{aligned} &amp; Q_h^{\\star}(s, a_1, a_2) = r_h(s, a_1, a_2) + (\\P_h V_{h+1}^\\star)(s, a_1, a_2), \\\\\\ &amp; (\\pi_{1,h}^\\star(\\cdot\\vert s), \\pi_{2,h}^\\star(\\cdot\\vert s)) = \\text{MatrixNash}(Q_h^{\\star}(s,\\cdot,\\cdot)), \\\\\\ &amp; V^{\\star}_h(s) = \\pi_{1,h}^\\star(\\cdot \\vert s)^\\top Q_h(s,\\cdot,\\cdot) \\pi_{2,h}^\\star(\\cdot \\vert s). \\end{aligned} \\] Return policies $(\\pi_1^\\star, \\pi_2^\\star)$ where \\(\\pi_i^\\star=\\{\\pi_{i,h}^\\star(\\cdot\\vert s)\\}_{(h,s)\\in[H]\\times \\cS}\\). Above, the \\(\\text{MatrixNash}\\) subroutine for any matrix \\(M\\in \\R^{A_1\\times A_2}\\) is defined as \\[ \\begin{align}\\label{equation:matrixnash} \\text{MatrixNash}(M) := \\arg \\big( \\max_{\\pi_1 \\in \\Delta_{\\cA_1}} \\min_{\\pi_2 \\in \\Delta_{\\cA_2}} \\pi_1^\\top M \\pi_2 \\big), \\end{align} \\] which can be implemented efficiently using linear programming. Here $\\Delta_{\\cA_1}$ and $\\Delta_{\\cA_2}$ are the probability simplex over action sets $\\cA_1$ and $\\cA_2$ respectively. Using backward induction over $h$, it is straightforward to show that the policies $(\\pi_1^\\star, \\pi_2^\\star)$ returned by Nash-VI is indeed a Nash equilibrium of the desired MG. The $V_h^{\\star}(s_h)$ and $Q_h^{\\star}(s,a_1,a_2)$ computed above are uniquely defined; we call them Nash values of the game at state $s$ and state-action tuple $(s,a_1,a_2)$ respectively. Value Iteration is not the only algorithm for computing Nash. As an alternative, the Nash Q-Learning algorithm (Hu &amp; Wellman 2003) performs incremental updates on the Q values, and uses the same matrix Nash subroutine to compute (Nash) V values from the Q values. Further, both Nash-VI and Nash Q-learning can be adapted when the game transitions and rewards are not known and have to be estimated from samples. For example, using random samples from a simulator (can query any $(s_h, a_{h,1}, a_{h,2})$ and obtain a sample of $(r_h, s_{h+1})$), an $\\eps$-Nash can be learned with $\\tO(H^3SA_1A_2/\\eps^2)$ samples with high probability, using variants of either Nash-VI (Zhang et al. 2020) or Nash Q-learning (Sidford et al. 2019). Centralization. Finally, we remark that both Nash-VI and Nash Q-learning are centralized training algorithms—that is, the computation for the two players are coupled (due to the joint computation in the matrix Nash subroutine). In order to execute these algorithms, the two players could not act in isolation, and have to coordinate or even live in a same centralized machine (self-play). In Section 4.2, we will present another decentralized algorithm, which is further capable of improving the dependency of sample complexity on the number of actions from \\(\\tO(A_1A_2)\\) to \\(\\tO(\\max\\{A_1, A_2\\})\\). 4. Sample-efficient learning of Nash equilibria in zero-sum MGs We now move on to the more challenging setting of learning Nash equilibria from interactive learning environments. Rather than assuming full knowledge or simulators of the game, we now assume that the algorithm can only learn about the game by playing repeated episodes of the game in a trial-and-error fashion (also known as the interactive/exploration setting). This requires the algorithms to further address the challenge of balancing exploration and exploitation. Our goal is to design algorithms that are sample-efficient, i.e. can learn an $\\eps$-Nash with as few episodes of play as possible. 4.1 Optimistic Nash-VI Our first algorithm is an optimistic variant of the Nash-VI algorithm we have seen in Section 3. This algorithm first appeared in (Liu et al. 2020), and builds on earlier sample-efficient Nash-VI type algorithms within (Bai &amp; Jin 2020, Xie et al. 2020). Algorithm (Optimistic Nash-VI, sketch): Initialize \\(\\up{Q}_h(s, a_1, a_2)\\setto H\\) and \\(\\low{Q}_h(s, a_1, a_2)\\setto 0\\) for all \\((s, a_1, a_2, h)\\). For episode $k=1,\\dots,K$: For step $h=H,\\dots,1$: For all $(s, a_1, a_2)\\in\\cS\\times \\cA_1\\times \\cA_2$: Set $t\\setto N_h(s, a_1, a_2)$ (visitation count). If $t&gt;0$ then Compute bonus \\(\\beta\\setto \\text{BernsteinBonus}(t, \\what{\\V}_h[(\\up{V}_{h+1} + \\low{V}_{h+1})/2] (s, a_1, a_2)\\). Compute additional bonus \\(\\gamma\\setto (c/H) \\what{\\P}_h(\\up{V}_{h+1} - \\low{V}_{h+1})(s, a_1, a_2)\\). \\(\\up{Q}_h(s, a_1, a_2)\\setto \\min\\{ (r_h + \\what{P}_h\\up{V}_{h+1})(s, a_1, a_2) + \\beta + \\gamma, H \\}\\). \\(\\low{Q}_h(s, a_1, a_2)\\setto \\max\\{ (r_h + \\what{P}_h\\low{V}_{h+1})(s, a_1, a_2) - \\beta - \\gamma, 0 \\}\\). For all $s\\in \\cS$: \\(\\pi_h(\\cdot, \\cdot \\vert s) \\setto \\text{MatrixCCE}(\\up{Q}_h(s, \\cdot, \\cdot), \\low{Q}_h(s, \\cdot, \\cdot))\\). \\(\\up{V}_h(s) \\setto \\langle \\pi_h(\\cdot, \\cdot \\vert s), \\up{Q}_h(s, \\cdot, \\cdot) \\rangle\\). \\(\\low{V}_h(s) \\setto \\langle \\pi_h(\\cdot, \\cdot \\vert s), \\low{Q}_h(s, \\cdot, \\cdot) \\rangle\\). Play an episode using (correlated) policy $\\pi$, and update the empirical models $\\what{\\P}_h$, $N_h$. Set $(\\pi_1^k, \\pi_2^k)$ to be the marginal policy of the current policy $\\pi$. Main techniques. Optimistic Nash-VI enhances the original Nash-VI in two main aspects in order to perform sample-efficient exploration in an interactive learning setting: The algorithm maintains two optimistic Q estimates: An upper estimate $\\up{Q}_h$, and a lower estimate $\\low{Q}_h$. The amount of optimisticity is governed by the two bonus functions $\\beta$ and $\\gamma$, where $\\beta$ is similar to the usual Bernstein bonus for exploration in single-agent settings (Azar et al. 2017, Jin et al. 2018), and $\\gamma$ is a specifically designed model-based bonus that allows a tighter analysis, akin to the one used in (Dann et al. 2018). We remark that different from the single-agent setting, the introduction of $\\gamma$ is key in achieving $\\mathcal{O}(S)$ sample complexity instead of $\\mathcal{O}(S^2)$. The policy is now computed using a matrix CCE subroutine. The use of matrix CCE in the context of learning Markov game is first introduced by (Xie et al. 2020). For any pair of matrices $\\up{M}, \\low{M}\\in\\R^{A_1\\times A_2}$, $ \\text{MatrixCCE}(\\up{M}, \\low{M})$ is defined as any correlated policy $\\pi \\in \\Delta_{\\cA_1\\times \\cA_2}$ such that the following holds: \\[ \\begin{aligned} &amp; \\E_{(a_1, a_2)\\sim \\pi} [\\up{M}(a_1, a_2)] \\ge \\max_{a_1&#39;\\in \\cA_1} \\E_{(a_1, a_2)\\sim \\pi}[\\up{M}(a_1&#39;, a_2)], \\\\ &amp; \\E_{(a_1, a_2)\\sim \\pi} [\\low{M}(a_1, a_2)] \\le \\min_{a_2&#39;\\in \\cA_2} \\E_{(a_1, a_2)\\sim \\pi}[\\low{M}(a_1, a_2&#39;)]. \\end{aligned} \\] In other words, if the two players jointly play $\\pi$, the max player has no gain in deviating for the payoff matrix $\\up{M}$, and the min player has no gain in deviating for the payoff matrix $\\low{M}$. The purpose of this matrix CCE subroutine is to find a good policy with respect to the upper / lower estimates in a computationally efficient fashion (this subroutine can be implemented using linear programming). Indeed, here a general-sum matrix Nash subroutine with respect to $(\\up{Q}_h(s,\\cdot,\\cdot), \\low{Q}_h(s,\\cdot,\\cdot))$ may as well be used (and leads to the same sample complexity results), yet would suffer from computational difficulties for large $A_1,A_2$, as finding Nash in a two-player general-sum matrix game is PPAD hard (Daskalakis et al. 2013). These techniques enable the optimistic Nash-VI algorithm to achieve the following sample complexity guarantee for learning Nash equilibria in zero-sum MGs. Theorem (Liu et al. 2020): With high probability, optimistic Nash-VI outputs an $\\eps$-Nash within $K=\\tO(H^3SA_1A_2/\\eps^2)$ episodes of play. Compared with the minimax sample complexity $H^3SA/\\eps^2$ for single-agent MDPs achieved by the UCBVI algorithm (Azar et al. 2017, with matching lower bound in (Jin et al. 2018, Domingues et al. 2021)), the sample complexity achieved by Nash-VI for zero-sum MGs only pays an additional $A_1A_2$, i.e. product of the action space. This may seem natural at first sight as the joint action space in a zero-sum MG does have size $A_1A_2$. However, as we will see in the next subsection, it is possible to design a decentralized learning algorithm that achieves a \\(\\max\\{A_1,A_2\\}\\) dependence on the action spaces. Centralized training, decentralized execution. Note that the $\\pi$ returned by the matrix CCE subroutines and used in the exploration step is a correlated policy. Therefore, the entire algorithm has to be executed in a centralized fashion. However, the final output is the marginals of this correlated policy and thus is an (approximately Nash) product policy that can be deployed in a decentralized way. This is reminiscent of the “centralized training, decentralized execution” paradigm which is also commonly used in empirical multi-agent RL. 4.2 V-Learning Our second algorithm is a recently proposed algorithm, V-Learning, which first appeared in (Bai et al. 2020) in the name of Nash V-Learning. Here, we present the simplified version of the algorithm in its journal version (Jin et al. 2021c). V-Learning itself can be viewed as a single-agent RL algorithm. To run V-learning in the multiagent setting, we simply let each player run the single-agent V-learning entirely on her own as long as she observes the states, rewards, and next states (as if it were a single-agent MDP). The opponent’s action does not need to be observed. Therefore, V-learning is naturally a decentralized algorithm. We present the algorithm as follows. Algorithm (V-Learning): For all $(h, s, a)\\in [H]\\times \\cS\\times \\cA$, initialize \\(V_h(s)\\setto H-h+1\\), \\(\\pi_{h}(a \\vert s)\\setto 1/A\\), and $N_h(s)\\setto 0$. For episode $k=1,\\dots,K$: Receive $s_1$. For $h=1,\\dots,H$: Take action $a_{h}\\sim \\pi_{h}(\\cdot \\vert s_h)$, observe reward $r_h$ and next state $s_{h+1}$. (No need to observe the opponent’s action.) $t=N_h(s_h)\\setto N_h(s_h)+1$. \\(V_{h}(s_h)\\setto (1-\\alpha_t) V_h(s_h) + \\alpha_t (r_h + \\min\\{V_{h+1}(s_{h+1}), H-h\\} + \\beta_t)\\). \\(\\pi_{h}(\\cdot \\vert s_h) \\setto \\text{Adv_Bandit_Update}(a_{h}, \\frac{H-r_h-\\min\\{V_{h+1}(s_{h+1}), H-h\\} }{H})\\) on the $(s_h, h)$-th adversarial bandit. Algorithm (Executing output policy of V-Learning): Sample $k\\setto\\text{Unif}([K])$. For step $h=1,\\dots,H$: Observe $s_h$, and set $t\\setto N_h^k(s_h)$. Set $k\\setto k_h^i(s_h)$, where $k_h^1(s_h)&lt;\\dots&lt;k_h^t(s_h)$ are the indices of all past visitations to $(h, s_h)$ prior to episode $k$, and $i\\in [t]$ is sampled randomly with probability $\\alpha_t^i$. Take action $a_{h}\\sim \\pi_{h}^k(\\cdot\\vert s_h)$. The structure of the V-Learning algorithm is rather distinct from either Nash-VI or Nash Q-learning (perhaps its closest relative among classical RL algorithms). We briefly explain its three main components: Incremental update of $V$: This part is similar to Q-learning, except that we do not model the Q functions, but rather directly model the V functions (hence the name V-learning). Consequently, this update avoids constructing Q functions in the multiagent setting (which is of the size $\\cA_1 \\cA_2$). It also “ignores” the actions taken by the opponent. The incremental update uses a step-size sequence \\(\\{\\alpha_t\\}_{t\\ge 0}\\). We choose \\(\\alpha_t=(H+1)/(H+t)\\) following the analysis of Q-learning in (Jin et al. 2018). Adversarial bandit subroutine: Recall that in Q-learning (or its optimistic version), the greedy (argmax) rule over the estimated Q function is used at each $(s,h)$ to determine the policy. In V-learning, the policy at each $(s_h, h)$ is rather maintained by an adversarial bandit subroutine (there are $SH$ such adversarial bandits in total). Concretely, after $(s_h, h)$ is encountered and $(r_h, s_{h+1})$ are received, the corresponding adversarial bandit performs an update step of the form “Action $a_{h}\\in\\cA$ suffered loss \\(\\frac{H-r_h-\\min\\{V_{h+1}(s_{h+1}), H-h\\} }{H}\\in [0,1]\\).” The adversarial bandit subroutine is then responsible for determining how to actually compute the new policy given this update. In V-Learning, we instantiate the adversarial bandit as a weighted follow-the-regularized-leader algorithm, in order to achieve a per-state weighted regret guarantee of a certain form, at each $(s, h)$. Output policy: The output policy of V-learning is a nested mixture policy: A random integer $k$ is updated by random sampling throughout the execution of this policy, and at each $h$ this $k$ determines a particular policy for drawing the action $a_{h}$. This makes the policy non-Markov in general, which is drastically different from Nash-VI as well as most classical algorithms in single-agent RL which outputs Markov policies. Technically, such an output policy is critically required in order to extract from the per-state “regret” guarantees at each $(s,h)$ a policy that enjoys an overall near-Nash guarantee on the overall game value. (Hence its original name “certified policy” in (Bai et al. 2020) as this policy “certifies” the value functions appearing in the per-state regret bounds.) Here, the sampling probabilities $\\alpha_t^i$ are defined as [ \\alpha_t^0 = \\prod_{j=1}^t (1-\\alpha_j),~~~\\alpha_t^i = \\alpha_i \\prod_{j=i+1}^t (1-\\alpha_j). ] The above techniques lead to the following guarantee on V-Learning for zero-sum MGs. Theorem (Bai et al. 2020, Jin et al. 2021c): Suppose both players run the V-Learning algorithm with the \\(\\text{Adv_Bandit_Update}\\) instantiated as a suitable weighted Follow-The-Regularized-Leader algorithm for each $(s,h)$. Then, running this for $K$ episodes, (the product of) their certified output policies $(\\what{\\pi}_1, \\what{\\pi}_2)$ is an $\\epsilon$-Nash as long as \\(K\\ge \\tO(H^5S\\max\\{A_1, A_2\\}/\\eps^2)\\). Compared with optimistic Nash-VI, Nash V-Learning achieves milder dependence on the action space (\\(\\max\\{A_1,A_2\\}\\) vs. $A_1A_2$) thanks to its decentralized nature, at the cost of worse $H$ factors. We remark that the original sample complexity presented in (Bai et al. 2020) is \\(\\tO(H^6S\\max\\{A_1,A_2\\}/\\eps^2)\\), and one $H$ factor can be improved by adopting the sharper analysis in (Tian et al. 2020). 4.3 Summary of algorithms The table below summarizes the algorithms we have introduced so far for learning $\\eps$-Nash in zero-sum MGs. Note that here Nash Q-Learning (and its optimistic version) was not presented in this blog post; the actual algorithm and theoretical guarantee can be found in Section 3 of (Bai et al. 2020). Algorithm Training Update Main estimand Sample complexity Nash-VI centralized model-based \\(\\P_h(s&#39;\\vert s,a_1,a_2)\\) \\(\\tO(H^3SA_1A_2/\\eps^2)\\) Nash Q-Learning centralized model-free \\(Q_h^{\\star}(s,a_1,a_2)\\) \\(\\tO(H^5SA_1A_2/\\eps^2)\\) V-Learning decentralized model-free \\(V_h^{\\star}(s)\\) \\(\\tO(H^5S\\max\\{A_1, A_2\\}/\\eps^2)\\) 5. General-sum MGs with many players We now shift attention to the more general case of general-sum MGs with $m$ players for any $m\\ge 2$. For general-sum MGs, we are not only interested in learning near-Nash product policies, but also the broader class of correlated equilibria for correlated policies, which we define as follows. 5.1 Correlated Equilibria (CE) and Coarse Correlated Equilibria (CCE) Definition (Coarse Correlated Equilibrium): A correlated policy $\\pi$ is an $\\eps$-Coarse Correlated Equilibrium (CCE), if any player could not improve her own reward by more than $\\eps$ by deviating from $\\pi$ and playing some other policy on her own: [ \\max_{i\\in[m]} \\max_{\\pi_i’} \\Big( V_{i,1}^{\\pi_i’, \\pi_{-i}} - V_{i,1}^{\\pi} \\Big) \\le \\eps. ] Definition (Correlated Equilibrium; Informal): A correlated policy $\\pi$ is an $\\eps$-Correlated Equilibrium (CE), if any player could not improve her own reward by more than $\\eps$ by first observing her own action sampled from the correlated policy at each state, then deviating to some other action: [ \\max_{i\\in[m]} \\max_{\\phi\\in \\Phi_i} \\Big( V_{i,1}^{\\phi\\diamond \\pi} - V_{i,1}^{\\pi} \\Big) \\le \\eps. ] Above, $\\Phi_i$ is the set of all possible strategy modification functions for player $i$ (for a formal definition see (Liu et al. 2020)). In general, we have {Nash}$\\subset${CE}$\\subset${CCE}. Therefore, CE, CCE can be thought of as relaxations of Nash and make sense as learning goals on their own right as general-sum Nash suffers from PPAD hardness in the computational efficiency anyway (Daskalakis et al. 2013). We remark that similar as Nash, the notions of CE and CCE are standard and widely studied in the game theory literature, with many real-world implications (see e.g. this book &amp; Wikipedia). The above definitions are direct extensions of the original definitions (for strategic games) into Markov games. 5.2 Value Iteration algorithms for Nash, CE, and CCE The model-based optimistic Nash-VI algorithm can be adapted straightforwardly to the case of multiple players, by simply replacing the MatrixCCE subroutine in Optimistic Nash-VI algorithm with (multi-dimensional) matrix {Nash,CE,CCE} subroutines. This gives the following result for learning equilibria in general-sum MGs. Theorem (Liu et al. 2020): A multi-player adaptation of the optimistic Nash-VI algorithm coupled with (multi-dimensional) matrix {Nash,CE,CCE} subroutines can learn an $\\eps$-{Nash, CE, CCE} of a general-sum MG within \\(\\tO(H^4S^2\\prod_{i\\le m} A_i/\\eps^2)\\) episodes of play. The conceptual advantage of this algorithm is that it estimates the full transition model $\\P_h(s_{h+1} \\vert s_h, \\ba_h)$ directly; it can then learn all three kinds of equilibria by plugging in the corresponding (multi-dimensional) matrix subroutines. However, this algorithm suffers from the curse of multiagents: the sample complexity scales as $\\prod_{i\\le m} A_i$, which is not surprising given that it estimates the full transition model. Also note that, despite the sample efficiency, the multi-player matrix Nash subroutine involved in the above algorithm does not likely admit a poly time implementation due to the PPAD hardness in computing general-sum Nash. 5.3 V-Learning for CE and CCE Recall that V-Learning is a decentralized algorithm that can be deployed by each player independently, and for zero-sum MGs one could extract from its execution history a suitable output policy that is near Nash. It turns out that this paradigm generalizes nicely to the problems of learning CE and CCE in general-sum MGs. Specifically, recent concurrent works show that V-Learning algorithms instantiated with suitable adversarial bandit subroutines could learn CE/CCE sample-efficiently, without suffering from the curse of multiagents: Theorem (Song et al. 2021 &amp; Jin et al. 2021c) For $m$-player general-sum MGs, suppose each player run the V-Learning algorithm independently with suitably chosen adversarial bandit subroutines, and we extract a correlated output policy in a suitable fashion. Then: Instantiating the adversarial bandit subroutine as a regret minimizer (CCE-V-Learning), the correlated output policy is an $\\eps$-CCE within \\(\\tO(H^5S\\max_{i\\le m} A_i/\\eps^2)\\) episodes of play; Instantiating the adversarial bandit subroutine as a swap regret minimizer (CE-V-Learning), the correlated output policy is an $\\eps$-CE within \\(\\tO(H^5S(\\max_{i\\le m} A_i)^2/\\eps^2)\\) episodes of play. Compared with the VI algorithm from the last section, V-Learning breaks the curse of multiagents as the sample complexity scales polynomially in $\\max_{i\\le m}A_i$, as opposed to $\\prod_{i\\le m} A_i$ which is exponential in $m$. This is thanks to the decentralized learning structure of the V-Learning algorithm. (Bib note: We remark that Song et al. 2021 actually achieves $\\tO(H^6S(\\max_{i\\le m}A_i)^2/\\eps^2)$ for learning CE, which is one $H$ factor worse than Jin et al. 2021c due to using a slightly different swap regret minimizer. The concurrent work of Mao &amp; Basar 2021 also achieves a $\\tO(H^6S\\max_{i\\le m}A_i/\\eps^2)$ result for learning $\\eps$-CCE.) 6. Learning MGs with function approximation In single-agent MDPs, there is a rich body of work on the theory of function approximation—that is, sample-efficient learning in large state/action spaces equipped with a restricted function class. Here we briefly review some recent advances on function approximation in Markov Games. Throughout this section, we shift back to considering two-player zero-sum MGs. 6.1 Linear function approximation Similar as a linear MDP, a (zero-sum) linear MG is a Markov Game whose transitions and rewards satisfy the following linear structure: \\[ \\begin{aligned} &amp; \\P_h(s&#39; | s, a_1, a_2) = \\langle \\phi(s, a_1, a_2), \\mu_h(s&#39;) \\rangle, \\\\ &amp; r_h(s, a_1, a_2) = \\langle \\phi(s, a_1, a_2), \\theta_h \\rangle, \\end{aligned} \\] where $\\phi:\\cS\\times \\cA_1\\times \\cA_2\\to \\R^d$ is a known feature map and $\\mu_h$, $\\theta_h$ are (unknown) parameters for the transitions and rewards. The following result shows that linear MGs can be learned sample-efficiently with mild polynomial dependencies on $d,H$, similar as in linear MDPs. Their algorithm is similar to the optimistic Nash-VI algorithm, and further builds upon the single-agent optimistic least-squares value iteration algorithm (Jin et al. 2020) to utilize the linear structure in the transitions and rewards, by using a bonus function similar to linear bandits. To adapt this algorithm into the multi-agent setting, their algorithm again replaces the maximization over actions by matrix CCE subroutine, and uses both the upper bounds and lower bounds. Theorem (Xie et al. 2020): For linear zero-sum MG with horizon length $H$ and feature dimension $d$, there exists an algorithm that learns an \\(\\eps\\)-Nash within \\(\\tO(d^3H^4/\\eps^2)\\) episodes of play. We remark that Chen et al. 2021 consider the related model of linear mixture MGs and design an algorithm with episode complexity $\\tO(d^2H^3/\\eps^2)$. 6.2 General function approximation; The power of exploiter The above sample complexity results are based on a direct adaptation of Nash VI into the linear setting. Such an approach critically depends on a restrictive assumption called optimistic closure, which is true for tabular MGs and linear MGs, but rarely holds in general function approximation. To remove such an assumption, in the single-agent setting, we can modify the standard optimistic VI algorithm which computes an upper bound of value functions at each step, to a globally optimistic version which only computes the upper bound of value functions at the first step. The latter approach considers confidence sets at all steps simultaneously, thus providing a much sharper optimistic value, and allowing the algorithm to learn a near-optimal policy sample-efficiently (Zanette et al. 2020, Jin et al. 2021a). To adapt such an idea to the multi-agent setting, it turns out that we need one more technique—the use of an exploiter. In contrast to the Nash-VI algorithm where both agents compute Nash policies simultaneously, the new type of algorithm treats two agents asymmetrically. It picks one agent as the main (learning) agent, while letting the other agent be the exploiter. During training, the main agent tries to learn the Nash policy, while the exploiter keeps playing the best response to the current policies of the main agents. That is, the exploiter facilitates the learning of the main player by deliberately exploiting her weakness. The resulting algorithm after combining VI with global optimism and exploiter is the algorithm Golf_with_Exploiter (Jin et al. 2021b). The following result shows that this algorithm is capable of sample-efficient learning if the function class has a low multiagent Bellman Eluder (BE) dimension—a complexity measure adapted from its single-agent version (Jin et al. 2021a). Theorem (Jin et al. 2021b): For any zero-sum MG with horizon length $H$ equipped with a Q-function class $\\cF$ whose multi-agent Bellman Eluder dimension is \\(\\tO(d)\\), Golf_with_Exploiter algorithm learns an $\\eps$-Nash within \\(\\tO(H^2d\\log(\\vert \\cF\\vert)/\\eps^2)\\) episodes of play. The concurrent work of Huang et al. 2021 also establish a \\(\\tO(1/\\eps^2)\\) sample complexity guarantee for zero-sum MG with general function approximation, under the slightly different complexity measure of minimax Eluder dimension. 7. Other research frontiers; End note In this blog post, we presented a brief overview of recent advances in multi-agent RL theory. We presented several learning goals (Nash, CE, CCE), planning and sample-efficient learning algorithms for these learning goals in two tabular settings (two-player zero-sum MGs, multi-player general-sum MGs), and touched on the related topic of function approximation. Apart from the aforementioned questions, there are many other active research topics in MARL theory we have not covered in this blog post: Further design and analysis of decentralized algorithms. Policy optimization algorithms for Markov Games. Other notions of equilibria (e.g. Stackelberg equilibria). Markov potential games. Imperfect-information games. Similar to the topics covered in our earlier sections, the above topics also admit unique theoretical challenges brought by multiple agents that are not present in single-agent settings. We believe that tackling these challenges will provide many exciting opportunities for theory research down the road. (Last edited: May 20, 2022)","headline":"Recent Progresses in Multi-Agent RL Theory","dateModified":"2021-11-14T09:00:00-08:00","datePublished":"2021-11-14T09:00:00-08:00","url":"/blog/marl_theory.html","mainEntityOfPage":{"@type":"WebPage","@id":"/blog/marl_theory.html"},"author":{"@type":"Person","name":"Yu Bai, Chi Jin"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/blog/feed.xml" title="MARL Theory" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">MARL Theory</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a href="https://yubai.org">Homepage</a>
        </div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <script>
MathJax = {
  tex: {
    inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
    displayMath: [ ['$$', '$$'], ["\[", "\]"], ["\\[", "\\]"] ]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Recent Progresses in Multi-Agent RL Theory</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2021-11-14T09:00:00-08:00" itemprop="datePublished">Nov 14, 2021
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">Yu Bai, Chi Jin</span></span></p>
</header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Reinforcement learning (RL) has made substantial empirical progresses in solving hard AI challenges in the past few years. A big portion of these progresses—<a href="https://www.nature.com/articles/nature16961">Go</a>, <a href="https://openai.com/five/">Dota 2</a>, <a href="https://www.nature.com/articles/s41586-019-1724-z">Starcraft</a>, <a href="https://arxiv.org/abs/2004.13332">economic simulation</a>, <a href="https://arxiv.org/abs/1909.07528">social behavior learning</a>, and so on—come from <em>multi-agent RL</em>, that is, sequential decision making involving more than one agents. While the theoretical study of (single-agent) RL has a long history and a vastly growing recent interest, multi-agent RL theory is arguably a newer and less developed field, with its own unique challenges and opportunities that we feel very excited about.</p>

<p>In this extended blog post, we present a brief overview of the basics of multi-agent RL theory, along with some recent theoretical developments in the past few years. We will focus on learning Markov games, and cover the basic formulations, learning goals, planning algorithms, as well as recent advances in sample-efficient learning algorithms under the interactive (exploration) setting.</p>

<p>The majority of this blog post will assume familiarity with the basics of RL theory in the single-agent setting (for example, materials in this <a href="https://rltheorybook.github.io/">fantastic book</a>). We will focus on “what is different” when it comes to multi-agent, and discuss the various recent developments and opportunities therein.</p>

<p>\(\def\cS{\mathcal S}\)
\(\def\cA{\mathcal A}\)
\(\def\cF{\mathcal F}\)
\(\def\mc{\mathcal}\)
\(\def\P{\mathbb P}\)
\(\def\E{\mathbb E}\)
\(\def\V{\mathbb V}\)
\(\def\R{\mathbb R}\)
\(\def\tO{\widetilde{\mathcal{O}}}\)
\(\def\eps{\varepsilon}\)
\(\def\epsilon{\varepsilon}\)
\(\def\setto{\leftarrow}\)
\(\def\up{\overline}\)
\(\def\low{\underline}\)
\(\def\what{\widehat}\)
\(\def\ba{\boldsymbol{a}}\)</p>

<h3 id="1-formulation-markov-games">1. Formulation: Markov Games</h3>

<p>There are various formulations for multi-agent RL. In this blog post, we will focus on <strong>Markov Games</strong> (MG; <a href="https://www.pnas.org/content/39/10/1095">Shapley 1953</a>, <a href="https://courses.cs.duke.edu/spring07/cps296.3/littman94markov.pdf">Littman 1994</a>), a generalization of the widely-used Markov Decision Process (MDP) framework into the case of multiple agents. We remark that there exist various other frameworks for modeling multi-agent sequential decision making, such as <a href="https://en.wikipedia.org/wiki/Extensive-form_game">extensive-form games</a>, which can be considered as Markov games with special (tree-like) structures in transition dynamics; when combined with imperfect information, this formulation is more suitable for modeling games such as Poker.</p>

<!-- cooperative games and Cooperative games are more often used in distributed learning setup where multiple agents are optimizing the same objective given local observations. It typically focuses only on cooperation while Markov games incorporate both cooperation and competition. -->

<p>Roughly speaking, a Markov Game has the same (state, action) -&gt; (reward, next state) structure as in an MDP, except that now “action” is replaced by the joint actions of all players, and “reward” is replaced by a collection of reward functions so that each player has her own reward. In words:</p>

<blockquote>
  <p><strong>Markov Games with m players</strong>: (state, m actions) -&gt; (m rewards, next state).</p>
</blockquote>

<p>Formally, we consider a multi-player general-sum Markov game with $m$ players, which can be described as a tuple \(\text{MG}(H, \mathcal{S}, \{\mathcal{A}_{i}\}_{i=1}^{m}, \mathbb{P},\{r_{i}\}_{i=1}^{m})\), where</p>
<ul>
  <li>$H$ is the horizon length,</li>
  <li>$\mathcal{S}$ is the state space with $\vert \mathcal{S}\vert=S$.</li>
  <li>\(\mathcal{A}_i\) is the action space for the $i$-th player with \(\vert\mathcal{A}_i\vert=A_i\). All players act simultaneously; we use \(\ba=(a_1,\dots,a_m)\in \prod_{i=1}^m \mathcal{A}_i\) to denote a joint action,</li>
  <li>\(\mathbb{P}=\{\mathbb{P}_h\}\) is the collection of transition probabilities, where \(\mathbb{P}_h(s_{h+1} \vert s_h, \ba_h)\) is the probability of transiting to the next state \(s_{h+1}\), given state-action pair $(s_h, \ba_h)$ at step $h$,</li>
  <li>\(r_i=\{r_{i,h}\}\) is the reward function for the \(i\)-th player. At step $h$, the $i$-th player will experience reward $r_{i,h}(s_h, \ba_h)\in[0, 1]$. For simplicity of presentation, we assume the reward is deterministic.</li>
</ul>

<p>A trajectory of an MG looks like this:
[
s_1, (a_{1, 1}, \dots, a_{m, 1}), (r_{1,1},\dots, r_{m, 1}), s_2, \dots, s_H, (a_{1, H}, \dots, a_{m, H}), (r_{1,H}, \dots, r_{m,H}).
]
Finally, a (Markov) policy for the $i$-th player is denoted by \(\pi_i=\{\pi_{i,h}\}_{h=1}^H\), where \(\pi_{i,h}(a_{i,h}\vert s_h)\) is the probability of taking action $a_{i,h}$ at step $h$ and state $s_h$. We use \(V_{i,h}^{\pi}(s_h)\) and $Q_{i,h}^{\pi}(s_h, \ba_h)$ to denote the value and Q-value functions for player $i$ at step $h$, when joint policy $\pi$ is used. (This can either be a <em>product policy</em> $\pi=(\pi_1,\dots,\pi_m)$ which can be executed independently, or a <em>correlated policy</em> which samples the joint action $\ba_h$ for all the players together in a potentially correlated fashion.) We assume the initial state $s_1$ is deterministic, so that \(V_{i,1}^{\pi} := V_{i,1}^\pi(s_1)\) is the overall return for the $i$-th player.</p>

<h4 id="11-special-case-zero-sum-games">1.1 Special case: Zero-sum games</h4>

<p>An important special case of Markov games is zero-sum (competitive) MGs:</p>

<blockquote>
  <p>A <strong>zero-sum Markov Game</strong> is a Markov Game with $m=2$, and $r_1\equiv -r_2$.</p>
</blockquote>

<p>As each player wishes to maximize her own reward, in a zero-sum MG, the two players are in a purely competitive relationship. We thus define the overall <em>value</em> in a zero-sum MG to be player 1’s value function: $V_h:=V_{1,h}$. We call player 1 the <em>max player</em> and player 2 the <em>min player</em>.</p>

<p>Zero-sum MGs model a number of real-world two-player strategic games, such as Go, Chess, Shogi, etc. From a mathematical perspective, zero-sum MGs is perhaps the most immediate extension of a single-agent MDP—solving it can be thought of as a certain min-max optimization, akin to how solving a single-agent MDP is a certain maximization problem.</p>

<!-- <span style="color:red"> let's ignore collaborative games?</span> -->

<h3 id="2-nash-equilibrium">2. Nash equilibrium</h3>

<p>In an MDP, the learning goal for an agent is to maximize her own cumulative reward. In an MG, this is still the learning goal for each individual agent, but we have to additionally decide on some notion of <em>equilibrium</em> to combine these individual optimality conditions.</p>

<p>Throughout the majority of this blog post, we consider the celebrated notion of Nash equilibrium (<a href="https://www.jstor.org/stable/1969529">Nash 1951</a>), which states that for each player $i$, fixing all other player’s policies, her own policy is a <em>best response</em> (i.e. maximizes her own cumulative reward):</p>

<blockquote>
  <p><strong>Nash equilibrium</strong>: Each player plays the best response to all other player’s policies.</p>
</blockquote>

<p>It is clear that the Nash equilibrium strictly generalizes the notion of an optimal policy for MDPs—An MG reduces to an MDP if player 1 acts and all other players are dummy (in the sense their actions do not affect transition and rewards). In this case, the best response of player 1 reduces to maximizing her own reward with respect to the environment (MDP) only.</p>

<p>Formally, we define an $\eps$-approximate Nash equilibrium (henceforth also $\eps$-Nash) as follows. Below, $\pi_{-i}$ denotes the collection of all but the $i$-th player’s policy within $\pi$.</p>

<p><strong>Definition</strong>: A product policy $\pi=(\pi_1,\dots, \pi_m)$ is an <strong>$\eps$-approximate Nash equilibrium</strong>, if we have
[
\max_{i\in[m]} \big( \max_{\pi_i’} V_{i,1}^ {\pi_i’, \pi_{-i}}- V_{i,1}^{\pi_i, \pi_{-i}} \big) \le \eps.
]
We say $\pi$ is an (exact) Nash equilibrium if the above holds with $\eps=0$.</p>

<p>The Nash equilibrium need not be unique for (general-sum) MGs. Also, note that by definition any Nash policy must be a <em>product policy</em> (i.e. all the players execute their own policy independently). There exist other proper notions of equilibria when players do play in a correlated fashion; we will dive deeper into this in Section 5.</p>

<h4 id="21-nash-equilibrium-in-zero-sum-games">2.1 Nash equilibrium in zero-sum games</h4>

<p>The Nash equilibrium enjoys additional nice properties in zero-sum MGs. Recall that $V_1^{\pi_1, \pi_2}$ denotes player 1’s value for which $\pi_1$ seeks to maximize and $\pi_2$ seeks to minimize.</p>

<p><strong>Proposition</strong>: For any zero-sum MG, we have</p>
<ul>
  <li>(Unique <em>Nash value</em>) There exists a $V_1^{\star}\in[0,H]$ such that all Nash equilibrium $(\pi_1^\star, \pi_2^\star)$ achieves this same value:
\(V_1^{\pi_1^\star, \pi_2^\star} = V_1^{\star},\)
even though the <em>Nash policy</em> $(\pi_1^\star, \pi_2^\star)$ may not be unique.</li>
  <li>
    <p> (Strong duality) Any Nash equilibrium is also a min-max solution and a max-min solution. In other words, the following two inequalities (which holds for any pair of $(\pi_1, \pi_2)$)
\[
\left\{
\begin{aligned}
&amp; \max_{\pi_1'} V_1^{\pi_1', \pi_2}  \ge \min_{\pi_2'} \max_{\pi_1'} V_1^{\pi_1', \pi_2'} \ge \max_{\pi_1'} \min_{\pi_2'} V_1^{\pi_1', \pi_2'} \ge \min_{\pi_2'} V_1^{\pi_1, \pi_2'} \\
&amp; \max_{\pi_1'} V_1^{\pi_1', \pi_2}  \ge V_1^{\pi_1, \pi_2} \ge \min_{\pi_2'} V_1^{\pi_1, \pi_2'}
\end{aligned}
\right.
\]
become <b>equalities</b> ($=V_1^{\star}$) if $(\pi_1, \pi_2)$ is a Nash equilibrium. </p>
  </li>
</ul>

<p>These nice properties make zero-sum MGs an appealing setting for learning Nash equilibria, which will be the focus of our next two sections.</p>

<h3 id="3-planning-algorithms-in-zero-sum-mgs">3. Planning algorithms in zero-sum MGs</h3>

<p>Given the above setup, a natural first question is: How do we compute the Nash equilibrium in zero-sum MGs assuming known transitions and rewards? In other words, what is a good <em>planning algorithm</em> for learning Nash equilibria? It turns out that a direct multi-agent adaptation of Value Iteration, dating back to (<a href="https://www.pnas.org/content/39/10/1095">Shapley 1953</a>), gives an algorithm for computing the Nash equilibrium for a known game. We term this algorithm as <em>Nash-VI</em> (Nash Value Iteration).</p>

<p>Similar to how Value Iteration (VI) computes the optimal values and policies for an MDP using backward dynamical programming, Nash-VI performs similar backward updates on an MG except for the policy selection mechanism: the maximization over the vector $Q^\star(s, \cdot)$ in VI is now replaced by a \(\text{MatrixNash}\) subroutine over the matrix $Q^{\star}(s, \cdot, \cdot)$. This makes sense given that the learning goal is now learning a Nash equilibrium policy over the two players jointly, instead of the reward maximizing policy for a single player. In words,</p>

<blockquote>
  <p><strong>Nash-VI</strong> = Value Iteration with \(\text{Max}(\cdot)\) replaced by \(\text{MatrixNash}(\cdot)\).</p>
</blockquote>

<p><strong>Algorithm</strong> (Nash-VI):</p>
<ul>
  <li>Initialize \(V_{H+1}(s)\equiv 0\) for all \(s\in\cS\).</li>
  <li>For $h=H,\dots,1$, compute the following over $s\in\cS$ and all $(a_1, a_2)\in\cA_1\times \cA_2$:</li>
</ul>
<p>
\[
\begin{aligned}
&amp; Q_h^{\star}(s, a_1, a_2) = r_h(s, a_1, a_2) + (\P_h V_{h+1}^\star)(s,
a_1, a_2), \\\
&amp; (\pi_{1,h}^\star(\cdot\vert s), \pi_{2,h}^\star(\cdot\vert s)) = \text{MatrixNash}(Q_h^{\star}(s,\cdot,\cdot)), \\\
&amp; V^{\star}_h(s) = \pi_{1,h}^\star(\cdot \vert s)^\top Q_h(s,\cdot,\cdot) \pi_{2,h}^\star(\cdot \vert s).
\end{aligned}
\]
</p>

<ul>
  <li>Return policies $(\pi_1^\star, \pi_2^\star)$ where \(\pi_i^\star=\{\pi_{i,h}^\star(\cdot\vert s)\}_{(h,s)\in[H]\times \cS}\).</li>
</ul>

<p>Above, the \(\text{MatrixNash}\) subroutine for any matrix \(M\in \R^{A_1\times A_2}\) is defined as
\[
\begin{align}\label{equation:matrixnash}
\text{MatrixNash}(M) := \arg \big( \max_{\pi_1 \in \Delta_{\cA_1}} \min_{\pi_2 \in \Delta_{\cA_2}} \pi_1^\top M \pi_2 \big),
\end{align}
\]
which can be implemented efficiently using linear programming. Here $\Delta_{\cA_1}$ and $\Delta_{\cA_2}$ are the probability simplex over action sets $\cA_1$ and $\cA_2$ respectively.</p>

<p>Using backward induction over $h$, it is straightforward to show that the policies $(\pi_1^\star, \pi_2^\star)$ returned by Nash-VI is indeed a Nash equilibrium of the desired MG. The $V_h^{\star}(s_h)$ and $Q_h^{\star}(s,a_1,a_2)$ computed above are uniquely defined; we call them <em>Nash values</em> of the game at state $s$ and state-action tuple $(s,a_1,a_2)$ respectively.</p>

<p>Value Iteration is not the only algorithm for computing Nash. As an alternative, the <em>Nash Q-Learning</em> algorithm (<a href="https://www.jmlr.org/papers/volume4/hu03a/hu03a.pdf">Hu &amp; Wellman 2003</a>) performs incremental updates on the Q values, and uses the same matrix Nash subroutine to compute (Nash) V values from the Q values.</p>

<p>Further, both Nash-VI and Nash Q-learning can be adapted when the game transitions and rewards are not known and have to be estimated from samples. For example, using random samples from a <em>simulator</em> (can query any $(s_h, a_{h,1}, a_{h,2})$ and obtain a sample of $(r_h, s_{h+1})$), an $\eps$-Nash can be learned with $\tO(H^3SA_1A_2/\eps^2)$ samples with high probability, using variants of either Nash-VI (<a href="https://arxiv.org/abs/2007.07461">Zhang et al. 2020</a>) or Nash Q-learning (<a href="https://arxiv.org/abs/1908.11071">Sidford et al. 2019</a>).</p>

<p><strong>Centralization</strong>. Finally, we remark that both Nash-VI and Nash Q-learning are <em>centralized</em> training algorithms—that is, the computation for the two players are coupled (due to the joint computation in the matrix Nash subroutine). In order to execute these algorithms, the two players could not act in isolation, and have to coordinate or even live in a same centralized machine (self-play). In Section 4.2, we will present another decentralized algorithm, which is further capable of improving the dependency of sample complexity on the number of actions from \(\tO(A_1A_2)\) to \(\tO(\max\{A_1, A_2\})\).</p>

<h3 id="4-sample-efficient-learning-of-nash-equilibria-in-zero-sum-mgs">4. Sample-efficient learning of Nash equilibria in zero-sum MGs</h3>

<p>We now move on to the more challenging setting of learning Nash equilibria from interactive learning environments. Rather than assuming full knowledge or simulators of the game, we now assume that the algorithm can only learn about the game by playing repeated episodes of the game in a trial-and-error fashion (also known as the interactive/exploration setting). This requires the algorithms to further address the challenge of balancing exploration and exploitation. Our goal is to design algorithms that are <em>sample-efficient</em>, i.e. can learn an $\eps$-Nash with as few episodes of play as possible.</p>

<h4 id="41-optimistic-nash-vi">4.1 Optimistic Nash-VI</h4>

<p>Our first algorithm is an optimistic variant of the Nash-VI algorithm we have seen in Section 3. This algorithm first appeared in (<a href="https://arxiv.org/abs/2010.01604">Liu et al. 2020</a>), and builds on earlier sample-efficient Nash-VI type algorithms within (<a href="https://arxiv.org/abs/2002.04017">Bai &amp; Jin 2020</a>, <a href="https://arxiv.org/abs/2002.07066">Xie et al. 2020</a>).</p>

<p><strong>Algorithm</strong> (Optimistic Nash-VI, sketch):</p>

<ul>
  <li>Initialize \(\up{Q}_h(s, a_1, a_2)\setto H\) and \(\low{Q}_h(s, a_1, a_2)\setto 0\) for all \((s, a_1, a_2, h)\).</li>
  <li>For episode $k=1,\dots,K$:
    <ul>
      <li>For step $h=H,\dots,1$:
        <ul>
          <li>For all $(s, a_1, a_2)\in\cS\times \cA_1\times \cA_2$:
            <ul>
              <li>Set $t\setto N_h(s, a_1, a_2)$ (visitation count).</li>
              <li>If $t&gt;0$ then
                <ul>
                  <li>Compute bonus \(\beta\setto \text{BernsteinBonus}(t, \what{\V}_h[(\up{V}_{h+1} + \low{V}_{h+1})/2] (s, a_1, a_2)\).</li>
                  <li>Compute additional bonus \(\gamma\setto (c/H) \what{\P}_h(\up{V}_{h+1} - \low{V}_{h+1})(s, a_1, a_2)\).</li>
                  <li>\(\up{Q}_h(s, a_1, a_2)\setto \min\{ (r_h + \what{P}_h\up{V}_{h+1})(s, a_1, a_2) + \beta + \gamma, H \}\).</li>
                  <li>\(\low{Q}_h(s, a_1, a_2)\setto \max\{ (r_h + \what{P}_h\low{V}_{h+1})(s, a_1, a_2) - \beta - \gamma, 0 \}\).</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>For all $s\in \cS$:
            <ul>
              <li>\(\pi_h(\cdot, \cdot \vert s) \setto \text{MatrixCCE}(\up{Q}_h(s, \cdot, \cdot), \low{Q}_h(s, \cdot, \cdot))\).</li>
              <li>\(\up{V}_h(s) \setto \langle \pi_h(\cdot, \cdot \vert s), \up{Q}_h(s, \cdot, \cdot) \rangle\).</li>
              <li>\(\low{V}_h(s) \setto \langle \pi_h(\cdot, \cdot \vert s), \low{Q}_h(s, \cdot, \cdot) \rangle\).</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Play an episode using (correlated) policy $\pi$, and update the empirical models $\what{\P}_h$, $N_h$.</li>
      <li>Set $(\pi_1^k, \pi_2^k)$ to be the marginal policy of the current policy $\pi$.</li>
    </ul>
  </li>
</ul>

<p><strong>Main techniques</strong>. Optimistic Nash-VI enhances the original Nash-VI in two main aspects in order to perform sample-efficient exploration in an interactive learning setting:</p>
<ul>
  <li>The algorithm maintains <em>two optimistic Q estimates</em>: An upper estimate $\up{Q}_h$, and a lower estimate $\low{Q}_h$. The amount of optimisticity is governed by the two bonus functions $\beta$ and $\gamma$, where $\beta$ is similar to the usual Bernstein bonus for exploration in single-agent settings (<a href="https://arxiv.org/abs/1703.05449">Azar et al. 2017</a>, <a href="https://arxiv.org/abs/1807.03765">Jin et al. 2018</a>), and $\gamma$ is a specifically designed model-based bonus that allows a tighter analysis, akin to the one used in (<a href="https://arxiv.org/abs/1811.03056">Dann et al. 2018</a>). We remark that different from the single-agent setting, the introduction of $\gamma$ is key in achieving $\mathcal{O}(S)$ sample complexity instead of $\mathcal{O}(S^2)$.</li>
  <li>
    <p> The policy is now computed using a <i>matrix CCE subroutine</i>. The use of matrix CCE in the context of learning Markov game is first introduced by (<a href="https://arxiv.org/abs/2002.07066">Xie et al. 2020</a>). For any pair of matrices $\up{M}, \low{M}\in\R^{A_1\times A_2}$, $ \text{MatrixCCE}(\up{M}, \low{M})$ is defined as any <i>correlated policy</i> $\pi \in \Delta_{\cA_1\times \cA_2}$ such that the following holds:
\[
\begin{aligned}
&amp; \E_{(a_1, a_2)\sim \pi} [\up{M}(a_1, a_2)] \ge \max_{a_1'\in \cA_1} \E_{(a_1, a_2)\sim \pi}[\up{M}(a_1', a_2)], \\
&amp; \E_{(a_1, a_2)\sim \pi} [\low{M}(a_1, a_2)] \le \min_{a_2'\in \cA_2} \E_{(a_1, a_2)\sim \pi}[\low{M}(a_1, a_2')].
\end{aligned}
\]  
In other words, if the two players jointly play $\pi$, the max player has no gain in deviating for the payoff matrix $\up{M}$, and the min player has no gain in deviating for the payoff matrix $\low{M}$. </p>

    <p>The purpose of this matrix CCE subroutine is to find a good policy with respect to the upper / lower estimates in a computationally efficient fashion (this subroutine can be implemented using linear programming). Indeed, here a general-sum matrix Nash subroutine with respect to $(\up{Q}_h(s,\cdot,\cdot), \low{Q}_h(s,\cdot,\cdot))$ may as well be used (and leads to the same sample complexity results), yet would suffer from computational difficulties for large $A_1,A_2$, as finding Nash in a two-player general-sum matrix game is PPAD hard (<a href="https://people.csail.mit.edu/costis/appxhardness.pdf">Daskalakis et al. 2013</a>).</p>
  </li>
</ul>

<p>These techniques enable the optimistic Nash-VI algorithm to achieve the following sample complexity guarantee for learning Nash equilibria in zero-sum MGs.</p>

<blockquote>
  <p><strong>Theorem</strong> (<a href="https://arxiv.org/abs/2010.01604">Liu et al. 2020</a>): With high probability, optimistic Nash-VI outputs an $\eps$-Nash within $K=\tO(H^3SA_1A_2/\eps^2)$ episodes of play.</p>
</blockquote>

<p>Compared with the minimax sample complexity $H^3SA/\eps^2$ for single-agent MDPs achieved by the UCBVI algorithm (<a href="https://arxiv.org/abs/1703.05449">Azar et al. 2017</a>, with matching lower bound in (<a href="https://arxiv.org/abs/1807.03765">Jin et al. 2018</a>, <a href="http://proceedings.mlr.press/v132/domingues21a/domingues21a.pdf">Domingues et al. 2021</a>)), the sample complexity achieved by Nash-VI for zero-sum MGs only pays an additional $A_1A_2$, i.e. product of the action space. This may seem natural at first sight as the joint action space in a zero-sum MG does have size $A_1A_2$. However, as we will see in the next subsection, it is possible to design a decentralized learning algorithm that achieves a \(\max\{A_1,A_2\}\) dependence on the action spaces.</p>

<p><strong>Centralized training, decentralized execution</strong>. Note that the $\pi$ returned by the matrix CCE subroutines and used in the exploration step is a correlated policy. Therefore, the entire algorithm has to be executed in a centralized fashion. However, the final output is the <em>marginals</em> of this correlated policy and thus is an (approximately Nash) product policy that can be deployed in a decentralized way. This is reminiscent of the “centralized training, decentralized execution” paradigm which is also commonly used in empirical multi-agent RL.</p>

<h4 id="42-v-learning">4.2 V-Learning</h4>

<p>Our second algorithm is a recently proposed algorithm, <em>V-Learning</em>, which first appeared in (<a href="https://arxiv.org/abs/2006.12007">Bai et al. 2020</a>) in the name of Nash V-Learning. Here, we present the simplified version of the algorithm in its journal version (<a href="https://arxiv.org/abs/2110.14555">Jin et al. 2021c</a>). V-Learning itself can be viewed as a <em>single-agent</em> RL algorithm. To run V-learning in the multiagent setting, we simply let each player run the single-agent V-learning entirely on her own as long as she observes the states, rewards, and next states (as if it were a single-agent MDP). The opponent’s action does not need to be observed.  Therefore, V-learning is naturally a <strong>decentralized</strong> algorithm.  We present the algorithm as follows.</p>

<p><strong>Algorithm</strong> (V-Learning):</p>

<ul>
  <li>For all $(h, s, a)\in [H]\times \cS\times \cA$, initialize \(V_h(s)\setto H-h+1\), \(\pi_{h}(a \vert s)\setto 1/A\), and $N_h(s)\setto 0$.</li>
  <li>For episode $k=1,\dots,K$:
    <ul>
      <li>Receive $s_1$.</li>
      <li>For $h=1,\dots,H$:
        <ul>
          <li>Take action $a_{h}\sim \pi_{h}(\cdot \vert s_h)$, observe reward $r_h$ and next state $s_{h+1}$.<br />
  (No need to observe the opponent’s action.)</li>
          <li>$t=N_h(s_h)\setto N_h(s_h)+1$.</li>
          <li>\(V_{h}(s_h)\setto (1-\alpha_t) V_h(s_h) + \alpha_t (r_h + \min\{V_{h+1}(s_{h+1}), H-h\} + \beta_t)\).</li>
          <li>\(\pi_{h}(\cdot \vert s_h) \setto \text{Adv_Bandit_Update}(a_{h}, \frac{H-r_h-\min\{V_{h+1}(s_{h+1}), H-h\} }{H})\) on the $(s_h, h)$-th adversarial bandit.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Algorithm</strong> (Executing output policy of V-Learning):</p>

<ul>
  <li>Sample $k\setto\text{Unif}([K])$.</li>
  <li>For step $h=1,\dots,H$:
    <ul>
      <li>Observe $s_h$, and set $t\setto N_h^k(s_h)$.</li>
      <li>Set $k\setto k_h^i(s_h)$, where $k_h^1(s_h)&lt;\dots&lt;k_h^t(s_h)$ are the indices of all past visitations to $(h, s_h)$ prior to episode $k$, and $i\in [t]$ is sampled randomly with probability $\alpha_t^i$.</li>
      <li>Take action $a_{h}\sim \pi_{h}^k(\cdot\vert s_h)$.</li>
    </ul>
  </li>
</ul>

<p>The structure of the V-Learning algorithm is rather distinct from either Nash-VI or Nash Q-learning (perhaps its closest relative among classical RL algorithms). We briefly explain its three main components:</p>
<ul>
  <li><strong>Incremental update of $V$</strong>: This part is similar to Q-learning, except that we do not model the Q functions, but rather directly model the V functions (hence the name V-learning). Consequently, this update avoids constructing Q functions in the multiagent setting (which is of the size $\cA_1 \cA_2$). It also “ignores” the actions taken by the opponent.<br />
The incremental update uses a step-size sequence \(\{\alpha_t\}_{t\ge 0}\). We choose \(\alpha_t=(H+1)/(H+t)\) following the analysis of Q-learning in (<a href="https://arxiv.org/abs/1807.03765">Jin et al. 2018</a>).</li>
  <li><strong>Adversarial bandit subroutine</strong>: Recall that in Q-learning (or its optimistic version), the greedy (argmax) rule over the estimated Q function is used at each $(s,h)$ to determine the policy. In V-learning, the policy at each $(s_h, h)$ is rather maintained by an <em>adversarial bandit subroutine</em> (there are $SH$ such adversarial bandits in total).<br />
Concretely, after $(s_h, h)$ is encountered and $(r_h, s_{h+1})$ are received, the corresponding adversarial bandit performs an update step of the form “Action $a_{h}\in\cA$ suffered loss \(\frac{H-r_h-\min\{V_{h+1}(s_{h+1}), H-h\} }{H}\in [0,1]\).” The adversarial bandit subroutine is then responsible for determining how to actually compute the new policy given this update.<br />
In V-Learning, we instantiate the adversarial bandit as a weighted follow-the-regularized-leader algorithm, in order to achieve a per-state <em>weighted regret</em> guarantee of a certain form, at each $(s, h)$.</li>
  <li><strong>Output policy</strong>: The output policy of V-learning is a <em>nested mixture</em> policy: A random integer $k$ is updated by random sampling throughout the execution of this policy, and at each $h$ this $k$ determines a particular policy for drawing the action $a_{h}$. This makes the policy <em>non-Markov</em> in general, which is drastically different from Nash-VI as well as most classical algorithms in single-agent RL which outputs Markov policies.<br />
Technically, such an output policy is critically required in order to extract from the per-state “regret” guarantees at each $(s,h)$ a policy that enjoys an overall near-Nash guarantee on the overall game value. (Hence its original name “certified policy” in (<a href="https://arxiv.org/abs/2006.12007">Bai et al. 2020</a>) as this policy “certifies” the value functions appearing in the per-state regret bounds.) Here, the sampling probabilities $\alpha_t^i$ are defined as
[
\alpha_t^0 = \prod_{j=1}^t (1-\alpha_j),~~~\alpha_t^i = \alpha_i \prod_{j=i+1}^t (1-\alpha_j).
]</li>
</ul>

<p>The above techniques lead to the following guarantee on V-Learning for zero-sum MGs.</p>

<blockquote>
  <p><strong>Theorem</strong> (<a href="https://arxiv.org/abs/2006.12007">Bai et al. 2020</a>, <a href="https://arxiv.org/abs/2110.14555">Jin et al. 2021c</a>): Suppose both players run the V-Learning algorithm with the \(\text{Adv_Bandit_Update}\) instantiated as a suitable weighted Follow-The-Regularized-Leader algorithm for each $(s,h)$. Then, running this for $K$ episodes, (the product of) their certified output policies $(\what{\pi}_1, \what{\pi}_2)$ is an $\epsilon$-Nash as long as \(K\ge \tO(H^5S\max\{A_1, A_2\}/\eps^2)\).</p>
</blockquote>

<p>Compared with optimistic Nash-VI, Nash V-Learning achieves milder dependence on the action space (\(\max\{A_1,A_2\}\) vs. $A_1A_2$) thanks to its decentralized nature, at the cost of worse $H$ factors. We remark that the original sample complexity presented in (<a href="https://arxiv.org/abs/2006.12007">Bai et al. 2020</a>) is \(\tO(H^6S\max\{A_1,A_2\}/\eps^2)\), and one $H$ factor can be improved by adopting the sharper analysis in (<a href="https://arxiv.org/abs/2010.15020">Tian et al. 2020</a>).</p>

<!-- <span style="color:red"> I'm not sure if we want to say this. Check whether Tian et al. formally give this result. </span> -->

<h4 id="43-summary-of-algorithms">4.3 Summary of algorithms</h4>

<p>The table below summarizes the algorithms we have introduced so far for learning $\eps$-Nash in zero-sum MGs. Note that here Nash Q-Learning (and its optimistic version) was not presented in this blog post; the actual algorithm and theoretical guarantee can be found in Section 3 of (<a href="https://arxiv.org/abs/2006.12007">Bai et al. 2020</a>).</p>

<table>
  <tbody>
    <tr>
      <td>Algorithm</td>
      <td>Training</td>
      <td>Update</td>
      <td>Main estimand</td>
      <td>Sample complexity</td>
    </tr>
    <tr>
      <td>Nash-VI</td>
      <td>centralized</td>
      <td>model-based</td>
      <td>\(\P_h(s'\vert s,a_1,a_2)\)</td>
      <td>\(\tO(H^3SA_1A_2/\eps^2)\)</td>
    </tr>
    <tr>
      <td>Nash Q-Learning</td>
      <td>centralized</td>
      <td>model-free</td>
      <td>\(Q_h^{\star}(s,a_1,a_2)\)</td>
      <td>\(\tO(H^5SA_1A_2/\eps^2)\)</td>
    </tr>
    <tr>
      <td>V-Learning</td>
      <td>decentralized</td>
      <td>model-free</td>
      <td>\(V_h^{\star}(s)\)</td>
      <td>\(\tO(H^5S\max\{A_1, A_2\}/\eps^2)\)</td>
    </tr>
  </tbody>
</table>

<h3 id="5-general-sum-mgs-with-many-players">5. General-sum MGs with many players</h3>

<p>We now shift attention to the more general case of general-sum MGs with $m$ players for any $m\ge 2$. For general-sum MGs, we are not only interested in learning near-Nash product policies, but also the broader class of <em>correlated equilibria</em> for correlated policies, which we define as follows.</p>

<h4 id="51-correlated-equilibria-ce-and-coarse-correlated-equilibria-cce">5.1 Correlated Equilibria (CE) and Coarse Correlated Equilibria (CCE)</h4>

<p><strong>Definition</strong> (Coarse Correlated Equilibrium): A correlated policy $\pi$ is an $\eps$-Coarse Correlated Equilibrium (CCE), if any player could not improve her own reward by more than $\eps$ by deviating from $\pi$ and playing some other policy on her own:
[
\max_{i\in[m]} \max_{\pi_i’} \Big( V_{i,1}^{\pi_i’, \pi_{-i}} - V_{i,1}^{\pi} \Big) \le \eps.
]</p>

<p><strong>Definition</strong> (Correlated Equilibrium; Informal): A correlated policy $\pi$ is an $\eps$-Correlated Equilibrium (CE), if any player could not improve her own reward by more than $\eps$ by first observing her own action sampled from the correlated policy at each state, then deviating to some other action:
[
\max_{i\in[m]} \max_{\phi\in \Phi_i} \Big( V_{i,1}^{\phi\diamond \pi} - V_{i,1}^{\pi} \Big) \le \eps.
]
Above, $\Phi_i$ is the set of all possible <em>strategy modification functions</em> for player $i$ (for a formal definition see (<a href="https://arxiv.org/abs/2010.01604">Liu et al. 2020</a>)).</p>

<!-- For two-player zero-sum MGs, the notions of Nash, CE, and CCE coincide (for Nash, we translate any correlated policy to the product of their marginals). However, -->
<p>In general, we have {Nash}$\subset${CE}$\subset${CCE}. Therefore, CE, CCE can be thought of as relaxations of Nash and make sense as learning goals on their own right as general-sum Nash suffers from PPAD hardness in the computational efficiency anyway (<a href="https://people.csail.mit.edu/costis/appxhardness.pdf">Daskalakis et al. 2013</a>).</p>

<p>We remark that similar as Nash, the notions of CE and CCE are standard and widely studied in the game theory literature, with many real-world implications (see e.g. this <a href="https://www.cs.cmu.edu/~sandholm/cs15-892F13/algorithmic-game-theory.pdf">book</a> &amp; <a href="https://en.wikipedia.org/wiki/Correlated_equilibrium">Wikipedia</a>). The above definitions are direct extensions of the original definitions (for strategic games) into Markov games.</p>

<h4 id="52-value-iteration-algorithms-for-nash-ce-and-cce">5.2 Value Iteration algorithms for Nash, CE, and CCE</h4>

<p>The model-based optimistic Nash-VI algorithm can be adapted straightforwardly to the case of multiple players, by simply replacing the MatrixCCE subroutine in Optimistic Nash-VI algorithm with (multi-dimensional) matrix {Nash,CE,CCE} subroutines. This gives the following result for learning equilibria in general-sum MGs.</p>

<blockquote>
  <p><strong>Theorem</strong> (<a href="https://arxiv.org/abs/2010.01604">Liu et al. 2020</a>): A multi-player adaptation of the optimistic Nash-VI algorithm coupled with (multi-dimensional) matrix {Nash,CE,CCE} subroutines can learn an $\eps$-{Nash, CE, CCE} of a general-sum MG within \(\tO(H^4S^2\prod_{i\le m} A_i/\eps^2)\) episodes of play.</p>
</blockquote>

<p>The conceptual advantage of this algorithm is that it estimates the full transition model $\P_h(s_{h+1} \vert s_h, \ba_h)$ directly; it can then learn all three kinds of equilibria by plugging in the corresponding (multi-dimensional) matrix subroutines. However, this algorithm suffers from the <em>curse of multiagents</em>: the sample complexity scales as $\prod_{i\le m} A_i$, which is not surprising given that it estimates the full transition model. Also note that, despite the sample efficiency, the multi-player matrix Nash subroutine involved in the above algorithm does not likely admit a poly time implementation due to the PPAD hardness in computing general-sum Nash.</p>

<h4 id="53-v-learning-for-ce-and-cce">5.3 V-Learning for CE and CCE</h4>

<p>Recall that V-Learning is a decentralized algorithm that can be deployed by each player independently, and for zero-sum MGs one could extract from its execution history a suitable output policy that is near Nash. It turns out that this paradigm generalizes nicely to the problems of learning CE and CCE in general-sum MGs. Specifically, recent concurrent works show that V-Learning algorithms instantiated with suitable adversarial bandit subroutines could learn CE/CCE sample-efficiently, without suffering from the curse of multiagents:</p>

<blockquote>
  <p><strong>Theorem</strong> (<a href="https://arxiv.org/abs/2110.04184">Song et al. 2021</a> &amp; <a href="https://arxiv.org/abs/2110.14555">Jin et al. 2021c</a>) For $m$-player general-sum MGs, suppose each player run the V-Learning algorithm independently with suitably chosen adversarial bandit subroutines, and we extract a <em>correlated</em> output policy in a suitable fashion. Then:</p>
  <ul>
    <li>Instantiating the adversarial bandit subroutine as a regret minimizer (CCE-V-Learning), the correlated output policy is an $\eps$-CCE within \(\tO(H^5S\max_{i\le m} A_i/\eps^2)\) episodes of play;</li>
    <li>Instantiating the adversarial bandit subroutine as a swap regret minimizer (CE-V-Learning), the correlated output policy is an $\eps$-CE within \(\tO(H^5S(\max_{i\le m} A_i)^2/\eps^2)\) episodes of play.</li>
  </ul>
</blockquote>

<p>Compared with the VI algorithm from the last section, V-Learning breaks the curse of multiagents as the sample complexity scales polynomially in $\max_{i\le m}A_i$, as opposed to $\prod_{i\le m} A_i$ which is exponential in $m$. This is thanks to the decentralized learning structure of the V-Learning algorithm.</p>

<!-- <span style="color:red"> more about proof </span> -->

<p>(Bib note: We remark that <a href="https://arxiv.org/abs/2110.04184">Song et al. 2021</a> actually achieves $\tO(H^6S(\max_{i\le m}A_i)^2/\eps^2)$ for learning CE, which is one $H$ factor worse than <a href="https://arxiv.org/abs/2110.14555">Jin et al. 2021c</a> due to using a slightly different swap regret minimizer. The concurrent work of <a href="https://arxiv.org/abs/2110.05682">Mao &amp; Basar 2021</a> also achieves a $\tO(H^6S\max_{i\le m}A_i/\eps^2)$ result for learning $\eps$-CCE.)</p>

<h3 id="6-learning-mgs-with-function-approximation">6. Learning MGs with function approximation</h3>

<p>In single-agent MDPs, there is a rich body of work on the theory of function approximation—that is, sample-efficient learning in large state/action spaces equipped with a restricted function class. Here we briefly review some recent advances on function approximation in Markov Games. Throughout this section, we shift back to considering two-player zero-sum MGs.</p>

<h4 id="61-linear-function-approximation">6.1 Linear function approximation</h4>

<p>Similar as a linear MDP, a (zero-sum) <em>linear MG</em> is a Markov Game whose transitions and rewards satisfy the following linear structure:</p>
<p>
\[
\begin{aligned}
&amp; \P_h(s' | s, a_1, a_2) = \langle \phi(s, a_1, a_2), \mu_h(s') \rangle, \\
&amp; r_h(s, a_1, a_2) = \langle \phi(s, a_1, a_2), \theta_h \rangle,
\end{aligned}
\]
</p>
<p>where $\phi:\cS\times \cA_1\times \cA_2\to \R^d$ is a known feature map and $\mu_h$, $\theta_h$ are (unknown) parameters for the transitions and rewards.</p>

<p>The following result shows that linear MGs can be learned sample-efficiently with mild polynomial dependencies on $d,H$, similar as in linear MDPs. Their algorithm is similar to the optimistic Nash-VI algorithm, and further builds upon the single-agent optimistic least-squares value iteration algorithm (<a href="https://arxiv.org/abs/1907.05388">Jin et al. 2020</a>) to utilize the linear structure in the transitions and rewards, by using a bonus function similar to linear bandits. To adapt this algorithm into the multi-agent setting, their algorithm again replaces the maximization over actions by matrix CCE subroutine, and uses both the upper bounds and lower bounds.</p>

<blockquote>
  <p><strong>Theorem</strong> (<a href="https://arxiv.org/abs/2002.07066">Xie et al. 2020</a>): For linear zero-sum MG with horizon length $H$ and feature dimension $d$, there exists an algorithm that learns an \(\eps\)-Nash within \(\tO(d^3H^4/\eps^2)\) episodes of play.</p>
</blockquote>

<p>We remark that <a href="https://arxiv.org/abs/2102.07404">Chen et al. 2021</a> consider the related model of linear mixture MGs and design an algorithm with episode complexity $\tO(d^2H^3/\eps^2)$.</p>

<h4 id="62-general-function-approximation-the-power-of-exploiter">6.2 General function approximation; The power of exploiter</h4>

<p>The above sample complexity results are based on a direct adaptation of Nash VI into the linear setting. Such an approach critically depends on a restrictive assumption called <em>optimistic closure</em>, which is true for tabular MGs and linear MGs, but rarely holds in general function approximation.</p>

<p>To remove such an assumption, in the single-agent setting, we can modify the standard optimistic VI algorithm which computes an upper bound of value functions at each step, to a <strong>globally optimistic</strong> version which only computes the upper bound of value functions at the first step. The latter approach considers confidence sets at all steps simultaneously, thus providing a much sharper optimistic value, and allowing the algorithm to learn a near-optimal policy sample-efficiently (<a href="https://arxiv.org/abs/2003.00153">Zanette et al. 2020</a>, <a href="https://arxiv.org/abs/2102.00815">Jin et al. 2021a</a>).</p>

<p>To adapt such an idea to the multi-agent setting, it turns out that we need one more technique—the use of an <strong>exploiter</strong>. In contrast to the Nash-VI algorithm where both agents compute Nash policies simultaneously, the new type of algorithm treats two agents asymmetrically. It picks one agent as the main (learning) agent, while letting the other agent be the exploiter. During training, the main agent tries to learn the Nash policy, while the exploiter keeps playing the best response to the current policies of the main agents. That is, the exploiter facilitates the learning of the main player by deliberately exploiting her weakness.</p>

<p>The resulting algorithm after combining VI with global optimism and exploiter is the algorithm Golf_with_Exploiter (<a href="https://arxiv.org/abs/2106.03352">Jin et al. 2021b</a>). The following result shows that this algorithm is capable of sample-efficient learning if the function class has a low multiagent Bellman Eluder (BE) dimension—a complexity measure adapted from its single-agent version (<a href="https://arxiv.org/abs/2102.00815">Jin et al. 2021a</a>).</p>

<blockquote>
  <p><strong>Theorem</strong> (<a href="https://arxiv.org/abs/2106.03352">Jin et al. 2021b</a>): For any zero-sum MG with horizon length $H$ equipped with a Q-function class $\cF$ whose multi-agent Bellman Eluder dimension is \(\tO(d)\), Golf_with_Exploiter algorithm learns an $\eps$-Nash within \(\tO(H^2d\log(\vert \cF\vert)/\eps^2)\) episodes of play.</p>
</blockquote>

<p>The concurrent work of <a href="https://arxiv.org/abs/2107.14702">Huang et al. 2021</a> also establish a \(\tO(1/\eps^2)\) sample complexity guarantee for zero-sum MG with general function approximation, under the slightly different complexity measure of minimax Eluder dimension.</p>

<h3 id="7-other-research-frontiers-end-note">7. Other research frontiers; End note</h3>

<p>In this blog post, we presented a brief overview of recent advances in multi-agent RL theory. We presented several learning goals (Nash, CE, CCE), planning and sample-efficient learning algorithms for these learning goals in two tabular settings (two-player zero-sum MGs, multi-player general-sum MGs), and touched on the related topic of function approximation.</p>

<p>Apart from the aforementioned questions, there are many other active research topics in MARL theory we have not covered in this blog post:</p>
<ul>
  <li>Further design and analysis of decentralized algorithms.</li>
  <li>Policy optimization algorithms for Markov Games.</li>
  <li>Other notions of equilibria (e.g. Stackelberg equilibria).</li>
  <li>Markov potential games.</li>
  <li>Imperfect-information games.</li>
</ul>

<p>Similar to the topics covered in our earlier sections, the above topics also admit unique theoretical challenges brought by multiple agents that are not present in single-agent settings. We believe that tackling these challenges will provide many exciting opportunities for theory research down the road.</p>

<p>(Last edited: May 20, 2022)</p>

  </div><a class="u-url" href="/blog/marl_theory.html" hidden></a>
</article>

      </div>
    </main>

<!---<footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">MARL Theory</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">MARL Theory</li><li><a class="u-email" href="mailto:yubai.pku@gmail.com">yubai.pku@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/allenbai01"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">allenbai01</span></a></li><li><a href="https://www.twitter.com/yubai01"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">yubai01</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>TBA.</p>
      </div>
    </div>

  </div>

</footer>
--->

  </body>

</html>
