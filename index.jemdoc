# jemdoc: menu{MENU}{index.html}
= Yu Bai

~~~
{}{img_left}{yub.png}{Yu Bai}{240}{320}{}

*About me*: I am a Research Scientist at [https://einstein.ai/
Salesforce Research].  In September 2019, I completed my PhD in
[https://statistics.stanford.edu/ Statistics] at
[http://www.stanford.edu/ Stanford University] (specializing in
machine learning), where I was fortunate to be advised by
Prof. [https://stanford.edu/~jduchi/ John Duchi]. I was a member of
the [http://statsml.stanford.edu/index.html Statistical Machine
Learning Group]. During my PhD, I also spent two summers in research
labs in the industry: at [https://research.google.com Google Research]
in 2016, and at Amazon AI Palo Alto in 2018.

My research interest lies broadly in machine learning.
My recent focus is on the theoretical foundations and principled
algorithm designs for deep learning, reinforcement learning, representation learning, and uncertainty quantification.

Prior to Stanford, I was an undergrad in mathematics at
[http://www.pku.edu.cn/ Peking University].

*News*:

# - *{{<font color="DarkBlue">Looking for self-motivated interns for summer 2021!</font>}}*

- Two papers accepted by NeurIPS 2020. Topics include  [https://arxiv.org/abs/2006.13436 new understandings of hierarchical learning in neural networks], and [https://arxiv.org/abs/2007.03760 near-optimal self-play algorithms for learning zero-sum Markov games].

- Our [https://arxiv.org/abs/2002.04017 paper] on self-play
  is accepted by ICML 2020. We present the first line of provable
  self-play algorithms for two-player Markov games.

- In October 2019, I will be joining [https://einstein.ai/ Salesforce
  Research] in Palo Alto as a research scientist. 

- I am attending the [https://simons.berkeley.edu/programs/dl2019
  Foundations of Deep Learning] program at the Simons Institute
  (Berkeley) as a research fellow in May - August 2019. 

*Contact*:

yu.bai (at) salesforce.com

[Yu_Bai_CV.pdf Curriculum Vitae] |
[https://scholar.google.com/citations?user=owqhKD8AAAAJ&hl=en&authuser=1
Google Scholar Profile] | [https://github.com/allenbai01 Github]

{{<a href="https://twitter.com/yubai01?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @yubai01</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>}}
~~~

== Research

* Preprints *

- [http://arxiv.org/abs/2010.05843 How Important is the Train-Validation Split in Meta-Learning?] \n
  Yu Bai, Minshuo Chen, Pan Zhou, Tuo Zhao, Jason D. Lee, Sham Kakade, Huan Wang, Caiming Xiong. \n
  Preprint, 2020. \n
  Preliminary version appearing at NeurIPS 2020 workshop on Meta-Learning.
  
- [http://arxiv.org/abs/2010.01604 A Sharp Analysis of Model-based Reinforcement Learning with Self-Play.] \n
  Qinghua Liu, Tiancheng Yu, Yu Bai, Chi Jin. \n
  Preprint, 2020.

- [https://arxiv.org/abs/2002.04010 Taylorized Training: Towards Better Approximation of Neural Network Training at Finite Width.] \n
  Yu Bai, Ben Krause, Huan Wang, Caiming Xiong, Richard Socher. \n
  Preprint, 2020. [https://github.com/allenbai01/taylorized-training \[Code\]]

- [https://arxiv.org/abs/1903.00184 Proximal algorithms for constrained composite optimization, with applications to solving low-rank SDPs.] \n
  Yu Bai, John C. Duchi, Song Mei. \n
  Preprint, 2019.
  
* Publications *

- [https://arxiv.org/abs/2007.03760 Near Optimal Provable Uniform
  Convergence in Off-Policy Evaluation for Reinforcement Learning.] \n
  Ming Yin, Yu Bai, Yu-Xiang Wang. \n
  AISTATS 2021 (oral presentation).  \n
  # Preliminary version appearing at NeurIPS 2020 workshop on Offline Reinforcement Learning.
  
- [https://arxiv.org/abs/2006.13436 Towards Understanding Hierarchical
  Learning: Benefits of Neural Representations.] \n
  Minshuo Chen, Yu Bai, Jason D. Lee, Tuo Zhao, Huan Wang, Caiming
  Xiong, Richard Socher. \n
  NeurIPS 2020.

- [https://arxiv.org/abs/2006.12007 Near-Optimal Reinforcement
  Learning with Self-Play.] \n
  Yu Bai, Chi Jin, Tiancheng Yu. \n
  NeurIPS 2020. \n
  # Preliminary version appeared as /oral presentation/ at ICML 2020 Workshop on the Theoretical Foundations of Reinforcemnet Learning.

- [https://arxiv.org/abs/2002.04017 Provable Self-Play Algorithms for Competitive Reinforcement Learning.] \n
  Yu Bai, Chi Jin. \n
  ICML 2020.
  
- [https://arxiv.org/abs/1910.01619 Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks.] \n
  Yu Bai, Jason D. Lee. \n
  ICLR 2020.
  
- [https://arxiv.org/abs/1905.12849 Provably Efficient Q-Learning with Low Switching Cost.] \n
  Yu Bai, Tengyang Xie, Nan Jiang, Yu-Xiang Wang. \n
  NeurIPS 2019.
  
- [https://arxiv.org/abs/1810.10702
  Subgradient Descent Learns Orthogonal Dictionaries.] \n
  Yu Bai, Qijia Jiang, Ju Sun. \n
  ICLR 2019.

- [https://arxiv.org/abs/1810.00861 
  ProxQuant: Quantized Neural Networks via Proximal Operators.] \n
  Yu Bai, Yu-Xiang Wang, Edo Liberty. \n
  ICLR 2019. [https://github.com/allenbai01/ProxQuant \[Code\]]

- [https://arxiv.org/abs/1806.10586 Approximability of Discriminators
  Implies Diversity in GANs.] \n
  Yu Bai, Tengyu Ma, Andrej Risteski. \n
  ICLR 2019.

- [https://arxiv.org/abs/1607.06534 The Landscape of Empirical Risk
  for Nonconvex Losses.] \n
  Song Mei, Yu Bai, Andrea Montanari, 2016.
  \n /The Annals of Statistics,/ Volume 46, Number 6A (2018), 2747-2774.

* Other technical reports *
  
- [https://arxiv.org/abs/1805.08756 Analysis of Sequantial Quadratic
  Programming through the Lens of Riemannian Optimization.] \n
  Yu Bai, Song Mei, 2018.
  
- [https://arxiv.org/abs/1707.03073 TAPAS: Two-pass Approximate
  Adaptive Sampling for Softmax.] \n
  Yu Bai, Sally Goldman, Li Zhang, 2017. 

== Talks
- Subgradient Descent Learns Orthogonal Dictionaries. \n
  ICLR, May 2019, New Orleans, LA. \n
  
- ProxQuant: Quantized Neural Networks via Proximal Operators \n
  ICLR, May 2019, New Orleans, LA. \n
  Bytedance AI Lab, Dec 2018, Menlo Park, CA. \n
  Amazon AI, Sep 2018, East Palo Alto, CA.

- On the Generalization and Approximation in Generative Adversarial Networks (GANs) \n
  ICLR, May 2019, New Orleans, LA. \n
  Google Brain, Nov 2018, Mountain View, CA. \n
  Salesforce Research, Nov 2018, Palo Alto, CA. \n
  Stanford ML Seminar, Oct 2018, Stanford, CA.

- Optimization Landscape of some Non-convex Learning Problems \n
  Stanford Theory Seminar, Apr 2018, Stanford, CA. \n
  Stanford ML Seminar, Apr 2017, Stanford, CA.

== Service
- Conference reviewing: COLT, NeurIPS (top 30\% reviewer in
2018), ICLR, ICML, AIStats, IEEE-ISIT. \n

- Journal reviewing: The Annals of Statistics, JASA (Journal of the American
Statistical Association), JMLR (Journal of Machine Learning Research),
IEEE-TSP (Transactions on Signal Processing), SICON (SIAM Journal on
Control and Optimization).



