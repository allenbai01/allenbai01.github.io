# jemdoc: menu{MENU}{index.html}
= Yu Bai

~~~
{}{img_left}{yub.png}{Yu Bai}{240}{320}{}

*About me*: I am a fifth-year PhD student in
[https://statistics.stanford.edu/ Statistics] at
[http://www.stanford.edu/ Stanford University] (specializing in
statistical learning theories and non-convex optimization), where I am
fortunate to be advised by Prof. [https://stanford.edu/~jduchi/ John
Duchi]. I am a member of the
[http://statsml.stanford.edu/index.html Statistical Machine
Learning Group]. Here is my [Yu_Bai_CV.pdf CV].

My research interest lies broadly in machine learning and deep
learning. I am particularly interested in optimization and
generalization theories of deep neural networks, as well as deep
generative models. 

I am delighted to complement my research with industrial
experiences. I spent two wonderful summers working as research interns
in the industry, at [https://research.google.com Google Research] in
2016 working with [https://research.google.com/pubs/LiZhang.html Li
Zhang], and at Amazon AI Palo Alto in 2018 working with
[https://edoliberty.github.io// Edo Liberty] and
[https://www.cs.ucsb.edu/~yuxiangw/ Yu-Xiang Wang].

Prior to Stanford, I was an undergrad in mathematics at
[http://www.pku.edu.cn/ Peking University].

*Contact*:

Sequoia Hall \n
390 Serra Mall, Stanford, CA 94305

yub (at) stanford.edu
~~~

== Publications

- [https://arxiv.org/abs/1903.00184 Proximal algorithms for constrained composite optimization, with applications to solving low-rank SDPs.] \n
  Yu Bai, John Duchi, and Song Mei, 2019.
  
- [https://arxiv.org/abs/1810.10702
  Subgradient Descent Learns Orthogonal Dictionaries.] \n
  Yu Bai, Qijia Jiang, Ju Sun, 2018. \n
  To appear in ICLR 2019.

- [https://arxiv.org/abs/1810.00861 
  ProxQuant: Quantized Neural Networks via Proximal Operators.] \n
  Yu Bai, Yu-Xiang Wang, Edo Liberty, 2018. \n
  To appear in ICLR 2019.

- [https://arxiv.org/abs/1806.10586 Approximability of Discriminators
  Implies Diversity in GANs.] \n
  Yu Bai, Tengyu Ma, Andrej Risteski, 2018. \n
  To appear in ICLR 2019.
  
- [https://arxiv.org/abs/1805.08756 Analysis of Sequantial Quadratic
  Programming through the Lens of Riemannian Optimization.] \n
  Yu Bai, Song Mei, 2018.
  
- [https://arxiv.org/abs/1707.03073 TAPAS: Two-pass Approximate
  Adaptive Sampling for Softmax.] \n
  Yu Bai, Sally Goldman, and Li Zhang, 2017. 

- [https://arxiv.org/abs/1607.06534 The Landscape of Empirical Risk
  for Nonconvex Losses.] \n
  Song Mei, Yu Bai, and Andrea Montanari, 2016.
  \n /The Annals of Statistics,/ Volume 46, Number 6A (2018), 2747-2774.

== Talks
- ProxQuant: Quantized Neural Networks via Proximal Operators \n
  Bytedance AI Lab, Dec 2018, Menlo Park, CA. \n
  Amazon AI, Sep 2018, East Palo Alto, CA.

- On the Generalization and Approximation in Generative Adversarial Networks (GANs) \n
  Google Brain, Nov 2018, Mountain View, CA. \n
  Salesforce Research, Nov 2018, Palo Alto, CA. \n
  Stanford ML Seminar, Oct 2018, Stanford, CA.

- Optimization Landscape of some Non-convex Learning Problems \n
  Stanford Theory Seminar, Apr 2018, Stanford, CA. \n
  Stanford ML Seminar, Apr 2017, Stanford, CA.

== Service
- Conference reviewing: COLT, NIPS (top 30\% reviewer), ICLR, ICML, IEEE-ISIT. \n
- Journal reviewing: JMLR, IEEE-TSP, SICON (SIAM Journal on Control and Optimization).