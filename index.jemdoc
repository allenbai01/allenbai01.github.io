# jemdoc: menu{MENU}{index.html}
= Yu Bai

~~~
{}{img_left}{yub.png}{Yu Bai}{180}{240}{}

*About me*: I am a fourth-year PhD student in
[https://statistics.stanford.edu/ Statistics] at
[http://www.stanford.edu/ Stanford University], where I am fortunate
to be advised by Prof. [https://stanford.edu/~jduchi/ John Duchi]. My
research interest lies broadly in the span of machine learning,
optimization, and statistics. I am particularly interested in deep
generative models, non-convex optimization, and principled approaches
for deep learning. Here is my [Yu_Bai_CV.pdf CV].

I am delighted to complement my research with industrial
experiences. I spent a wonderful summer at
[https://research.google.com Google Research] in 2016 working with
[https://research.google.com/pubs/LiZhang.html Li Zhang]. Currently
(summer 2018) I am working as a research intern at Amazon AI in Palo
Alto.

Prior to Stanford, I was an undergrad in mathematics at
[http://www.pku.edu.cn/ Peking University].

*Contact*:

Sequoia Hall \n
390 Serra Mall, Stanford, CA 94305

yub (at) stanford.edu
~~~

== Publications
- [https://arxiv.org/abs/1806.10586 Approximability of Discriminators
  Implies Diversity in GANs.] \n
  Yu Bai, Tengyu Ma, Andrej Risteski, 2018.

- [https://arxiv.org/abs/1805.08756 On the Connection Between
  Sequential Quadratic Programming and Riemannian Gradient
  Methods.] \n Yu Bai, Song Mei, 2018. 
  
- [https://arxiv.org/abs/1707.03073 TAPAS: Two-pass Approximate
  Adaptive Sampling for Softmax.] \n
  Yu Bai, Sally Goldman, and Li Zhang, 2017. 

- [https://arxiv.org/abs/1607.06534 The Landscape of Empirical Risk
  for Nonconvex Losses.] \n
  Song Mei, Yu Bai, and Andrea Montanari, 2016.
  \n /To appear in the Annals of Statistics./
