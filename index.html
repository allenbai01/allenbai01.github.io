<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yu Bai</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Yu Bai</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Yu Bai</h1>
</div>
<table class="imgtable"><tr><td>
<img src="yub.png" alt="Yu Bai" width="240px" height="320px" />&nbsp;</td>
<td align="left"><p><b>About me</b>: I am a Research Scientist at <a href="https://einstein.ai/">Salesforce Research</a>.  In September 2019, I completed my PhD in <a href="https://statistics.stanford.edu/">Statistics</a> at <a href="http://www.stanford.edu/">Stanford University</a> (specializing in machine learning), where I was fortunate to be advised by Prof. <a href="https://stanford.edu/~jduchi/">John Duchi</a> and was a member of the <a href="http://statsml.stanford.edu/index.html">Statistical Machine Learning Group</a>. During my PhD I also spent times at the research labs of Google and Amazon. Prior to Stanford, I was an undergrad in mathematics at <a href="http://www.pku.edu.cn/">Peking University</a>.</p>
<p>My research interest lies broadly in machine learning. My recent focus is on the theoretical foundations of deep learning, reinforcement learning, representation learning, and uncertainty quantification.</p>
<p><b>News</b>:</p>
<ul>
<li><p>New <a href="https://blog.einstein.ai/beyond-ntk/">blog</a> <a href="http://www.offconvex.org/2021/03/25/beyondNTK/">post</a> on our &ldquo;Beyond NTK&rdquo; line of work. We explore how to escape the &ldquo;NTK ball&rdquo; around the initialization, and why that can bring provable learning benefits to neural networks for learning certain functions.</p>
</li>
</ul>
<ul>
<li><p>Two papers accepted by NeurIPS 2020. Topics include  <a href="https://arxiv.org/abs/2006.13436">new understandings of hierarchical learning in neural networks</a>, and <a href="https://arxiv.org/abs/2007.03760">near-optimal self-play algorithms for learning zero-sum Markov games</a>.</p>
</li>
</ul>
<ul>
<li><p>Our <a href="https://arxiv.org/abs/2002.04017">paper</a> on self-play
is accepted by ICML 2020. We present the first line of provable
self-play algorithms for two-player Markov games.</p>
</li>
</ul>
<ul>
<li><p>In October 2019, I will be joining <a href="https://einstein.ai/">Salesforce
Research</a> in Palo Alto as a research scientist. </p>
</li>
</ul>
<ul>
<li><p>I am attending the <a href="https://simons.berkeley.edu/programs/dl2019">Foundations of Deep Learning</a> program at the Simons Institute
(Berkeley) as a research fellow in May - August 2019. </p>
</li>
</ul>
<p><b>Contact</b>:</p>
<p>yu.bai (at) salesforce.com</p>
<p><a href="Yu_Bai_CV.pdf">Curriculum Vitae</a> |
<a href="https://scholar.google.com/citations?user=owqhKD8AAAAJ&amp;hl=en&amp;authuser=1">Google Scholar Profile</a> | <a href="https://github.com/allenbai01">Github</a></p>
<p><a href="https://twitter.com/yubai01?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @yubai01</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
</td></tr></table>
<h2>Research</h2>
<p><b> Preprints </b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2102.11494">Sample-Efficient Learning of Stackelberg Equilibria in General-Sum Games.</a> <br />
Yu Bai, Chi Jin, Huan Wang, Caiming Xiong. <br />
Preprint, 2021.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2102.01748">Near-Optimal Offline Reinforcement Learning via Double Variance Reduction.</a> <br />
Ming Yin, Yu Bai, Yu-Xiang Wang. <br />
Preprint, 2021.</p>
</li>
</ul>
<p><b> Publications </b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2102.07856">Don't Just Blame Over-parametrization for Over-confidence: Theoretical Analysis of Calibration in Binary Classification.</a> <br />
Yu Bai, Song Mei, Huan Wang, Caiming Xiong. <br />
ICML 2021.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2103.04554">Exact Gap between Generalization Error and Uniform Convergence in Random Feature Models.</a> <br />
Zitong Yang, Yu Bai, Song Mei. <br />
ICML 2021.</p>
</li>
</ul>
<ul>
<li><p><a href="http://arxiv.org/abs/2010.05843">How Important is the Train-Validation Split in Meta-Learning?</a> <br />
Yu Bai, Minshuo Chen, Pan Zhou, Tuo Zhao, Jason D. Lee, Sham Kakade, Huan Wang, Caiming Xiong. <br />
ICML 2021.
</p>
</li>
</ul>
<ul>
<li><p><a href="http://arxiv.org/abs/2010.01604">A Sharp Analysis of Model-based Reinforcement Learning with Self-Play.</a> <br />
Qinghua Liu, Tiancheng Yu, Yu Bai, Chi Jin. <br />
ICML 2021.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2007.03760">Near Optimal Provable Uniform
Convergence in Off-Policy Evaluation for Reinforcement Learning.</a> <br />
Ming Yin, Yu Bai, Yu-Xiang Wang. <br />
AISTATS 2021 (oral presentation).  <br />
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2006.13436">Towards Understanding Hierarchical
Learning: Benefits of Neural Representations.</a> <br />
Minshuo Chen, Yu Bai, Jason D. Lee, Tuo Zhao, Huan Wang, Caiming
Xiong, Richard Socher. <br />
NeurIPS 2020. <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2006.12007">Near-Optimal Reinforcement
Learning with Self-Play.</a> <br />
Yu Bai, Chi Jin, Tiancheng Yu. <br />
NeurIPS 2020. <br />
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2002.04017">Provable Self-Play Algorithms for Competitive Reinforcement Learning.</a> <br />
Yu Bai, Chi Jin. <br />
ICML 2020.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1910.01619">Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks.</a> <br />
Yu Bai, Jason D. Lee. <br />
ICLR 2020.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1905.12849">Provably Efficient Q-Learning with Low Switching Cost.</a> <br />
Yu Bai, Tengyang Xie, Nan Jiang, Yu-Xiang Wang. <br />
NeurIPS 2019.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1810.10702">Subgradient Descent Learns Orthogonal Dictionaries.</a> <br />
Yu Bai, Qijia Jiang, Ju Sun. <br />
ICLR 2019.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1810.00861">ProxQuant: Quantized Neural Networks via Proximal Operators.</a> <br />
Yu Bai, Yu-Xiang Wang, Edo Liberty. <br />
ICLR 2019. <a href="https://github.com/allenbai01/ProxQuant">[Code]</a></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1806.10586">Approximability of Discriminators
Implies Diversity in GANs.</a> <br />
Yu Bai, Tengyu Ma, Andrej Risteski. <br />
ICLR 2019.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1607.06534">The Landscape of Empirical Risk
for Nonconvex Losses.</a> <br />
Song Mei, Yu Bai, Andrea Montanari, 2016.
<br /> <i>The Annals of Statistics,</i> Volume 46, Number 6A (2018), 2747-2774.</p>
</li>
</ul>
<p><b> Other technical reports </b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2002.04010">Taylorized Training: Towards Better Approximation of Neural Network Training at Finite Width.</a> <br />
Yu Bai, Ben Krause, Huan Wang, Caiming Xiong, Richard Socher. <br />
Preprint, 2020. <a href="https://github.com/allenbai01/taylorized-training">[Code]</a></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1903.00184">Proximal algorithms for constrained composite optimization, with applications to solving low-rank SDPs.</a> <br />
Yu Bai, John C. Duchi, Song Mei. <br />
Preprint, 2019.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1805.08756">Analysis of Sequantial Quadratic
Programming through the Lens of Riemannian Optimization.</a> <br />
Yu Bai, Song Mei, 2018.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1707.03073">TAPAS: Two-pass Approximate
Adaptive Sampling for Softmax.</a> <br />
Yu Bai, Sally Goldman, Li Zhang, 2017. </p>
</li>
</ul>
<h2>Talks</h2>
<ul>
<li><p>How Important is the Train-Validtaion Split in Meta-Learning? <br />
One World Seminar on the Mathematics of Machine Learning, October 2020.</p>
</li>
</ul>
<ul>
<li><p>Provable Self-Play Algorithms for Competitive Reinforcement Learning. <br />
ICML, July 2020. <br />
Facebook AI Research, March 2020.</p>
</li>
</ul>
<ul>
<li><p>Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks. <br />
Simons Institute, August 2020. <br />
ICLR, April 2020.</p>
</li>
</ul>
<ul>
<li><p>Subgradient Descent Learns Orthogonal Dictionaries. <br />
ICLR, May 2019, New Orleans, LA. <br /></p>
</li>
</ul>
<ul>
<li><p>ProxQuant: Quantized Neural Networks via Proximal Operators <br />
ICLR, May 2019, New Orleans, LA. <br />
Bytedance AI Lab, Dec 2018, Menlo Park, CA. <br />
Amazon AI, Sep 2018, East Palo Alto, CA.</p>
</li>
</ul>
<ul>
<li><p>On the Generalization and Approximation in Generative Adversarial Networks (GANs) <br />
ICLR, May 2019, New Orleans, LA. <br />
Google Brain, Nov 2018, Mountain View, CA. <br />
Salesforce Research, Nov 2018, Palo Alto, CA. <br />
Stanford ML Seminar, Oct 2018, Stanford, CA.</p>
</li>
</ul>
<ul>
<li><p>Optimization Landscape of some Non-convex Learning Problems <br />
Stanford Theory Seminar, Apr 2018, Stanford, CA. <br />
Stanford ML Seminar, Apr 2017, Stanford, CA.</p>
</li>
</ul>
<h2>Service</h2>
<ul>
<li><p>Conference reviewing:  NeurIPS (2018-2020, top 30% reviewer in 2018), ICLR (2019-2021), ICML (2019-2021), COLT (2019-2020), AIStats (2020), IEEE-ISIT (2018). <br /></p>
</li>
</ul>
<ul>
<li><p>Journal reviewing: The Annals of Statistics, JASA (Journal of the American
Statistical Association), JMLR (Journal of Machine Learning Research),
IEEE-TSP (Transactions on Signal Processing), SICON (SIAM Journal on
Control and Optimization).</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2021-05-09 00:07:05 PDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-101507093-1', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>
