<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5H5S8N88V2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-5H5S8N88V2');
</script>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yu Bai</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Yu Bai</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="talks.html">Talks</a></div>
<div class="menu-item"><a href="service.html">Service</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="blog/marl_theory.html">MARL&nbsp;Blog</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Yu Bai</h1>
</div>
<table class="imgtable"><tr><td>
<img src="yub_square.jpg" alt="Yu Bai" width="180px" height="180px" />&nbsp;</td>
<td align="left"><p>Yu Bai <br />
Email: yu.bai (at) salesforce (dot) com</p>
<p><a href="Yu_Bai_CV.pdf">Curriculum Vitae</a> |
<a href="https://scholar.google.com/citations?user=owqhKD8AAAAJ&amp;hl=en&amp;authuser=1">Google Scholar Profile</a> | <a href="https://github.com/allenbai01">Github</a></p>
<p><a href="https://twitter.com/yubai01?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @yubai01</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
</td></tr></table>
<h2>About Me</h2>
<p>I am a Senior Research Scientist at <a href="https://www.salesforceairesearch.com/">Salesforce AI Research</a> in Palo Alto, CA. My research interest lies broadly in machine learning, such as deep learning, reinforcement learning, learning in games, and uncertainty quantification.</p>
<p>Before joining Salesforce, I completed my PhD in <a href="https://statistics.stanford.edu/">Statistics</a> at <a href="http://www.stanford.edu/">Stanford University</a> (specializing in machine learning) in September 2019, where I was fortunate to be advised by Prof. <a href="https://stanford.edu/~jduchi/">John Duchi</a> and was a member of the <a href="http://ml.stanford.edu/">Machine Learning Group</a>. During my PhD I also spent times at the research labs of Google and Amazon. Prior to Stanford, I was an undergrad in mathematics at <a href="http://www.pku.edu.cn/">Peking University</a>.</p>
<p>My recent goal is to unveil new understandings of foundation models (such as large language models) and transformers using a combination of experiments and machine learning theory. See our recent <a href="https://arxiv.org/abs/2306.04637">paper</a> on understanding in-context learning for an example of my recent work.</p>
<p>Besides, I am also interested in Theoretical foundations of deep learning (<a href="https://blog.salesforceairesearch.com/beyond-ntk/">blog post</a>); Reinforcement learning theory (<a href="https://yubai.org/Slides/pomdp.pdf">slides</a> on partially observable RL); Multi-agent reinforcement learning and games  (<a href="https://yubai.org/blog/marl_theory.html">blog post</a>, <a href="https://yubai.org/Slides/marl_lecture.pdf">slides</a> on MARL, <a href="https://yubai.org/Slides/efg.pdf">slides</a> on Extensive-Form Games); and Uncertainty quantification (<a href="https://yubai.org/Slides/uncertainty.pdf">slides</a>).</p>
<p><b>News</b>:</p>
<ul>
<li><p>[May 2023] Invited talk at SIAM OP23, Seattle.</p>
</li>
</ul>
<ul>
<li><p>[Apr 2023] Three papers accepted at ICML 2023.</p>
</li>
</ul>
<ul>
<li><p>[Mar 2023] I will serve as an Area Chair for NeurIPS 2023.</p>
</li>
</ul>
<ul>
<li><p>[Jan 2023] Three papers accepted at ICLR 2023.</p>
</li>
</ul>
<ul>
<li><p>[Nov 2022] Excited to be giving an invited talk &ldquo;Recent Progresses on the Theory of Multi-Agent Reinforcement Learning and Games&rdquo; at <a href="https://cs332.stanford.edu">Stanford CS332</a>.</p>
</li>
</ul>
<ul>
<li><p>[Sep 2022] Four papers accepted at NeurIPS 2022.</p>
</li>
</ul>
<ul>
<li><p>[May 2022] Excited to be speaking at the <a href="https://sites.google.com/view/rltheoryseminars/home?authuser=0">RL theory seminar</a> about our <a href="https://arxiv.org/abs/2110.04184">work</a> on sample-efficient learning of general-sum Markov Games with a large number of players!</p>
</li>
</ul>
<h2>Recent Work</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2306.04637">Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection.</a> <br />
Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, Song Mei. <br /></p>
</li>
</ul>
<h2>Research Focus and Selected Publications</h2>
<div class="infoblock">
<div class="blocktitle">Foundation Models and Transformers</div>
<div class="blockcontent">
<p>Our goal is to unveil new understandings of transformers and large language models.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2306.04637">Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection.</a> <br />
Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, Song Mei. <br /></p>
</li>
</ul>
</div></div>
<div class="infoblock">
<div class="blocktitle">Multi-Agent Reinforcement Learning Theory</div>
<div class="blockcontent">
<p>We developed the first line of provably efficient algorithms for multi-agent reinforcement learning.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2302.06606">Breaking the Curse of Multiagency: Provably Efficient Decentralized Multi-Agent RL with Function Approximation.</a> <br />
Yuanhao Wang, Qinghua Liu, Yu Bai, Chi Jin. <br />
COLT 2023.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2206.02640">Policy Optimization for Markov Games: Unified Framework and Faster Convergence.</a> <br />
Runyu Zhang, Qinghua Liu, Huan Wang, Caiming Xiong, Na Li, Yu Bai. <br />
NeurIPS 2022.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2110.04184">When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?</a> <br />
Ziang Song, Song Mei, Yu Bai. <br />
ICLR 2022.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2006.12007">Near-Optimal Reinforcement Learning with Self-Play.</a> <br />
Yu Bai, Chi Jin, Tiancheng Yu. <br />
NeurIPS 2020. <br />
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2002.04017">Provable Self-Play Algorithms for Competitive Reinforcement Learning.</a> <br />
Yu Bai, Chi Jin. <br />
ICML 2020.</p>
</li>
</ul>
</div></div>
<div class="infoblock">
<div class="blocktitle">Deep Learning Theory</div>
<div class="blockcontent">
<p>We developed optimization and generalization results for overparametrized neural networks beyond the Neural Tangent Kenrels (NTK) regime, and identified provable advantages over the NTK regime.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2006.13436">Towards Understanding Hierarchical
Learning: Benefits of Neural Representations.</a> <br />
Minshuo Chen, Yu Bai, Jason D. Lee, Tuo Zhao, Huan Wang, Caiming
Xiong, Richard Socher. <br />
NeurIPS 2020. <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1910.01619">Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks.</a> <br />
Yu Bai, Jason D. Lee. <br />
ICLR 2020.</p>
</li>
</ul>
</div></div>
<div class="infoblock">
<div class="blocktitle">Partially Observable Reinforcement Learning</div>
<div class="blockcontent">
<p>We designed sharp sample-efficient algorithms and studied the fundamental limits for partially observable reinforcement learning.</p>
<ul>
<li><p><a href="http://arxiv.org/abs/2302.01333">Lower Bounds for Learning in Revealing POMDPs.</a> <br />
Fan Chen, Huan Wang, Caiming Xiong, Song Mei, Yu Bai. <br />
ICML 2023. <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2209.14990">Partially Observable RL with B-Stability: Unified Structural Condition and Sharp Sample-Efficient Algorithms.</a> <br />
Fan Chen, Yu Bai, Song Mei. <br />
ICLR 2023 (<b>Notable-top-25% / &ldquo;Spotlight&rdquo;</b>).</p>
</li>
</ul>
</div></div>
<div class="infoblock">
<div class="blocktitle">Learning in Games</div>
<div class="blockcontent">
<p>We designed near-optimal algorithms for learning equilibria in various multi-player games under bandit feedback.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2210.11402">Learning Rationalizable Equilibria in Multiplayer Games.</a> <br />
Yuanhao Wang, Dingwen Kong, Yu Bai, Chi Jin. <br />
ICLR 2023.</p>
</li>
</ul>
<ul>
<li><p><a href="http://arxiv.org/abs/2205.15294">Efficient Phi-Regret Minimization in Extensive-Form Games via Online Mirror Descent.</a> <br />
Yu Bai, Chi Jin, Song Mei, Ziang Song, Tiancheng Yu. <br />
NeurIPS 2022 (<b>Oral</b>).</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2205.07223">Sample-Efficient Learning of Correlated Equilibria in Extensive-Form Games.</a> <br />
Ziang Song, Song Mei, Yu Bai. <br />
NeurIPS 2022.</p>
</li>
</ul>
<ul>
<li><p><a href="http://arxiv.org/abs/2202.01752">Near-Optimal Learning of Extensive-Form Games with Imperfect Information.</a> <br />
Yu Bai, Chi Jin, Song Mei, Tiancheng Yu. <br />
ICML 2022.
</p>
</li>
</ul>
</div></div>
<div class="infoblock">
<div class="blocktitle">Uncertainty Quantification in Machine Learning</div>
<div class="blockcontent">
<p>We gave precise theoretical characterizations of the calibration and coverage of vanilla machine learning algorithms, and developed new uncertainty quantificaiton algorithms with valid guarantees and improved efficiency.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2202.11091">Efficient and Differentiable Conformal Prediction with General Function Classes.</a> <br />
Yu Bai, Song Mei, Huan Wang, Yingbo Zhou, Caiming Xiong. <br />
ICLR 2022. <a href="https://github.com/allenbai01/cp-gen">[Code]</a></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2106.05515">Understanding the Under-Coverage Bias in Uncertainty Estimation.</a> <br />
Yu Bai, Song Mei, Huan Wang, Caiming Xiong. <br />
NeurIPS 2021 (<b>Spotlight</b>). <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2102.07856">Don't Just Blame Over-parametrization for Over-confidence: Theoretical Analysis of Calibration in Binary Classification.</a> <br />
Yu Bai, Song Mei, Huan Wang, Caiming Xiong. <br />
ICML 2021.</p>
</li>
</ul>
</div></div>
<div id="footer">
<div id="footer-text">
Page generated 2023-06-07 18:28:48 PDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
