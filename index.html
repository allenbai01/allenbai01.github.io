<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yu Bai</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Yu Bai</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Yu Bai</h1>
</div>
<table class="imgtable"><tr><td>
<img src="yub.png" alt="Yu Bai" width="240px" height="320px" />&nbsp;</td>
<td align="left"><p><b>About me</b>: I recently completed my PhD in
<a href="https://statistics.stanford.edu/">Statistics</a> at
<a href="http://www.stanford.edu/">Stanford University</a> (specializing in
statistical learning theories and non-convex optimization), where I was
fortunate to be advised by Prof. <a href="https://stanford.edu/~jduchi/">John
Duchi</a>. I was a member of the
<a href="http://statsml.stanford.edu/index.html">Statistical Machine
Learning Group</a>. Here is my <a href="Yu_Bai_CV.pdf">CV</a>.</p>
<p>My research interest lies broadly in machine learning and deep
learning. I am particularly interested in optimization and
generalization theories of deep neural networks, 
statistical reinforcement learning, and generative models.</p>
<p>I am delighted to complement my research with industrial
experiences. I spent two wonderful summers working as research interns
in the industry, at <a href="https://research.google.com">Google Research</a> in
2016 working with <a href="https://research.google.com/pubs/LiZhang.html">Li
Zhang</a>, and at Amazon AI Palo Alto in 2018 working with
<a href="https://edoliberty.github.io//">Edo Liberty</a> and
<a href="https://www.cs.ucsb.edu/~yuxiangw/">Yu-Xiang Wang</a>.</p>
<p>Prior to Stanford, I was an undergrad in mathematics at
<a href="http://www.pku.edu.cn/">Peking University</a>.</p>
<p><b>News</b>:</p>
<ul>
<li><p>In October 2019, I will be joining <a href="https://einstein.ai/">Salesforce Research</a> in Palo Alto as a research scientist.</p>
</li>
</ul>
<ul>
<li><p>I attended the <a href="https://simons.berkeley.edu/programs/dl2019">Foundations of Deep Learning</a> program at the Simons Institute (Berkeley) as a research fellow in May - August 2019.</p>
</li>
</ul>
<p><b>Contact</b>:</p>
<p>Sequoia Hall <br />
390 Serra Mall, Stanford, CA 94305</p>
<p>yub (at) stanford.edu</p>
</td></tr></table>
<h2>Publications</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/1905.12849">Provably Efficient Q-Learning with Low Switching Cost.</a> <br />
Yu Bai, Tengyang Xie, Nan Jiang, Yu-Xiang Wang. <br />
NeurIPS 2019.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1903.00184">Proximal algorithms for constrained composite optimization, with applications to solving low-rank SDPs.</a> <br />
Yu Bai, John Duchi, and Song Mei, 2019.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1810.10702">Subgradient Descent Learns Orthogonal Dictionaries.</a> <br />
Yu Bai, Qijia Jiang, Ju Sun. <br />
ICLR 2019.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1810.00861">ProxQuant: Quantized Neural Networks via Proximal Operators.</a> <br />
Yu Bai, Yu-Xiang Wang, Edo Liberty. <br />
ICLR 2019.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1806.10586">Approximability of Discriminators
Implies Diversity in GANs.</a> <br />
Yu Bai, Tengyu Ma, Andrej Risteski. <br />
ICLR 2019.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1805.08756">Analysis of Sequantial Quadratic
Programming through the Lens of Riemannian Optimization.</a> <br />
Yu Bai, Song Mei, 2018.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1707.03073">TAPAS: Two-pass Approximate
Adaptive Sampling for Softmax.</a> <br />
Yu Bai, Sally Goldman, and Li Zhang, 2017. </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1607.06534">The Landscape of Empirical Risk
for Nonconvex Losses.</a> <br />
Song Mei, Yu Bai, and Andrea Montanari, 2016.
<br /> <i>The Annals of Statistics,</i> Volume 46, Number 6A (2018), 2747-2774.</p>
</li>
</ul>
<h2>Talks</h2>
<ul>
<li><p>Subgradient Descent Learns Orthogonal Dictionaries. <br />
ICLR, May 2019, New Orleans, LA. <br /></p>
</li>
</ul>
<ul>
<li><p>ProxQuant: Quantized Neural Networks via Proximal Operators <br />
ICLR, May 2019, New Orleans, LA. <br />
Bytedance AI Lab, Dec 2018, Menlo Park, CA. <br />
Amazon AI, Sep 2018, East Palo Alto, CA.</p>
</li>
</ul>
<ul>
<li><p>On the Generalization and Approximation in Generative Adversarial Networks (GANs) <br />
ICLR, May 2019, New Orleans, LA. <br />
Google Brain, Nov 2018, Mountain View, CA. <br />
Salesforce Research, Nov 2018, Palo Alto, CA. <br />
Stanford ML Seminar, Oct 2018, Stanford, CA.</p>
</li>
</ul>
<ul>
<li><p>Optimization Landscape of some Non-convex Learning Problems <br />
Stanford Theory Seminar, Apr 2018, Stanford, CA. <br />
Stanford ML Seminar, Apr 2017, Stanford, CA.</p>
</li>
</ul>
<h2>Service</h2>
<ul>
<li><p>Conference reviewing: COLT, NIPS (top 30% reviewer in 2018), ICLR, ICML, AISTATS, IEEE-ISIT. <br /></p>
</li>
<li><p>Journal reviewing: JMLR, IEEE-TSP, SICON (SIAM Journal on Control and Optimization)</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2019-09-04 23:02:00 PDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-101507093-1', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>
